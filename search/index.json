[{"content":"彼得原理是彼得博士觀察眾多低效組織所得出的結論 1\n只有升遷的機會夠多夠頻繁，最終人們都會抵得不適任的職位\n這些不適任的人做出不恰當的決策，導致整個組織的效能與產出十分低落，但有趣的是這些不適任的人不是刻意擺爛，甚至在他們升職之前都是優秀且適任的員工2，到底發生才會產生這樣的悲劇？我們又該如何避免這樣的悲劇發生呢？\n造成彼得原理的原因 主要有兩個\n找工作的技能與實際工作的技能無關3\n當主管職位空缺時，底下有一批工程師，這時候會怎麼從中選拔出適合的人選當作主管？是透過年資、資歷還是身為工程師的工作能力 ?!\n主管必須有良好的溝通與管理能力，如果用上述的幾種方式都可能選出不適任的主管，例如寫程式速度很快的工程師升上主管後依然親力親為，不懂的管理與分配工作，整體的產能只會下降\n所以當提拔的評估方式與實際工作的技能脫鉤，就容易有不適任的狀況 彼得苦樂論4\n如果某種行為帶來正面回饋，那未來該行為就會持續發生，從動物實驗(如古典制約)，到人類的社會化過程都是如此 我們從小就被灌輸成績、金錢與地位的重要，逐漸內化成我們的價值觀就是要不斷的往階層上爬，如果不往上爬反而會被咎責\n衍生的問題是「當我們升遷後發現不適任，通常不會有往後退的想法」，只能無限的上攀直到不適任為止 彼得原理看似例外但其實不是例外 有沒有違反彼得原理的存在？也就是說「人連續獲得提拔但還是依然適用的階段」，作者提出幾個很有趣看似例外但不是例外的案例5\n雞肋升遷\n有時候會看到某些能力不足的人依然可以獲得「升遷」，但這種屬於明升暗降，掛個更好的頭銜但其實拔除實質的負面影響力 倒置彼得：重視程序勝過於結果\n常見於行政人員，瘋狂的填寫各種表格卻不思考這對於結果是否有幫助，這與組織的文化有關，如果組織在意員工的穩定性勝過於能力，那就會造成這個情況劣化 適任高峰：在自己位置發揮卓越的人可能只是達到頂峰沒有位置可以升遷\n在某些領域有些人爬到頂峰依然適任，但可能不安份跨足其他領域讓自己最終不適任，例如希特勒是好的政治家，但不是好的元帥 階級去角質：當表現太好反而有可能被認為不適任，因為實力好到足以擾亂整個階層的穩定 子承父業 彼得原理看婚姻11 社會價值導致兩性對於彼此的期待過高，導致自己成為不適任的伴侶，進而導致不幸福的婚姻\n灰姑娘雖然透過魔法變漂亮嫁給王子，但有沒有可能因為家世背景不同，王子很快就受不了灰姑娘不懂禮俗與上流文化 ?! 反而使得這場婚姻以悲劇收場\n症狀 最後當人抵達不適任職位時，稱為「最終職位症候群」，以下來看各種相關的症狀\n外顯的症狀 開始用一些外在、虛無縹緲的事情來掩飾自己的無能，條列幾個蠻有趣且常見的症狀6\n變態辦公桌：巨大化辦公桌、檔案癖 顧影自憐：喜歡回憶當年 (還在適任階段) 圖表狂熱 反覆無常 用外表評斷事情 口若懸河不知所云 自創名詞自我解釋 ~你的主管又中了幾項呢~\n內在症狀 胃潰瘍、高血壓、便秘、腹瀉等壓力造成的文明病，常出現在所謂的「成功人士」身上，這時候可能會採用治標不治本的方式，例如轉移焦點培養新的興趣等，但真正問題是「不適任」導致的壓力過大\nPansci 最近的影片 科學實證「心情不佳真的會造成消化、皮膚發炎、心血管健康問題」，但為什麼？佐證了心情與身體疾病的關聯\n如何避免不適任的狀況 主要分成兩個面向，一個是「個人要認清不應該無綫往上攀爬階層」以及「管理者應該避免升遷不適任人選」\n個人篇 前面提到彼得苦樂論，人被社會培養成只能從外在獲得動機與快樂，但最高層次應該是要「自我肯定」，這背後需要有明確的個人目標、清晰的計畫邁向目標、有可量化的指標時刻注意離目標的距離，最終透過自我的檢視來肯定自己7\n擺脫外在的價值觀才能帶來真正的快樂，也就避免掉無謂的攀比與不適任的機會\n如何讓人生保持快樂是個大哉問，《彼得處方》在個人篇只有稍微點出一些常見的作法，例如運動、靜心、盤點自己熱愛的事情、設定個人目標等\n實際面對升遷時，可以用兩種彼得預防性治療來思考\n負面思考8\n使用「想想未來要跟主管的主管共事、升遷後的工作與生活型態是否想要？當有了更多的錢想要做什麼？真的需要升遷或更多的錢嗎？」決定是否要升遷 假性不適任9\n在一些無損專業能力的小毛病犯錯，讓別人誤以為已經到達不適任，避開升遷機會，切記不可以直接回絕升遷這會讓人覺得沒有上進心等負面觀感，要用比較迂迴的手段達到不升遷的目的 如果已經升遷到不適任階級，可以用彼得安慰劑來緩解症狀\n永無止盡的準備：逃避實際執行 專精枝微末節的小事 以形象蓋過工作表現 做跟工作完全無關的事情 成為職務代理人：避免了原本不適任的職位，也不用承擔現有代理職位的責任 管理者篇 管理者首要之重是明定目標與職位所需的能力，才能依此來評斷人選是否適合，可以透過試行升遷、暗中考核、提前培養能力等方式，先確保人選有足夠能力才升遷9\n往後退一步，升遷只是管理者激勵員工的一種方式，這邊需要了解到員工工作的動機因素，這點在《你如何衡量你的人生》書中有特別著重 雙因素理論 12\n雙因素理論：薪水只是「保健因素」，充其量讓你不討厭工作而「內在動機」才會讓工作更快樂\n更近一步來說，消滅不快樂不會使你快樂!，這是一個蠻有趣的觀念，更進一步說明可以參考 人生真正的快樂是什麼？大多數人都理解錯了\u0026hellip; ► 聽聽哈佛教授怎麼說 - Dr. Arthur Brooks 阿瑟·布魯克斯\n攤開所有的獎勵辦法，包含薪水、升遷、提升地位、績效考核、員工分潤外，還有讚美、授權、強化員工能力等方式10，需要妥善依照員工能力與階段給與對應的獎勵如 情境式領導，單純用薪資或升遷不是良好的激勵辦法\n總結 彼得博士在書中用比較諷刺辛辣的口吻在描述，並創造一堆彼得前綴的詞彙來諷刺現象，讀起來蠻有趣的，看到某一些人物設定與場景跟以前的工作經驗吻合不免會心一笑，但看看別人想想自己，也在思考自己是否一直被外在所驅動，抑或是自己已經成為彼得而不自知 (希望還沒)\n整理上這兩本書蠻值得一起看，《彼得處方》這本書內容個人覺得比較單薄，只能說蜻蜓點水式把常見的作法提點出來，但內容比較少需要再往下鑽研\n附錄 目前嘗試用卡片盒筆記 heptabase《彼得原理》與《彼得處方》筆記，盡量會把摘要內容對應的書頁附上，方便回頭查閱\n《彼得原理》p42 《彼得原理》p48 《彼得原理》p7 《彼得處方》p104 《彼得原理》p54 《彼得原理》p162 《彼得處方》p75 《彼得原理》p219 《彼得處方》p189 《彼得處方》p213 《彼得處方》p30 《你如何衡量你的人生》p54 ","date":"2023-05-17T00:21:40.869Z","permalink":"https://yuanchieh.page/posts/2023/2023-05-17-%E5%BD%BC%E5%BE%97%E5%8E%9F%E7%90%86%E8%88%87%E5%BD%BC%E5%BE%97%E8%99%95%E6%96%B9%E5%BF%83%E5%BE%97%E5%88%86%E4%BA%AB/","title":"《彼得原理》與《彼得處方》心得分享"},{"content":"之前對於露營車野營一直充滿著想像，想像自己可以很貼近大自然，睡醒打開車門就可以沈浸在海聲、山林之中，最近排出三天兩夜租借 GoodGoodVan，簡單分享一些心得與優劣，以及簡易的行程表供大家參考 主要想一天看海一天看山，所以第一天挑戰車泊外澳車站，第二天定露營地 夏諾營營地\n優點 貼近大自然\n我們第一天停在外澳車站對面的停車位，一早打開就真的是看海聽浪聲，在夏諾營營地晚上可以看夜景、早上就是看看河景，非常迷人 鋪上燈具氣氛滿分\n第一天晚上有到外澳沙灘上坐坐，鋪上燈具效果十足，吹著海風貼著音樂十分愜意 車子會不會不好開 身為一個月頂多只開一次 iRent 的開車小白，一開始租車有點怕 Veryca (GoodGoodVan 車型)會不會太大台不好開，但實際開了三天覺得沒什麼問題，除了確實車身比較長所以倒車比較困難，但其餘上路、停車都沒有問題\n甚至我覺得油門踩起來比 Toyota 的 Yaris 有力 😆 而且車身比較高視野跟坐姿也比較舒服一些，只是在轉彎時離心力有點明顯稍微不穩，其餘狀況都很 OK，開上高速公路也都沒什麼狀況\n缺點 說完優點來講一些租車前沒注意到的缺點\n隱蔽性較差，淺眠者容易會被吵醒\n第一天是車泊外澳車站，剛好那邊有酒吧開比較晚，所以都會有人聲跟走動聲，待在車內因為隔音也沒有很好，所以只要有人走動就會被吵醒一下，睡眠品質不太好；\n第二天到夏諾營營地剛好當天只有我們一組客人，很安靜所以睡不得錯，但如果有其他組一起露營就不確定睡眠品質了\n車內空氣不流通 GoodGoodVan 後座只有一扇可以外推的對外窗，其餘幾面都不能開，所以空氣很不流通，在山上還好平地晚上就有點悶熱\n室內冷氣非常吵基本不能用 室內冷氣必須接電才能啟動，但非常吵我覺得睡覺要開根本睡不著，直接看影片比較實際 後座的床不好睡\n床是後座打平直接睡，但後座不是完全平整，所以睡起來腰的部分簍空其實不太好睡，後來有鋪個睡袋才稍微好一些\n有看一些比較好的露營車車款會有額外的床板跟睡墊，這樣比較平整應該才會比較好睡；床的尺寸大概兩個成人 (170 cm) 剛剛好，在高一點腳就會懸空了\n如果有常在露營的，缺點1 應該不是問題 / 至於缺點2、3開到山區比較涼爽的地方也不是問題，缺點4 就要自己準備睡墊或本身易睡體質才能夠克服\n其他雜記 行程簡易流水帳 Day1\n10 點在淡水站取車 先回家載行李，直接到宜蘭 屋台拉麵 川湯泡 Spa 加自助晚餐，Spa 有多種口味的溫泉蠻奇特，晚餐還算不錯 夜宿外澳車站對面停車場，有點熱有點吵，建議停過去一點的公廁比較安靜一些 Day2\n金車生技養殖中心看一些海洋生物還不錯 樂色山 看積木作品，場館不大但蠻有趣的，蠻多有趣的收藏跟作品 南方澳漁港 吃午餐跟買晚上食材，大推生蝦十分的甜美，順便買了蛤蠣和小章魚，非常划算 夏諾營營地上山的路有點小有點抖，但山上的風景很不錯，營地也很乾淨 Day3 簡單買個伴手禮就回程\n橘之鄉 潭酵天 在旁邊的龍潭客棧吃午餐還不錯 事前準備 露營資訊 在台灣一講到露營就必須提到 車床天地，出發前在上面東找找西找找才找到目前的位置 露營地租借是透過露營樂，不過頗尷尬的平台之前有發生過個資外洩我老婆有遇到詐騙\u0026hellip; 但因為還算方便所以我們這次也還是用這個平台訂購 Youtube 頻道 憂娘駕駛Outdoor，當初會有車泊的想法也是因為看到這部影片，才想說找個地點車泊看看 車泊：直接睡車上，停在停車場之類的地方，通常附近只會有廁所而不會有淋浴的地方\nGoodGoodVan 相關 租借三天兩夜費用是假日 5800 + 平日 4800 - 續借優惠 1000 共 9600，另外 5000 元押金；\n取還車地點是在關渡捷運站，比較友善是 10:00 a.m. 租借可以第三天 10:00 p.m. 回車 (當天下訂的優惠)，這樣第三天就不會太趕可以慢慢回程\n整體租借還算滿意，有附贈藍牙喇叭、氣氛燈具、鍋具卡式爐，只要自己在準備碗筷、食材即可，但他們的鍋具有點老舊可以的話建議自備 之前看他們有上節目分享，IG 經營的也不錯\n總結 身為露營小白，三天兩夜的車露體驗覺得還不賴，時間再長可能就變成一種折磨 XD 同時也解鎖一個成就，之後有機會再來租其他更好的露營車體驗看看\n","date":"2023-05-15T00:21:40.869Z","permalink":"https://yuanchieh.page/posts/2023/2023-05-15-%E6%96%B0%E6%89%8B%E7%A7%9F%E5%80%9F%E9%9C%B2%E7%87%9F%E8%BB%8A%E8%88%87%E4%B8%89%E5%A4%A9%E5%85%A9%E5%A4%9C%E4%B9%8B%E6%97%85/","title":"新手租借露營車與三天兩夜之旅"},{"content":"最近看到一篇很不錯的文章MongoDB internal Architecture，主要在描述 MongoDB Storage Engine 的演進\n其中在 5.3 比較大的改變是引入了 Clustered Collection 調整了 Storage Engine 儲存的機制，這也連帶影響 MongoDB 在 Collection 上的效能表現，以下從文章中摘要 Storage Engine 演進，並透過 Benchmark 去驗證看看實際的表現\nMongoDB Storage Engine 演進 MMAPv1 最一開始 MongoDB 的 Storage Engine 是 MMAPv1，採用 B+Tree 架構以 _id 為 primary key，leaf node 儲存 DiskLoc 直接指向儲存於硬碟上的位置(透過 file name + offset) MMAPv1 的優點是查詢速度很快，只需要 O(logN) 在 B+Tree 找到 leaf node + O(1) 去硬碟讀取資料即可\n但有幾個重大缺點\n因為是儲存實際 Disk 位置，所以當 document 因為 insert / update / delete 而改變 Disk 位置 時，就需要大規模的改寫 DiskLoc 造成效能的影響 儲存時沒有壓縮 一開始只提供 database level / collection level 的 lock，這也導致在 parallel 執行上效率不高 最終在 MongoDB v4.2 就全面移除了\nWiredTiger MongoDB 在 2014 年收購了 WiredTiger 這個 Storage Engine，並於 3.0 加入於 3.2 變成預設的 Storage Engine\n在儲存上同樣是採用 B+Tree 結構，但這時候 _id 不是 Clustered Index Key 而是在內部 WiredTiger 會自動為每個 document 設定 recordId 當作儲存排序的依據\n所以 _id 跟其他 secondary index 相同，都是先在自己的 B+Tree 上查找，找到對應的 recordId 後再去 recordId Clustered Index 搜尋\n也因此 WiredTiger 的缺點是查詢變慢一些，因為需要搜尋兩次 B+Tree 才能去 Disk 撈資料\n但有幾項優點\nInsert、Update、Delete 效能比 MMAPv1 穩定 提供 document level lock，大幅改善 parallel 效能 壓縮 BSON 在儲存，減少 Disk 用量代表可放進 buffer pool 的資料筆數也增加 根據這篇文章 Linkbench for MySQL \u0026amp; MongoDB with a cached database 的實驗可以看到 MongoDB WiredTiger 在查詢與寫入都吊打其他的 Storage Engine 有趣的是 MySQL 寫入、查詢吊打 MongoDB XD\n在單機的環境下看起來老牌 RDBMS 比較厲害，只是實際選擇還需要再考量到 scalability 等狀況\nWiredTiger Clustered Collection 原理 接著到了 MongoDB 5.3 的 Clustered Collection，剛剛有提到實際內部儲存依據是用 recordId 排序，那如果直接拿 _id 當作 Clustered Index 的 key 是不是就能更加速寫入跟查詢的效率？這正是 Clustered Collection 所採用的方式，直接拿 _id 當作 Clustered Index\n根據官方文件，在建立 Clustered Collection 就必須指定用 _id 建立，同時必須是 unique\n1 2 3 4 5 6 client[\u0026#34;collection\u0026#34;].create({ :clustered_index =\u0026gt; { :key =\u0026gt; { :_id =\u0026gt; 1 }, :unique =\u0026gt; true, } }) 根據文件 Clustered Collections所述有幾個優點\n大幅度增加 _id 搜尋速度，尤其是 range scan 上，因為少了一次去查找 recordId index Faster queries on clustered collections without needing a secondary index, such as queries with range scans and equality comparisons on the clustered index key.\n增加 CRUD 的效能 Clustered collections have additional performance improvements for inserts, updates, deletes, and queries.\n但也不是全然沒有壞處\nSecondary Index leaf node 會儲存 Clustered Index 的 key，而因為前面提到原本 collection Clustered Index 是用 recordId(8byte)，而 _id 預設是 12byte，所以就直接影響了 Secondary Index 的 size (增加約 20%) _id 是用戶可以預設，如果設錯可能會讓效能變差 整體來說目前 MongoDB Clustered Collection 儲存方式有點接近 MySQL，同樣都是\n指定 Primary Key 當作 Clustered Index 影響實際儲存的方式 Secondary Index leaf node 指向 Clsutered Index 同樣的如果 _id 是採用 UUID 等亂序，也同樣會有效能上的影響，這部分可以參考 UUIDs are Popular, but Bad for Performance — Let’s Discuss，這點後續 benchmark 會驗證\nBenchmark 透過以下幾個實驗來實際檢驗一下 MongoDB v6.0.5 Clustered Collection 的效能\n比對 CRUD 在 Clustered Collection vs 一般 Collection 差異\n預期 CRUD 在 Clustered Collection 應該要較好 針對 Secondary Index，比較實際大小與效能\n預期 Secondary Index 在 Clustered Collection 儲存空間要比較大，如果尺寸沒有大到記憶體塞不下的狀況，效能部分預期是差不多 _id 改用 UUID 對於寫入效能的影響\n預期 Clustered Collection 會有比較差的表現，因為底層儲存結構的關係 相關程式碼在 sj82516/mongodb-bm-cluster-collection\nInsert 比較 實驗每個 iteration 準備 100 萬筆資料並用 insert_many 批次寫入 1000 筆，總共插入 2000 萬筆；實驗組是 clustered collection 但對照組不是\n從圖表來看 Clustered Collection 有稍微比較好但沒有到非常明顯的變化\nSearch 比較 測試過程有發現一個 MongoDB 的 bug Performance Issue about Clustered Collection : where there are more than one _id search condition, the search would fallback to COLLSCAN，主要是 find 條件中有多個 _id 時 index 會沒有吃到 clustered index 導致查詢非常慢\n但有趣的是 secondary index 目前測試是沒有這個問題的\n1 2 3 4 5 6 user system total real cluster search 1.011011 0.095830 1.106841 ( 27.275139) normal search 0.405232 0.034662 0.439894 ( 0.449653) user system total real cluster email search 0.417697 0.035939 0.453636 ( 0.556692) normal email search 0.407986 0.027175 0.435161 ( 0.487835) 可以看到 cluster search 慢到爆，delete_many 有同樣的 issue\n本來以為是不是 driver 問題，但實際用 Mongo Compass 官方 GUI tool 去分析查詢，確實發現只要 $in 裡面的條件超過一筆就會變成 COLLSCAN\nSecondary Index 大小 可以看出同樣的 documents 下 Secondary Index 在 Clustered Index 確實變大快 25%\nUUID 寫入的影響 實驗每個 iteration 準備 100 萬筆資料並用 insert_many 批次寫入 1000 筆，兩個 collection 都是 clustered collection，實驗組 _id 設定為 UUID\n可以看到 UUID 對於效能的負面影響十分巨大\n延伸閱讀 以下是我在閱讀時想到的一些額外問題，覺得蠻有趣也順便記錄一下\n為什麼 WiredTiger 一開始不支援 Clustered Index 既然有這麼大的好處，為什麼一開始不支援呢？在這個 google group discuss 有稍微帶到 Purpose of a separate index storage for primary id for MongoDB\n理解起來比較像技術債，一開始 MMAPv1 是使用 DiscLoc，後來 WiredTiger 改成用 RecordId 而不是直接綁死 Disk 位置\nthe origin of this decision was the MMAPv1 storage engine. There, indexes would map keys to 'DiskLoc' values, containing a pair of 32-bit integers representing the file number in the database and offset in that file. Since then we have replaced DiskLoc by a RecordId that does not represent a physical location, but rather a logical document number.\n另外就是 MongoDB 對於 Primary Key 使用有額外的用途，包含 Sharding，所以實作比想像中複雜 另外也有提到即使有效能上的提升但對於 Secondary Index 是有負面影響\nwe could gain efficiency (both time and space) by using the primary key directly to index the data. However, as MongoDB puts few restrictions on the primary key, it is common for these primary keys to have a non-trivial size. This in turn means that all secondary indexes become less efficient. Some users have many indexes, so they'd be negatively affected by such a change. \u0026hellip;\u0026hellip;\n結語 整體來說 Clustered Collection 寫入效能確實有好上一些，搜尋部分因為有 bug 暫時還不確定，而 Secondary Index 體積會有感的變大；目前整題評估下來不建議採用 Clustered Collection，些微的效益比不上這些風險與副作用\n","date":"2023-05-05T00:21:40.869Z","permalink":"https://yuanchieh.page/posts/2023/2023-05-05-mongodb-clustered-collection-%E8%88%87-benchmark-%E5%AF%A6%E9%A9%97/","title":"MongoDB Clustered Collection 與 Benchmark 實驗"},{"content":"Golang 本身不支援繼承，原因在官方的 QnA 有回答，與其專注在 type 本身的關係，還不如注重在 interface 能否滿足特定的行為\nTypes can satisfy many interfaces at once, without the complexities of traditional multiple inheritance.\n而坊間大多的 Golang 繼承實作都是透過 embedded struct 透過 composition 模擬 inheritance，如這篇 秒懂 go 语言的继承 或這一篇 Inheritance in GoLang\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 // 动物类 class Animal { public String name; public String subject; void eat(String food) { System.out.println(name + \u0026#34;喜欢吃：\u0026#34; + food + \u0026#34;,它属于：\u0026#34; + subject); } } // 猫类。 猫类继承动物类 class Cat extends Animal{ // 猫自己的属性和方法 public int age; void sleep() { System.out.println(name + \u0026#34;今年\u0026#34; + age + \u0026#34;岁了，特别喜欢睡觉\u0026#34;); } } public class Test{ public static void main(String[] args){ // 创建一个动物实例 Animal a = new Animal(); a.name = \u0026#34;动物\u0026#34;; a.subject = \u0026#34;动物科\u0026#34;; a.eat(\u0026#34;肉\u0026#34;); // 创建一个猫实例 Cat cat = new Cat(); cat.name = \u0026#34;咪咪\u0026#34;; cat.subject = \u0026#34;猫科\u0026#34;; cat.age = 1; cat.eat(\u0026#34;鱼\u0026#34;); cat.sleep(); } } ———————————————— 原文作者：pureyb 转自链接：https://learnku.com/articles/32295 版权声明：著作权归作者所有。商业转载请联系作者获得授权，非商业转载请保留以上作者信息和原文链接。 實作上 subclass (cat) 將 superclass (animal) embedded，當 subclass 呼叫沒有定義的 method 時，會去調用 superclass 的 method，例如上面案例中 cat.eat 是調用 animal.eat\n但這樣就真的滿足繼承的條件了嗎？\n為什麼我們會需要繼承 回歸原點，我們會在什麼時候偏好繼承大過於組合這樣的實作方式 ?\n就我自己的習慣是\n型別之間有很明確 is-a 的關聯 實作上大多的行為相同，只有部分的行為不同，會需要套用 樣板方法 Template Method 時 例如汽車、機車都是交通工具，他們都可以提供從 A 移動到 B 的服務，但他們騎乘的方式不同\n另外在實作繼承時，別忘了 SOLID 原則中的 Liskov 替換原則，行為上 subclass 應該要能夠替換 superclass 所出現的地方\n更具體描述繼承在實作上的需求\nLiskov 替換原則：subclass 替換 suplerclass 不應該有型別上的錯誤 樣板方法：superclass 定義呼叫流程，而 subclass 可以複寫部分方法 method propagate：當呼叫 subclass 不存在的方法，會往 superclass 去查找 我們來檢視上面的作法以上需求\n假設 Animal 有一個 method 是 wakeUp，wakeUp 固定會 yell \u0026amp; eat，但不同的動物有不同的 yell 方式與 eat 內容\ngo playground\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 // You can edit this code! // Click here and start typing. package main import \u0026#34;fmt\u0026#34; type Animal struct { Name string } func (a *Animal) Wakeup() { a.yell() a.eat() } func (a *Animal) yell() { fmt.Println(\u0026#34;default yell\u0026#34;) } func (a *Animal) eat() { fmt.Println(\u0026#34;default eat\u0026#34;) } type Cat struct { Animal } func (c *Cat) yell() { fmt.Println(\u0026#34;meow meow\u0026#34;) } func (c *Cat) eat() { fmt.Println(\u0026#34;cat eat meat\u0026#34;) } func main() { cat := \u0026amp;Cat{ Animal{ Name: \u0026#34;BigCat\u0026#34;, }, } cat.Wakeup() } // output // default yell // default eat 咦?! 輸出竟然不是呼叫 cat.yell() 和 cat.eat()，反而是呼叫到 animal.yell() / animal.eat()，對比 Ruby 的執行結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class Animal def wakeup yell eat end def yell puts \u0026#34;default yell\u0026#34; end def eat puts \u0026#34;default eat\u0026#34; end end class Cat \u0026lt; Animal def yell puts \u0026#34;meow meow\u0026#34; end def eat puts \u0026#34;cat eat meat\u0026#34; end end cat = Cat.new cat.wakeup # meow meow # cat eat meat 究竟是哪裡出錯了 ?!\n注意 Golang embedded struct 執行的角色 更具體的描述可以參考此篇 Type embedding: Golang\u0026rsquo;s fake inheritance，當我們利用 method propagate 找到 superclass 定義的方法時，要注意 此時執行的角色是 superclass 而不是 subclass!\n例如上面的案例，wakeup 是定義在 Animal 當中，Cat 呼叫 wakeup 最後是用 Animal 去執行，而當 Animal 執行 wakeup 時就是呼叫 Animal.yell / Animal.eat 透過 Interface 與重新調整 Embedded 方向 先上結論，將 subclass 要個別實作的方法抽成 interface，並改將 subclass embedded 到 superclass 中，參考 go playground\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 // You can edit this code! // Click here and start typing. package main import \u0026#34;fmt\u0026#34; type ISubClass interface { Temp1() Temp2() } type SuperClass struct { sub ISubClass } func (s *SuperClass) Method() { s.sub.Temp1() s.sub.Temp2() } type SubClass1 struct{} func (s *SubClass1) Temp1() { s.temp3() fmt.Println(\u0026#34;temp1 from SubClass1\u0026#34;) } func (s *SubClass1) Temp2() { fmt.Println(\u0026#34;temp2 from SubClass1\u0026#34;) } type SubClass2 struct{} func (s *SubClass2) Temp1() { fmt.Println(\u0026#34;temp1 from SubClass2\u0026#34;) } func (s *SubClass2) Temp2() { fmt.Println(\u0026#34;temp2 from SubClass2\u0026#34;) } func main() { cls1 := \u0026amp;SuperClass{ sub: \u0026amp;SubClass1{}, } cls2 := \u0026amp;SuperClass{ sub: \u0026amp;SubClass2{}, } cls1.Method() cls2.Method() } 這樣做就滿足了\nLiskov 替換原則：不同的 subclass 都依附在 SuperClass，所以直接替換沒問題 Template method：Superclass 透過 interface 呼叫 subclass method method propagate：這個比較 tricky，因為是直接呼叫 Superclass，所以也沒有 method propagate 的問題 以上滿足我們前面一開始對於繼承的定義，但有幾個侷限\n因為我們反轉 embedded struct 的位置，所以 subclass 不能呼叫 superclass 任何的方法或參數，只能從 superclass 呼叫 subclass，這點我覺得比較還好，如果呼叫方向交錯反而很亂 subclass 的型別不明確，因為都掛在 superclass 下，如果要判斷型別需要額外處理 subclass 不能有客製化的 Public Method，會被 interface 侷限，例如我只有 SubClass1 想要有 Temp3 的 public method，變成 interface 也要增加否則無法呼叫 1 2 3 4 5 6 7 8 9 10 11 type ISubClass interface { .... Temp3() } func (s *SubClass1) Temp3() { fmt.Println(\u0026#34;temp3 from SubClass1\u0026#34;) } func (s *SubClass2) Temp3() { // SubClass2 被迫也要增加，反則無法滿足 interface } 如果 subclass 想要呼叫 superclass 方法 侷限第一點提到因為 embedded 目前是掛在 superclass 下，反過來就是 subclass 無法呼叫 superclass，如果希望 subclass 呼叫 superclass 的方法，就需要像 Template Method 由 superclass 主動呼叫，例如\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 type SuperClass struct { SubClass Name string } type SubClass struct {} func (s *SubClass) ShowName() { // 想要取得 SuperClass Name 屬性 // 無法這樣存取!! \u0026#34;prefix\u0026#34; + s.Name } ////// 調整做法 ///////// type SuperClass struct { SubClass Name string } // 改從 superclass 處理，把 name pass 給 subclass func (s *SuperClass) ShowName() { pfxName := s.ParseName(s.Name) } type SubClass struct {} func (s *SubClass) ParseName(name string) { // 想要取得 SuperClass Name 屬性 // 無法這樣存取!! name } 結語 以上提供不同的思路，透過改變 embedded struct 的不同層級，會有不同的效果\nsubclass embedded superclass：subclass 可以任意定義 public method，但無法實作 Template method superclass embedded subclass：可以實作 Template method，但 subclass 無法有額外的 public method，除非擴充 interface 再仔細想想方法二，在其他 OOP language 中，套用 Template method 時 subclass 定義的 method 可以直接存取 superclass 定義的參數，但是在 Golang 中是無法做到的\n所以在 Golang 中要仿造 Template method 反而會比較像 Strategy pattern，superclass 需要抽換某些行為 (策略)，那就透過不同的策略設計並在初始化時帶入\n整體實作還是用 組合來代替繼承，在使用 Golang 上建議還是不要用繼承的概念去思考會讓實作比較順\n","date":"2023-04-27T00:21:40.869Z","permalink":"https://yuanchieh.page/posts/2023/2023-04-27-%E6%8E%A2%E8%A8%8E-golang-%E5%AF%A6%E4%BD%9C%E9%A1%9E%E4%BC%BC%E7%B9%BC%E6%89%BF%E7%9A%84%E4%B8%8D%E5%90%8C%E5%81%9A%E6%B3%95/","title":"探討 Golang 實作類似繼承的不同做法"},{"content":" 當我們希望打造一個基於語意相似的文字搜尋時，一般的作法是將文字透過 AI model 轉成 embeddeding vector，透過計算 vector distance (cosine distancing) 找出距離最相近的文字 kNN (k nearest neighbors)，例如 OpenAI 的 demo code：Semantic_text_search_using_embeddings\n當資料量小的時候，這樣做不會有什麼問題，但仔細看目前的比對方式是把輸入跟資料集的資料每一筆 vector 都做 distance 計算，所以複雜度會是 O(N)，N 是資料集數量，如果資料量一大，這勢必會是造成瓶頸\nVector DB 就是要解決這樣的問題，主要透過\n改用 ANN (approximate nearest neighbors) 取代 kNN，用相似度查詢換取執行速度 提供 database 功能，包含持久化保存、水平擴展 (sharding)、高可用性、API 封裝等功能 目前搜尋市面上有幾個常見的選擇\nPinecone：閉源專案，就先略過不用 Milvus：純 vector DB，看起來在基礎建設 (scaling、availability) 等做得比較完整 Weaviate：有 module 可以支援 AI model 整合，也支援純 vector DB 使用 Chroma：Fireship demo 用 這次 Demo 選擇用 Weaviate，主要是有支援 module 直接整合 AI model，這樣我就不用另外想 embedding vector 該如何產生\nWeaviate DB 基本介紹 Weaviate DB 有以下幾個特色\n便利性：\nmodule 支援常見的 AI model，包含 transformer、openai 等，透過參數可以直接調整，但有些 AI model 部分需要自己架設 server，Weaviate DB 核心並不包含 AI model，只是會直接透過 interface 調用 搜尋與過濾：\n除了 vector 搜尋外，在文字上會結合 inverted index 增加搜尋的精準度與效率，並提供 filter 可以透過屬性篩選 API 接口支援 REST 與 GraphQL 擴充性： 會提供 Sharding 與 High Availability (後者還在開發中) 儲存概念 以 Class 為管理單位，可以理解成 table 或 collection 的概念，每個 class 有自己 vectorize 的機制、sharding 設定等，例如\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 { \u0026#34;class\u0026#34;: \u0026#34;string\u0026#34;, // The name of the class in string format \u0026#34;description\u0026#34;: \u0026#34;string\u0026#34;, // A description for your reference \u0026#34;vectorIndexType\u0026#34;: \u0026#34;hnsw\u0026#34;, // Defaults to hnsw, can be omitted in schema definition since this is the only available type for now \u0026#34;vectorIndexConfig\u0026#34;: { ... // Vector index type specific settings, including distance metric }, \u0026#34;vectorizer\u0026#34;: \u0026#34;text2vec-contextionary\u0026#34;, // Vectorizer to use for data objects added to this class \u0026#34;moduleConfig\u0026#34;: { \u0026#34;text2vec-contextionary\u0026#34;: { \u0026#34;vectorizeClassName\u0026#34;: true // Include the class name in vector calculation (default true) } }, \u0026#34;properties\u0026#34;: [ // An array of the properties you are adding, same as a Property Object { \u0026#34;name\u0026#34;: \u0026#34;string\u0026#34;, // The name of the property \u0026#34;description\u0026#34;: \u0026#34;string\u0026#34;, // A description for your reference \u0026#34;dataType\u0026#34;: [ // The data type of the object as described above. When creating cross-references, a property can have multiple data types, hence the array syntax. \u0026#34;string\u0026#34; ], \u0026#34;moduleConfig\u0026#34;: { // Module-specific settings \u0026#34;text2vec-contextionary\u0026#34;: { \u0026#34;skip\u0026#34;: true, // If true, the whole property will NOT be included in vectorization. Default is false, meaning that the object will be NOT be skipped. \u0026#34;vectorizePropertyName\u0026#34;: true, // Whether the name of the property is used in the calculation for the vector position of data objects. Default false. } }, \u0026#34;indexInverted\u0026#34;: true // Optional, default is true. By default each property is fully indexed both for full-text, as well as vector search. You can ignore properties in searches by explicitly setting index to false. } ], \u0026#34;invertedIndexConfig\u0026#34;: { ... }, \u0026#34;shardingConfig\u0026#34;: { ... // Optional, controls behavior of class in a multi-node setting, see section below } } 每個 Class 內包含多個 Object 以 json 格式儲存，Object 內的屬性 (Property) 支援多種格式，額外有提供地理位置、日期、電話號碼(有點不解?!)，其中地理位置在查詢上還支援方圓內的篩選\nClass schema 是靜態的，如果要增減欄位需要透過 API 修改，而不像某些 NoSQL 是動態寫入的\n物理儲存以 Shard 為單位 一個 Class 在物理儲存上有多個 Shard，每個 Shard 會包含 vector index / object store / inverted index，其中 vecotr index 目前是用 HNSW，其餘兩個是用 LSMTree 儲存\nDemo - 打造簡易的文字查詢 程式碼在 sj82516/weaviate-db-demo，在本地端啟動 Weaviate DB 並做簡易的文字查詢\n透過 docker-compose 啟動 這邊我們只做文字搜尋的部分，選用 transformer 當作 AI model，可以查看不同的 modules\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 version: \u0026#39;3.4\u0026#39; services: weaviate: image: semitechnologies/weaviate:1.18.3 ports: - \u0026#34;8080:8080\u0026#34; environment: QUERY_DEFAULTS_LIMIT: 20 AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: \u0026#39;true\u0026#39; PERSISTENCE_DATA_PATH: \u0026#34;./data\u0026#34; DEFAULT_VECTORIZER_MODULE: text2vec-transformers ENABLE_MODULES: text2vec-transformers TRANSFORMERS_INFERENCE_API: http://t2v-transformers:8080 CLUSTER_HOSTNAME: \u0026#39;node1\u0026#39; t2v-transformers: image: semitechnologies/transformers-inference:sentence-transformers-multi-qa-MiniLM-L6-cos-v1 environment: ENABLE_CUDA: 0 # set to 1 to enable # NVIDIA_VISIBLE_DEVICES: all # enable if running with CUDA 建立 Class 與匯入資料 Class schema 可以主動宣告，或是讓 Weaviate DB 自動幫忙建立 (有點像 Elasticsearch)，這邊建立了一個 Class 並指定 module 用 text-transformer\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 client.Schema().ClassCreator().WithClass(\u0026amp;models.Class{ Class: className, Description: \u0026#34;all books I have\u0026#34;, Vectorizer: \u0026#34;text2vec-transformers\u0026#34;, ModuleConfig: map[string]interface{}{ \u0026#34;text2vec-transformers\u0026#34;: map[string]interface{}{}, }, Properties: []*models.Property{ { Name: \u0026#34;title\u0026#34;, DataType: []string{\u0026#34;text\u0026#34;}, }, }, }) 匯入資料可以批次匯入\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 objects := []*models.Object{ { Class: className, Properties: map[string]interface{}{ \u0026#34;title\u0026#34;: \u0026#34;Hello World Blue\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;program\u0026#34;, }, }, { Class: className, Properties: map[string]interface{}{ \u0026#34;title\u0026#34;: \u0026#34;Hello World Red\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;program\u0026#34;, }, }, { Class: className, Properties: map[string]interface{}{ \u0026#34;title\u0026#34;: \u0026#34;Hello World Yellow\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;science\u0026#34;, }, }, } client.Batch().ObjectsBatcher(). WithObjects(objects...). WithConsistencyLevel(replication.ConsistencyLevel.ALL). Do(context.Background()) 搜尋與篩選 完整查詢有三個部分 搜尋 + 篩選 + 欄位過濾\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 // 文字搜尋 nearText := client.GraphQL().NearTextArgBuilder(). // 搜尋的關鍵字 WithConcepts(concepts). // 讓向量往某個方向靠近 WithMoveTo(\u0026amp;graphql.MoveParameters{ Force: 0.5, Concepts: []string{ \u0026#34;Yellow\u0026#34;, }, }) // 篩選機制 where := filters.Where(). WithPath([]string{\u0026#34;type\u0026#34;}). WithOperator(filters.Equal). WithValueText(\u0026#34;program\u0026#34;) // 選擇欄位回傳 fields := []graphql.Field{ {Name: \u0026#34;title\u0026#34;}, // 額外的欄位，算是 metadata {Name: \u0026#34;_additional\u0026#34;, Fields: []graphql.Field{ {Name: \u0026#34;id\u0026#34;}, {Name: \u0026#34;distance\u0026#34;}, }}, } // GraphQL Request result, err := client.GraphQL().Get(). WithClassName(className). WithNearText(nearText). WithWhere(where). WithFields(fields...). Do(context.Background()) if result.Errors != nil { for _, err := range result.Errors { fmt.Println(err.Message) } return } r := result.Data[\u0026#34;Get\u0026#34;].(map[string]interface{})[className].([]interface{}) jsonbody, err := json.Marshal(r) if err != nil { // do error check fmt.Println(err) return } books := []Book{} if err := json.Unmarshal(jsonbody, \u0026amp;books); err != nil { // do error check fmt.Println(err) return } for _, book := range books { fmt.Println(book) } 在 searching 搜尋中，如果是 text Weaviate DB 會用 inverted index 與 vector 搜尋 WithNearText，找出的結果會給出對應的 distance\n1 2 3 4 5 6 7 8 nearText := client.GraphQL().NearTextArgBuilder(). WithConcepts(concepts). WithMoveTo(\u0026amp;graphql.MoveParameters{ Force: 0.5, Concepts: []string{ \u0026#34;Yellow\u0026#34;, }, }) concepts 搜尋的文字陣列 WithMoveTo 額外指定搜尋要特別接近哪些關鍵字\n參數可參考 Vector search parameters 也可以針對屬性做篩選，例如我只要 object 中 type = program 的資料\n1 2 3 4 where := filters.Where(). WithPath([]string{\u0026#34;type\u0026#34;}). WithOperator(filters.Equal). WithValueText(\u0026#34;program\u0026#34;) 搜尋的結果大概是 (Yellow 被篩選掉)\n{Hello World Blue {b9f93a34-6cbd-45b3-afc3-f82e9d1d0da8 0.54240143}}\n{Hello World Red {6575290e-53cc-4ab9-8d39-330e5a47b0d1 0.55338395}}\nANN 演算法：HNSW 介紹 HNSW (Hierarchical Navigable Small World) 是一種 ANN 演算法，主要是借鏡 借鏡 probability skip list，透過分層查詢，先從最上層最稀疏的 layer 開始找最相近的鄰居，接著往下一層找該鄰居的最相近鄰居，一直到最底層 (layer 0)\n查詢複雜度降至 log (N)\n這部分有兩個參數可以注意\nefConstruction: 決定每次查詢回傳的鄰居數量，數量越多查詢越精準，但是效率越差 maxConnections: 決定每個 point 可以連接的 edge 數量，同樣是數量越多越精準 如何決定 point 要插入哪一層 這部分便是透過機率所決定，越上層機率越低 結語 快速瞭解了一下 Vector DB，未來如果有需要做向量查詢使用上應該會蠻方便的，之後有機會在評估一下 Milvus，看起來基礎建設比 Weaviate DB 更完善、Github 上熱度也更高、也支援更多的 ANN 演算法\n另外 Elasticsearch 在 8.0 也有支援 vector search，如果以經有在用 ES 也可以直接當作 Vector DB 使用\n","date":"2023-04-24T00:21:40.869Z","permalink":"https://yuanchieh.page/posts/2023/2023-04-24-vector-db-%E5%88%9D%E6%8E%A2%E8%88%87-weaviate-db-%E6%95%99%E5%AD%B8/","title":"Vector DB 初探與 Weaviate DB 教學"},{"content":"在套用 Clean Architecture (後續簡稱 CA)過程，最常討論的問題莫過於「Transaction 如果跨多個 repository，該怎麼處理？如果 Transaction 由 Use Case 控制會不會違反 CA 原則？如果放到 Repository 那有一些判斷的邏輯是不是也混雜進去？」\n這個問題確實有點棘手，網路上也常看到各種不同的作法，決定今天重新整理一下，檢視不同的作法與考量，並透過實際的案例去驗證不同作法的優劣，使用動態語言 Ruby / 靜態語言 Golang 確保實作是真實可行\n目前想到的驗證場景為\n「用戶」帳號有點數，透過點數購買「商品」並成立對應的「訂單」 商品必須有足夠數量、用戶點數必須有足夠的點數才可以成立訂單 以上行為必須包含在一個 transaction 中\n外部的幾種做法 1. 由 UseCase 控制，因為 Usecase 才知道所有的 Context 這部分說法是從《Clean Architecture 實作篇》第 84 頁所截取的內容，作者提到只有 Usecase 有足夠的上下文去判斷這幾個 repository 操作是否該放到同一個 transaction，範例 code 如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 @UseCase @Transactional public class SendMoneyService implements SendMoneyUseCase { @Override public boolean sendMoney(SendMoneyCommand command) { .... AccountId sourceAccountId = sourceAccount.getId() .orElseThrow(() -\u0026gt; new IllegalStateException(\u0026#34;expected source account ID not to be empty\u0026#34;)); AccountId targetAccountId = targetAccount.getId() .orElseThrow(() -\u0026gt; new IllegalStateException(\u0026#34;expected target account ID not to be empty\u0026#34;)); accountLock.lockAccount(sourceAccountId); if (!sourceAccount.withdraw(command.getMoney(), targetAccountId)) { accountLock.releaseAccount(sourceAccountId); return false; } accountLock.lockAccount(targetAccountId); if (!targetAccount.deposit(command.getMoney(), sourceAccountId)) { accountLock.releaseAccount(sourceAccountId); accountLock.releaseAccount(targetAccountId); return false; } ..... } } 作者直接使用 framework 提供的 annotation @Transaction 把所有的操作都放到同一個 Transaction 中\n2. 應該放到 repository 中，有其他的需求用 callback function 補充 這是我在 Coscup 聽到的 golang 版本 CA 實作，作者一開始想要的解法是usecase 控制 lock，repository 控制 transaction\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 func (s *BarterService) ExchangeGoods(ctx context.Context, param ExchangeGoodsParam) common.Error { // 1. Claim an event to exchange goods X and Y s.lockServer.claim(X, Y, ttl) // 2. Check ownership of request good // 3. Check the target good exist or not // 4. Exchange ownership of two goods // ....... // 5. Disclaim the event s.lockServer.disclaim(X, Y) return nil } func (r *PostgresRepository) CheckOwnerIDsAndUpdateGoods(ctx context.Context, param CheckOwnerIDsAndUpdateGoodsParam) (updatedGoods []barter.Good, err common.Error) { tx, err := r.beginTx() if err != nil { return nil, err } defer func() { err = r.finishTx(err, tx) }() // 1. Get goods again if _, err = r.getGoodByIDandOwnerID(ctx, tx, param.G1.ID, param.G1.OnwerID); err != nil { // ...} if _, err = r.getGoodByIDandOwnerID(ctx, tx, param.G2.ID, param.G2.OnwerID); err != nil { // ...} // 2. Update Goods for i := range goods { updatedGood, err := r.updateGood(ctx, tx, param.updateGoods[i]) if err != nil { return nil, err } updatedGoods = append(updatedGoods, *updatedGood) } return updatedGoods, nil } 但在這個 issue 中有人補充不同的作法 Is there a race condition bug?，另一個人提到 lock 比較不像商務邏輯，比較像實作的細節，所以他會想要放到 repository 中，而商務邏輯就用 callback function 方式傳入\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 func (s *BarterService) ExchangeGoods(ctx context.Context, param ExchangeGoodsParam) common.Error { g1, g2, err := s.goodRepo.UpdateTwoGoods(ctx, param.G1, param.G2, func(g1, g2 barter.Good) common.Error { // 1. Check ownership of request good // ...... // 2. Check the target good exist or not // ...... // 3. Exchange ownership of two goods // ...... return nil }) return err } func (r *PostgresRepository) UpdateTwoGoods( ctx context.Context, id, id2 string, updateFn func(*barter.Good, *barter.Good) (*barter.Good, *barter.Good, common.Error), ) (barter.Good, barter.Good, err common.Error) { tx, err := r.beginTx() if err != nil { return nil, err } defer func() { err = r.finishTx(err, tx) }() // 1. Fetch required data if g1, err = r.getGoodByID(ctx, tx, id); err != nil { // ... } if g2, err = r.getGoodByID(ctx, tx, id2); err != nil { // ... } // 2. Apply business logic (usecase) if g1, g2, err = updateFn(g1, g2); err != nil { // ... } return g1, g2, nil } 重新思考 重新抽象化一下遇到的困境\n1 2 3 4 5 usecase 1. 從 repo 讀取，同時要 lock 2. 針對讀取的數值判斷 3. 根據判斷，將結果寫回 repo 4. 以上三步要在同一個 transaction 重新審思 CA 的條件，有幾點規則我們應該要遵守\nrepository 應該只包含儲存層的操作，不應該有商務邏輯 usecase 不應該知道太多 repository 細節，或換個角度，當抽換 repository 時應該要很容易，不改變 usecase 的實作 根據以上的條件，來測試兩種方式\nusecase 控制 lock 與 transaction repository 用 callback function 注入商務邏輯 兩者實作起來的感覺 實作比較 參考程式碼 clean-architecture-transaction-issue\n方法一：use case 控制 transaction 與 lock 首先在 usecase 中控制\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def run(user_id, product_id, count) user_repository.transaction() user = user_repository.find_by_id_with_lock(user_id) product = product_repository.find_by_id_with_lock(product_id) total = product.price * count return unless user.can_purchase?(total) return unless product.can_purchase?(count) # to test race condition sleep 1 user.points -= total product.stock -= count order = Order.new(user, product, count) user_repository.save(user) product_repository.save(product) order_repository.create(order) user_repository.commit() end 這邊暴露了蠻多關於 DB 的細節，包含 transaction / lock / commit，但所有的商業邏輯也都在 usecase 中；\n但整體傷害應該不算到太嚴重，主要是也沒有直接跟 DB 耦合，如果抽換成其他 NoSQL 頂多 transaction / commit method 留空，不會直接違反 CA 依賴方向的原則\n方法二：由 repository 控制 transaction 由 repository 控制 transaction，商務邏輯用 block 方式傳入，其餘的邏輯都在 repository 中\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # usecase def run_in_repo(user_id, product_id, count) aggregate_root_repository.purchase_product(user_id, product_id, count) do |user, product| total = product.price * count next false unless user.can_purchase?(total) next false unless product.can_purchase?(count) true end end # repository class AggregateRootRepository \u0026lt; Repository def purchase_product(user_id, product_id, count) transaction result = client.prepare(\u0026#34;SELECT * FROM users WHERE id = ? FOR UPDATE\u0026#34;) .execute(user_id) .first user = User.new(result[\u0026#39;id\u0026#39;], result[\u0026#39;points\u0026#39;]) result = client.prepare(\u0026#34;SELECT * FROM products WHERE id = ? FOR UPDATE\u0026#34;) .execute(product_id) .first product = Product.new(result[\u0026#39;id\u0026#39;], result[\u0026#39;price\u0026#39;], result[\u0026#39;stock\u0026#39;]) is_pass = yield(user, product, count) return unless is_pass client.prepare(\u0026#34;INSERT INTO orders (user_id, product_id, count) VALUES (?, ?, ?)\u0026#34;) .execute(user.id, product.id, count) total = product.price * count client.prepare(\u0026#34;UPDATE users SET points = ? WHERE id = ?\u0026#34;) .execute(user.points - total, user.id) client.prepare(\u0026#34;UPDATE products SET stock = ? WHERE id = ?\u0026#34;) .execute(product.stock - count, product.id) commit end end 這邊的商務邏輯只有判斷是否可以購買，其餘的 DB 操作封裝在 repository 中，這邊取名叫做 Aggregate Root 是想呼應 DDD 裡面的想法「由 Aggregate Root 操作保證底下多個 Aggregate 的一致性」，靈感是來自之前上 Teddy 的課程與 FB 討論\n比較兩者差異 usecase 乾淨程度(方法二勝)：\n蠻明顯作法很乾淨，把所有的 DB lock / transaction 都封裝得一乾二凈，而商務邏輯還是保留在 usecase 中呼叫 擴充性(方法一勝)：\n如果未來商務邏輯變得更複雜，有可能 DB 查完資料要增加新的比對，例如「高級會員有更多的折價」、「特殊商品買 10 送 1」等等，方法二需要不斷的增加 function 傳入，而方法一因為都是在 usecase 操作，所以直接增加就好，相對好擴充 可測試性(方法一勝)： 我覺得兩個作法最大的差異是可測試性，測試可以用 integration test 連 DB 一起測 / 或是 unit test 把其他相依的物件 mock 掉； integration 測試部分兩者沒太多差異 (參考 main_spec.rb)；\n但是 unit test 就有很大的差異 (參考 usecase.rb)，因為方法一的 repository 方法都是 public 讓 usecase 呼叫，所以很好 mock，但方法二目前我想不到比較好的方法測試，因為 callback function (block) 是在 repo 中呼叫，那我直接 mock 掉不就什麼都測不了了 ?!! 結論 即使 usecase 會比較混亂一些，我還是選擇方法一\n(golang 版本待補)\n","date":"2023-04-21T02:21:40.869Z","permalink":"https://yuanchieh.page/posts/2023/2023-04-21-%E5%9C%A8-clean-architecture-%E4%B8%8B-transaction-%E8%A9%B2%E5%A6%82%E4%BD%95%E5%AF%A6%E4%BD%9C%E7%9A%84%E5%95%8F%E9%A1%8C%E7%99%BC%E6%83%B3-golang-%E8%88%87-ruby-%E5%AF%A6%E4%BD%9C/","title":"在 Clean Architecture 下 transaction 該如何實作的問題發想 (Golang 與 Ruby 實作)"},{"content":"有好一陣子覺得自己花了蠻多時間在學習上，假日也會規劃讀書寫程式，但總覺得自己並沒有真正積累或很巨幅的成長，這些努力與付出好像是在滿足自己的焦慮，而不是真正的產生改變與成就\n直到看了《最有生產力的一年》，才發現自己之前在安排學習項目與規劃上，有了思維上的誤差，試著套用書中的作法，逐步調整出自己實踐的方式，覺得生產力上開始有一些正向的變化\n以下分享書中的一些觀念與做法，並分享自己如何實踐與一些感想\n三個觀念 觀念一：生產力的重點是「成就」，而不是完成多少事 作者分享到一提到生產力大家關注的往往是我如何在一定的時間完成多少事，但真正重要的是「你完成的多少的成就」 (p19)\n成就與一般事項的區分在於成就是產生重大改變與意義的事情，就好比說急迫性與重要性矩陣，如果你一直在處理緊急卻不重要的雜事，例如回回 Email 、參加不必要的會議，看起來完成很多事但真正重要的事情都沒有被處理，所以作者提到重點要放在「成就」而非完成事項的數量\n必須意識到：不是每件事都同等的重要\n觀念二：成就是要連結自己的價值觀，找出重要且有動力的事項 延續成就的定義，這會依照每個人的價值觀而有所不同，而找出自己人生的價值觀是一件很重要但我覺得不是這麼簡單、具體的事情\n作者這邊提供一個有趣的思考方式「想像自己突然每天多了兩小時，你會拿來做什麼？」 提高生產力無非是想為自己賺到更多的時間與精力，那這多出來的時間怎麼運用，就會反映自己的價值觀與排序方式 (p33)\n另一個建議是可以居高臨下，從生活的不同面向去反思，如頭腦(學習)、身體狀態、情感、職涯、財務、人際關係、休閒樂趣 (p208)，這邊呼應到另一篇寫日記的方式 朱騏-寫日記的策略 1 : 日記到底要寫哪些內容？，用完整的角度去檢視比較不會有遺漏\n觀念三：終結時間管理，精力與專注力才是重點 作者提到時間經濟是因為工廠的興起，人們的收入直接與工作時間掛鉤，做多少時間拿多少報酬\n但現在的社會是注重產出，能夠一個小時完成的成就為什麼要用兩個小時 ?!，而要在更短的時間做到相同的成就，就需要高度的精力與專注力的投入，這正也是我們所能控制的\n作者曾經實驗過一週工作 90 小時 vs 一週工作 20 小時，發現 90 小時的產出只比 20 小時多一些，原因有幾個\n時間太長生產力會嚴重下滑 嘗試壓縮目標時間，反而會讓自己用更高的專注力完成 切記，忙碌不等於有效率\n之前的我有點踩到著個坑，只關注到自己花很多時間，卻沒意識到自己專注力與生產力，導致在瞎忙\n三個實踐方式 作者分享了 20 多個改善生產力實驗，這邊我挑 3 個覺得最有效果的\n實踐一：找出生理黃金時段 延續觀念三，配合個人的作息，人每天的精力與專注力在不同的時間點都會有不同的表現，所以要找出自己的 黃金時段，將最重要的事情在最佳的時段處理，這就是所謂的管理精力與專注力\n作者是建議花一週的時間，每個小時就紀錄 1) 精力與專注力的狀態 2) 完成的事項 3) 是否有拖延的狀況 (p67) 追蹤一段時間，就能掌握自己的生理狀況\n另外有幾個方法可以改善自己的生理狀況，例如 睡到自然醒 / 戒糖 / 運動 / 冥想等，這部分比較為人熟知就不另外贅述\n實踐二：安排最重要的三件事 每天不要安排太多的事情，只要安排3~5 件重要的事就好，這個數量的事項可以記在腦中，就不用額外的任務管理工具 (p51)\n這點個人也非常有感，之前曾經試著用 trello / asana / google calendar 想要去管理個人工作事項，發現到頭來花在任務事項規劃的精力遠高於實際執行的投入，這點也回呼到觀念一，我真正要成就的應該是工作項目，而不是規劃工作項目本身\n實踐三：區分時間管理 - 創作者與管理者 觀念三提到時間管理相對不是這麼重要，心中可能會有個疑惑「我在公司就有很多會議要開，真的不需要時間管理嗎？」這邊作者引述 YC 創辦人 Paul Graham 的時間管理方式 manager\u0026rsquo;s schedule and the maker\u0026rsquo;s schedule，他將時間管理分成兩種模式 管理者模式與創作者模式\n管理者模式：需要與他人時常開會、回覆 Email 的管理類型事項，通常會以一個小時為單位切分，並排入不同的議程 創作者模式：專注於自己的產出，例如作家或程序員，通常會以更大的時間區段 (一個上午或下午) 安排自己的工作 要先識別自己屬於哪種類型的工作者，接著用對應的方式安排工作項目，也有可能是混合模式，例如作者分享他早上是創作者模式，下午採用管理者模式\n這邊延伸討論 Paul Graham 原文 中幾個有趣的論點\n身為創作者，不要小看一個小小的會議怎麼打斷你的工作流，試著想想當某一天都沒有開會那生產力是多麼驚人 I find one meeting can sometimes affect a whole day. A meeting commonly blows at least half a day, by breaking up a morning or afternoon. (\u0026hellip;..) Don\u0026rsquo;t your spirits rise at the thought of having an entire day free to work, with no appointments at all?\n如果你是管理者要帶領一群創作者，要小心自己的安排會議的方式是不是恰當的 Each type of schedule works fine by itself. Problems arise when they meet. Since most powerful people operate on the manager\u0026rsquo;s schedule, they\u0026rsquo;re in a position to make everyone resonate at their frequency if they want to.\nPaul Graham 在管理 YC 投資的團隊，是透過 office hour 提供投資的團隊主動預約時段，這樣團隊可以主動選擇適合的時間 (maker style)，而不是被動的接受預約 (manager style)；\n對於 Paul Graham 本人來說，他把所有的 office hour 都排在自己工作日的最後，切分出自己的管理者與創作者時間 By using the classic device for simulating the manager\u0026rsquo;s schedule within the maker\u0026rsquo;s: office hours. \u0026hellip;. These chunks of time are at the end of my working day. (\u0026hellip;..) Because they come at the end of my day these meetings are never an interruption.\n自我實踐 以下分享我自己的生產力管理方式，主要透過 Heptabase 這套筆記軟體，使用 Heptabase 的好處是，我可以用圖形化的方式一目瞭然相關的紀錄\n我主要會分成 年 / 季 / 月 / 週 / 日，我會希望四者的目標是環環相扣，當我在安排每日工作事項，會想要知道這有沒有緊扣我每週的目標，而每週的目標制訂時要緊扣每月的目標，以此類推\n目標管理 在制定每年的目標上，去年我跟老婆花了一天的時間，採用 Year Compass模板，我覺得蠻不錯的，覆蓋的範圍蠻完整\n在制定年度目標我目前還沒有很清晰的畫面，但會試著用比較具體的描述去補充，例如說今年年度目標有一個是「離開現在的公司，去更有制度的地方」，當初設立時我心中沒有一個明確的夢想公司，只用更有制度去涵蓋，但我有補充幾個細項去描繪，包含\n產品發展的制度，是否有清楚的 product roadmap、在產品的功能開發週期是否完整 專案管理流程，是否一堆隕石跟 BDD (Boss Driven Design) 針對每日任務上，我是直接用 Heptabase 最近支援的 Journal card 去規劃 拆成兩個時段工作前與工作中，各安排 3 ~ 5 個項目 提前展開未來幾天的卡片，想到就先寫成具體的執行項目，不要當天要執行時才想實作的細節\n例如說今天安排刷題，可以的話前一天就把題目挑出來 / 如果今天要看書，就先把書的頁碼或章節標記出來，有具體的項目讓執行時會更順暢 把卡片攤開，當發現自己有點發散時提醒自己回來看卡片的內容 每天 review，簡單紀錄自己執行狀況，以及花費在每個任務上的時間 代辦事項管理 蔡加尼克效應(Zeigarnik effect) : 不完整或被中斷的任務，會帶給我們很大的心理壓力\n這是個蠻有趣的心理實驗，蔡加尼克發現餐廳服務生對於未結帳的訂單有很好的記憶，但只要訂單一結帳完成，就會很難回想起\n作者提到當心中有一個想法或代辦事項時，人們會很怕忘記，所以會一直給自己心理壓力，導致大腦的思緒也受到影響，此時最好的方式是寫下來，當大腦意識到這件事已經被紀錄，就會釋放心理壓力，更專注在其他事情上\n這個理論在《卡片盒筆記》也有提起，我目前的做法是當研究某個主題出現有趣的分支時，先開成卡片標記成 todo，接著先專注在原本的主題研究，避免太過於發散\n而這個 todo 清單就會變成我安排工作時可以檢索的項目，Heptabase 有 tag 的功能 (大部分的筆記軟體應該都會有) 另外推薦 tag 的用法，我是參考 電腦玩物 - 2021覆盤：如何用標籤管理上萬則筆記？Evernote, Notion, obsidian 都適用，tag 應該是以功能性為導向而非內容，todo 就是一種功能性，代表尚未完成的代排事項\n除了打上 todo tag，我會另外標記任務的類型與難易度，主要是我會希望均衡的執行，我不希望某個禮拜都在閱讀而沒有產出、或是都挑簡單的任務而沒有高難度的任務執行\n實踐後的反思 不要高估一天所能達到的成就，也不要低估一年所能累積的效果。\n在套用任務規劃的這段時間，又在重新體會了這句話，我發現我在制定年、季的目標時會訂的太小，而在月、週、日又企圖一次做到太多事，尤其是一開始每日任務都開太大，導致不斷地延期\n目前試跑了三週，預計會調整了幾個方向\n目標加上預估時間與實際花費時間，去收斂自己可以完成的項目 保留彈性，除了安排一整天都不要工作外，再排另一天不要預先規劃項目，保留彈性給自己消化延遲的項目 自我 retro，增加自己喜歡 / 不喜歡 / 想要 / 不想要的環節，去更近一步知道自己適合的項目 「拉高自己的格局，為自己設定適合的年度與季度目標」這件事我還在摸索，目前還是以自己不足的知識漏洞去加強為主，希望透過持續的 retro 可以收斂出適合自己的長遠目標\n總結 優化自己的生產力會是一個持續的過程，《最有生產力的一年》這本書我覺得提供一個很好的基底，可以在這之上打造出屬於自己的生產力流程，未來會持續的優化，有什麼有趣的發現會持續的迭代，如果有不同的做法或是工具的推薦，也歡迎留言~\n","date":"2023-04-01T02:21:40.869Z","permalink":"https://yuanchieh.page/posts/2023/2023-04-01-%E6%9C%80%E6%9C%89%E7%94%9F%E7%94%A2%E5%8A%9B%E7%9A%84%E4%B8%80%E5%B9%B4%E8%AE%80%E6%9B%B8%E5%BF%83%E5%BE%97%E8%88%87%E5%80%8B%E4%BA%BA%E5%AF%A6%E8%B8%90/","title":"《最有生產力的一年》讀書心得與個人實踐"},{"content":"前言 公司當前是透過 GitOps ArgoCD 這套工具來管理持續部署的流程，ArgoCD 有個方便的 hook 機制 pre sync 和 post sync，可以在機器部署前與部署後執行對應的腳本\n在我們原本的 Rails 部署機制，會於 pre sync 階段執行\ndb:migration：負責 DB schema 改動 data:migration：data-migrate 負責跑一些 Rails script 回補資料等 避免 API server 上線因為有髒資料或錯誤的 DB schema 而執行錯誤 1 2 3 4 . └── db/ ├── data =\u0026gt; data migration at presync └── migrate =\u0026gt; schema change 但有時候我們會需要在機器部署完成後觸發 script，例如說當我們新增了 sidekiq job 需要觸發時，必須等到 sidekiq server 都更新到最新的版本才可以觸發，否則會 sidekiq server 會認不得新的 job\n簡而言之，我們會需要在 pre sync 有一套 data-migration，在 post sync 也需要在一套 data-migration，但同一套 data-migration 是不能直接用在兩個觸發點，執行時會無法區分哪些 migration script 該在什麼時間點觸發\n化成具體的需求是提供一套基於 data-migration 的 post sync 觸發機制，希望封裝出類似於 data-migration 的效果\n透過指令可以簡單產生 migration script template $ $ bin/rails generate post_sync hello_post_sync 執行指令 $ bin/rake post_sync:migrate 紀錄在 DB 中避免 script 反覆執行 不影響現有的 pre sync 機制 1 2 3 4 5 . └── db/ ├── data =\u0026gt; data migration at presync ├── migrate =\u0026gt; schema change └── post_sync =\u0026gt; data migration at post sync 以下將介紹透過 rails generator 簡單魔改達到我們要的效果\n實作 1. 透過 Generator 產生 migration script template 透過 Rails Generator 基於模板產生對應的檔案，透過 generator 產生 generator $ bin/rails generate generator initializer\n在 generator 中可以定義產生檔案的方式，也可以從 CLI 吃不同的參數\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class InitializerGenerator \u0026lt; Rails::Generators::NamedBase source_root File.expand_path(\u0026#39;templates\u0026#39;, __dir__) def copy_initializer_file copy_file \u0026#34;initializer.rb\u0026#34;, \u0026#34;config/initializers/#{file_name}.rb\u0026#34; end def create_helper_file create_file \u0026#34;app/helpers/#{file_name}_helper.rb\u0026#34;, \u0026lt;\u0026lt;-FILE module #{class_name}Helper attr_reader :#{plural_name}, :#{plural_name.singularize} end FILE end end file_name 是從 Rails::Generators::NamedBase 繼承而來，從 CLI 讀取，例如 $ bin/rails generate initializer core_extensions file_name 就是 core_extensions copy_file 複製檔案 create_file 直接輸入檔案內容 其他更多有趣的 method 10 Generator methods Generator 實作 目標是打造一個類似 data-migrate 的 generator，儲存在獨立的 folder 下，並有著 timestamp 結尾的檔案\n在閱讀 data-migrate 原始碼時，看到可以針對 data_migrations script 的 folder 進行調整\n1 @data_migrations_path = \u0026#34;db/data/\u0026#34; 所以靈機一動就想說在 generator 中如果可以動態置換，並觸發 data-migrate 就可以產生新的檔案到對應的資料夾下，而不會污染於本的 data-migrate\n1 2 3 4 5 6 7 8 9 10 # lib/generators/post_sync_generator.rb class post syncGenerator \u0026lt; Rails::Generators::NamedBase def copy_initializer_file DataMigrate.configure do |config| config.data_migrations_path = POST_SYNC_PATH end Rails::Generators.invoke(\u0026#34;data_migration\u0026#34;, [file_name]) end end data-migrate 本身就有提供 data_migration 的 generator，而且可以直接在 generator 中觸發另一個 generator，再加上 file_name 可以從 CLI 讀取，這樣就可以串起來\n成果是輸入 $ rails generate post_sync xxxx_xxx，會在指定的路徑下產生\n1 2 3 4 5 6 7 . └── rails/ └── db/ ├── data/ │ └── 20210408091311_migration.rb └── post_sync/ └── 20210429100009_post_sync.rb 2. 註冊 Rake 指令 當我們希望透過 CLI 觸發特定指令時，可以透過 Rake，處理任務與任務間相依性的 gem 套件\n這邊的用法相當簡單，同樣去改 migrations_path，並觸發原本 data migrate 的 Rake task 即可\n1 2 3 4 5 6 7 8 9 10 11 # lib/tasks/post_sync.rake namespace :post_sync do task :migrate do DataMigrate.configure do |config| config.data_migrations_path = POST_SYNC_PATH end Rake::Task[\u0026#39;data:migrate\u0026#39;].invoke end end 3. 避免 timestamp collision 前面我們透過 generator 與 rake 成功產生 migration script template 與執行 post sync，並且在檔案路徑上與原本的 data migration 分開\n但目前還有個問題是最後 data-migrate 紀錄 script 是否曾經跑過還是在同一張 DB table 中，這邊沒有 config 可以直接調整\n後來跟同事討論後，決定手寫一個檢查在 CI 執行時確認 db/data 跟 db/post_sync 下的 timestamp 沒有重複\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class post syncChecker CHECK_FOLDER = [ DataMigrate.config.data_migrations_path, POST_SYNC_PATH ] class \u0026lt;\u0026lt; self def is_collision? (pre sync_versions, post sync_versions) = CHECK_FOLDER.map do |path| list_migration_versions(path: File.join(Rails.root, path)) end puts pre sync_versions (pre sync_versions \u0026amp; post sync_versions).present? end def list_migration_versions(path:) Dir.entries(path) .map { |filename| parse_version(filename: filename) } .compact end # ref: https://github.com/ilyakatz/data-migrate/blob/2dd90c495b4d57e4dc700e3f9be149c7e2b93b57/lib/data_migrate/data_migrator_five.rb#L44 def parse_version(filename:) result = /(\\d{14})_(.+)\\.rb$/.match(filename) return unless result.present? result[1] end end end 夾在測試中執行\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # spec/unit/lib/post_sync_checker_spec.rb require \u0026#39;rails_helper\u0026#39; describe \u0026#34;post syncChecker\u0026#34; do let(:version) { 12345678901234 } let(:file_paths) { ::post syncChecker::CHECK_FOLDER.map do |path| File.join(Rails.root, path, \u0026#34;#{version}_test.rb\u0026#34;) end } context \u0026#34;when there are two file with same version\u0026#34; do before do file_paths.each do |file_path| File.new(file_path, File::CREAT) end end after do file_paths.each do |file_path| File.delete(file_path) end end it \u0026#39;should failed\u0026#39; do expect(::post syncChecker.is_collision?).to eq(true) end end it \u0026#34;check when running test\u0026#34; do expect(::post syncChecker.is_collision?).to eq(false) end 總結 這是一個小小的工具，不過在對於整個部署流程有還不錯的幫助\n","date":"2023-03-28T02:21:40.869Z","permalink":"https://yuanchieh.page/posts/2023/2023-03-28-rails-%E9%83%A8%E7%BD%B2%E5%84%AA%E5%8C%96-%E9%AD%94%E6%94%B9-data-migration-%E5%AF%A6%E4%BD%9C-post-sync-%E6%A9%9F%E5%88%B6/","title":"Rails 部署優化: 魔改 data-migration 實作 post sync 機制"},{"content":"前言 Elasticsearch 是常用來做全文搜尋的 NoSQL Database，公司在使用上有自架也有用 AWS Opensearch 託管服務 要特別留意的是 Opensearch 是 AWS 自己 fork 維護，兩者不完全兼容，至少在 client sdk 連線是不兼容的! 原本使用 Golang Elasticsearch 官方 SDK v7.17 要連線 Opensearch 回直接拋錯\nerror: the client noticed that the server is not a supported distribution of Elasticsearch\n一些相關的文章 [Elasticsearch] The Server Is Not A Supported Distribution Of Elasticsearch\nSince the AWS Elasticsearch Service has incompatible APIs with Elasticsearch itself, either by missing APIs or has incompatible format, we do not support it.\nBeef 應該可以往前追溯幾年前 Elasticsearch 覺得 AWS 在白嫖開源社群進而更改授權，使得 AWS 決定自己維護 Opensearch (新聞 AWS分叉Elasticsearch重新命名為OpenSearch)\n但有趣的是我測試了一些場景，AWS Opensearch client SDK 可以連線 Elasticsearch (以下簡稱 ES) 與 Opensearch server，所以以下就用 Opensearch cliend SDK 操作 ES，後續不會針對 ES 本身有太多介紹，可以參考之前的幾篇文章 Elasticsearch 教學 - API 操作 和 Elasticsearch 系統介紹與評估\nClient SDK 使用 以下使用會覆蓋幾個場景\n建立 Client 連線 建立 Index 插入 Document 搜尋 Document 建立 Mapping 刪除 Index 完整原始碼參考 https://github.com/sj82516/elasticsearch-golang，或是參考 Opensearch 官方文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 func main() { // connect to elasticsearch client, _ := opensearch.NewClient(opensearch.Config{ Addresses: []string{ \u0026#34;http://localhost:9200\u0026#34;, }, }) // create index res, _ := client.Indices.Create(index) fmt.Println(res) // create document createThenSearch(res, client) // refresh client.Indices.Refresh() } func createThenSearch(res *opensearchapi.Response, client *opensearch.Client) { // create document createDocument(res, client, `{\u0026#34;key\u0026#34;: \u0026#34;key1\u0026#34;, \u0026#34;title\u0026#34;:\u0026#34;Test my first document\u0026#34;, \u0026#34;number\u0026#34;: 5}`) createDocument(res, client, `{\u0026#34;key\u0026#34;: \u0026#34;key1.child\u0026#34;, \u0026#34;title\u0026#34;:\u0026#34;Test my first document\u0026#34;, \u0026#34;number\u0026#34;: 5}`) // refresh client.Indices.Refresh() // search document by text field r := search(res, client, \u0026#34;title\u0026#34;, \u0026#34;document\u0026#34;) fmt.Println(\u0026#34;search by title:\u0026#34;, r) r = search(res, client, \u0026#34;key\u0026#34;, \u0026#34;key1\u0026#34;) fmt.Println(\u0026#34;search by key:\u0026#34;, r) r = search(res, client, \u0026#34;key\u0026#34;, \u0026#34;child\u0026#34;) fmt.Println(\u0026#34;search by key:\u0026#34;, r) } func createDocument(res *opensearchapi.Response, client *opensearch.Client, document string) { req := opensearchapi.IndexRequest{ Index: index, Body: strings.NewReader(document), } res, _ = req.Do(context.Background(), client.Transport) } type searchResponse struct { Hits struct { Total struct { Value int `json:\u0026#34;value\u0026#34;` } `json:\u0026#34;total\u0026#34;` Hits []struct { Score float64 `json:\u0026#34;_score\u0026#34;` Source struct { Key string `json:\u0026#34;key\u0026#34;` Title string `json:\u0026#34;title\u0026#34;` Number int `json:\u0026#34;number\u0026#34;` } `json:\u0026#34;_source\u0026#34;` } `json:\u0026#34;hits\u0026#34;` } `json:\u0026#34;hits\u0026#34;` } func search(res *opensearchapi.Response, client *opensearch.Client, key string, value string) searchResponse { s := map[string]interface{}{ \u0026#34;query\u0026#34;: map[string]interface{}{ \u0026#34;match\u0026#34;: map[string]interface{}{ key: value, }, }, } body, _ := json.Marshal(s) searchReq := opensearchapi.SearchRequest{ Index: []string{index}, Body: bytes.NewReader(body), } res, _ = searchReq.Do(context.Background(), client.Transport) var r searchResponse json.NewDecoder(res.Body).Decode(\u0026amp;r) return r } 以上是大致的 API 操作，呼叫起來不太麻煩，只是文件有點簡陋需要不停的查找，建議可以開著 Kibana 的後台對應查詢與回應，有 hint 蠻方便的 有兩個地方需要特別留意\n如果是本地端測試，記得在 create document 後 client.Indices.Refresh() 強制 refresh index，因為 ES 收到建立請求後需要一段時間處理才能夠查詢，所以要強制 refresh 才能直接查! 預設 ES 的 string 輸入都會是 text 型別，而 text 型別會經過 tokenize 、analyzer 加工後支援全文搜尋，這其中的處理包含了去除冗詞贅字、斷詞等等；\n所以像是我原本預期 key 這個欄位是完全匹配，也就是查詢時要完整的命中，但因為預設是全文搜尋，所以會把沒有完全匹配的結果也回傳，如圖下我查詢 \u0026ldquo;key\u0026rdquo;=\u0026ldquo;key1\u0026rdquo;，結果連 \u0026ldquo;key1.child\u0026rdquo; 都回傳了 如果要解決這個問題的話，需要在 Index 建立後增加 Mapping，Mapping 是指定 Index 每個欄位的處理方式，可以切換不同的型別、指定是否要被 indexing 等\n1 2 3 4 5 6 7 8 9 10 func createMapping(res *opensearchapi.Response, client *opensearch.Client) *opensearchapi.Response { mapping := `{\u0026#34;properties\u0026#34;:{\u0026#34;key\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;keyword\u0026#34;}}}` res, _ = client.Indices.Create(index) req := opensearchapi.IndicesPutMappingRequest{ Index: []string{index}, Body: bytes.NewReader([]byte(mapping)), } res, _ = req.Do(context.Background(), client.Transport) return res } 需要特別留意 Mapping 必須要在 Index 為空的情況下才能生效，如果已經有 document 就不行，需要重新建立\n以下查詢 \u0026ldquo;key\u0026rdquo;=\u0026ldquo;key1\u0026rdquo; 成功回傳一筆資料 ","date":"2023-03-17T02:21:40.869Z","permalink":"https://yuanchieh.page/posts/2023/2023-03-17-elasticsearch-%E6%93%8D%E4%BD%9C-golang-opensearch-sdk-%E4%BD%BF%E7%94%A8%E7%AD%86%E8%A8%98/","title":"Elasticsearch 操作： Golang Opensearch SDK 使用筆記"},{"content":"上一篇 驗證與授權的差別，淺談 OAuth 2.0 與 OpenID Connect 淺談到 OAuth 2.0 與 OIDC 的差異，我們是以 Client 的角度去理解如果像第三方取得驗證與授權，但如果今天我們要 Google 一樣自己處理授權的設計，思考的方式就會有所不同\n參考以下的文章，分享這幾天研究微服務下該如何設計驗證的機制\nBest Practices for Authorization in Microservices Zanzibar: Google’s Consistent, Global Authorization System How Netflix Is Solving Authorization Across Their Cloud [I] - Manish Mehta \u0026amp; Torin Sandall, Netflix Open Policy Agent 看到網路上有時會縮寫 (Authentication -\u0026gt; AuthN / Authorization -\u0026gt; AuthZ)，以下也會用此縮寫\n一. 所謂的授權驗證 Authorization 具體來說，授權設定主要是判斷\n{某人} 是否可以針對 {某資源} 執行 {某操作}\n現今有三種主流管理授權的方式\nRBAC ABAC ReBAC 1. RBAC 制定 Role 並綁定對應的權限，例如 AWS IAM，案例如 “如果用戶是 Admin 權限才可以瀏覽此頁面“\n但是權限就直接綁死 Role，如果同個 Role 下突然想要在拆分更細的控制，就要增加新的 Role\n2. ABAC 透過 Attribute 綁定對應的權限，例如 “如果用戶是 10 月以前註冊，可以享有 xxx 優惠“，Attribute 制定相對就彈性許多，AWS IAM 也可以用 ABAC 設定\n3. ReBAC 當角色或資源是有層狀的繼承關係時，可以透過描述 Relation 方式來制定權限，例如 “Group A 包含 Group B，小明是 Group B 成員，某個檔案是 Group A 所有成員都可以讀取，則小明也應該要可以讀取“\n二. 微服務下的驗證架構 在 monolithic 架構下，因為 DB 都在一塊，所以 authorization 可以直接 query 檢查即可，例如\n1 2 3 4 5 6 7 8 9 # 如果用戶是 admin if user.is_admin // xxxx end # 如果用戶是資源擁有者 if user.id == document.onwer_id // xxxx end 驗證變成是商業邏輯的一部分\n但如果今天是在 microservice 情況下，該如何判斷用戶是否有權限可以操作呢？ 大致有以下三種方法\n1. 將資料放在原位，透過 API 呼叫 假設今天獨立一個 document service，每個 document 隸屬於某個組織 org 下，而 org 管理是屬於 user service 的範疇 則 user service 可以開一條 API 專門查詢 user 所隸屬的 org 這是最簡單且有效的方式，但如果服務越變越多，則會有幾個問題\n服務間判端可能有重複呼叫 (例如 document 往上多一層 folder，變成要判斷 user 是否有權限操作 document 與 folder) API 呼叫會有延遲 如果驗證方式改變，多個 service 也要跟著改動 (例如 service A / service B 都是用 org 驗證，但之後突然變成要用 user 本身驗證) 2. 在 Gateway 注入授權驗證所需的資料 既然服務都需要 user org 這個資訊，那我們在 request 近來時就透過 gateway 把 user 資訊塞好塞滿，好處有\n減少多餘 request，所有下游的 service 都能讀取到 user org 參數 如果授權的參數很有限，例如只有切分幾種 role，那 gateway 會是最方便的 缺點是 Gateway 會需要知道所有 service 需要的資料，如果今天驗證方式改變 Gateway 也要跟著改變\n3. 獨立的授權驗證服務 有一個獨立的 AuthZ service，其他 service 收到 request 都直接向 AuthZ service 驗證授權，將驗證與商業邏輯拆開並集中管理授權邏輯\n特別適用於微服務眾多且需要互相溝通的情況 / 有第三方廠商要分享資料時，例如 Google / Airbnb / Netflix\n缺點是\nAuthZ service 與其餘 Service 的內容耦合，因為 AuthZ service 並須知道所有的 resource，才能對應設定權限，例如 document / folder 等 多一組服務要維護 大部分的公司應該都不需要如此複雜，通常也沒有獨立的團隊在維護驗證授權，但可以借鏡 Google 設計的 Zanzibar 與 CNCF 的開源專案 Open Policy Agent (OPA) 來看大型軟體採用的授權檢查方法\n三. Google 中心化 AuthZ 服務 - Zanzibar Zanzibar 是中心化的 AuthZ 服務，提供統一的 data model 與 config language (描述 ReBAC 的規則)，他有以下的表現\n效能：延遲 p95 \u0026lt; 10ms 規模：數十億的規則 / 每秒百萬等級 request，囊括 Calendar, Cloud, Drive, Maps, Doc 等 可用性：99.999% 實際設定可能像以下圖示 (非 google 官方，參考自 zanzibar.academy) 為什麼需要中心化的 AuthZ 服務 提供統一的語意和用戶體驗 當多個應用程式互交相錯時，更容易實作 例如發送 Gmail 時會幫你檢查附件中的 Google Doc 收件人是否有權限讀取\nTuple: 描述「物件與物件的關係」或「物件與用戶的關係」 Zanzinbar 有自己一套描述規則的語言，每條規則稱為一個 tuple，可以描述\nuser U has relation R to object O set of users S has relation R to object O 以下是論文的案例\ndoc:readme#owner@10 User 10 is an owner of doc:readme group:eng#member@11 User 11 is a member of group:eng\ndoc:readme#viewer@group:eng#member Members of group:eng are viewers of doc:readme\ndoc:readme#parent@folder:A#... doc:readme is in folder:A\n後續 Client 就可以透過 check API 指定 user + operation + object 請求驗證\n一些實作的困難與克服方式 通篇論文主要在講設計 Zanzibar 的挑戰，因為驗證服務需要滿足\nFlexibility 讓多種服務都能設計自己的驗證規則 Correctness 驗證是核心的用戶隱私保護所以判斷結果一定要正確 Low Latency 因為大多數的 Request 都需要驗證 Scale 因為 Request 會非常頻繁 (請求每秒百萬且橫跨整個地球) High Availability 因為每個服務都會依賴驗證服務 以下提幾點比較有趣的困難與克服方式\n1. 設定的最終一致性 當 ACL (Access Control List) 發生改變時，操作的順序必須嚴格遵守，否則會不小心把舊的 ACL 套用到新的物件上 / 舊 ACL 影響到新的內容 (同物件)，否則會有以下錯誤\nex1. Alice 移除 Bob 權限 (1)，此時新增物件 O (2) 繼承上層權限，則 Bob 不應該擁有 O 的權限 (3) 如果 2 比 1 先套用，則 Bob 就不小心有了 O 的權限 (2 \u0026gt; 1) ex2. Alice 移除 Bob 權限 (1)，此時物件 O 新增內容 (2)，則 Bob 不應該看到新的內容 (3) 如果 2 比 1 先套用，且 Bob 在 1 之前讀取到新的內容 (套用順序：2 \u0026gt; 3 \u0026gt; 1)，但理論上他不應該看到新的內容 以上問題稱為 new enemy problem，Zanzibar 透過 external consistency 與 snapshot reads with bounded staleness 解決此問題\nexternal consistency\n當操作 Tx 在操作 Ty 之前，則當 T 時看到 Ty 生效時，則 Tx 必定生效，Zanzibar 是透過 Google Spanner 儲存，仰賴 data storage 的特性保證 snapshot reads with bounded staleness\n當物件更新時 (ex2-2)，當下版本會對應一個稱為 zookie token 並記錄當下的時間；之後 client request 都必須帶上 zookie，只要讀取的時候比對 zookie 時間小於等於儲存層更新的時間 (ex2-3)，則代表更新先前的 ACL (ex2-1)都已經被套用 2. Leopard Indexing 因為權限判定可能是嵌套很多層 (Group A 包含 Group B 包含 Group C \u0026hellip;.)，這樣在判斷上會有需要 traverse 很多節點 (圖學問題)，Zanzibar 設計 Leopard Indexing，將 group 層級攤平並儲存在 memory 中，這樣降低了搜尋的複雜度\n3. 把較慢的請求發給不同的 server (Request Hedging) Zanzibar 會有一個閥值，當發現請求比較慢時，會發送給其他回應速度比較快的 server，優化特別慢的請求 (通常也代表特別複雜)\n其他還包含一些 Cache 設計 / 整體架構 / 對外開放 API 等，這邊就先不贅述，來看一下第三方提供的 Zanzibar like 雲端服務，部落格跟教學寫得都很好\nauthzed-A managed permissions database for everyone auth0，還在 preview 階段，但他們的教學不錯 oso 四. 針對 cloud native 的 solution - Open Policy Agent 當我們把目光從應用程式移開，整個系統架構處處都需要驗證的服務\n某個網域的流量是否可以進來 / 出去 某台機器開發者是不是可以 ssh DB 是不是可以被第三方廠商 access Open Policy Agent (簡稱 OPA) 針對 cloud native 架構設計驗證規則的解法，中心化管理驗證規則與其他邏輯解耦，整個架構統一一套驗證規則語言，支援 Kubernetes / Envoy / Terraform (決定用戶是否能用 tf 調整某種資源) / Kafka 等，也開放 HTTP API 所以應用程式層也可以使用，另外也有提供 Golang Libary 與 WASM 可以整合\n流程大概是 Client 發出請求 OPA agent 根據請求找到對應的 policy (用 rego 語言撰寫)，結合 data 判斷 client 是否有足夠權限 實際的 policy 制定如\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package httpapi.authz # bob is alice\u0026#39;s manager, and betty is charlie\u0026#39;s. subordinates := {\u0026#34;alice\u0026#34;: [], \u0026#34;charlie\u0026#34;: [], \u0026#34;bob\u0026#34;: [\u0026#34;alice\u0026#34;], \u0026#34;betty\u0026#34;: [\u0026#34;charlie\u0026#34;]} default allow := false # Allow users to get their own salaries. allow { input.method == \u0026#34;GET\u0026#34; input.path == [\u0026#34;finance\u0026#34;, \u0026#34;salary\u0026#34;, input.user] } # Allow managers to get their subordinates\u0026#39; salaries. allow { some username input.method == \u0026#34;GET\u0026#34; input.path = [\u0026#34;finance\u0026#34;, \u0026#34;salary\u0026#34;, username] subordinates[input.user][_] == username } 請求類似於\n1 2 3 4 5 6 7 8 9 10 11 input_dict = { # create input to hand to OPA \u0026#34;input\u0026#34;: { \u0026#34;user\u0026#34;: http_api_user, \u0026#34;path\u0026#34;: http_api_path_list, # Ex: [\u0026#34;finance\u0026#34;, \u0026#34;salary\u0026#34;, \u0026#34;alice\u0026#34;] \u0026#34;method\u0026#34;: request.method # HTTP verb, e.g. GET, POST, PUT, ... } } # ask OPA for a policy decision # (in reality OPA URL would be constructed from environment) rsp = requests.post(\u0026#34;http://127.0.0.1:8181/v1/data/httpapi/authz\u0026#34;, json=input_dict) if rsp.json()[\u0026#34;allow\u0026#34;]: data 部份不一定要寫死，而是可以從外部的 DB 撈取或是擷取 request 中的 jwt token，參考 External Data，算是蠻有彈性的\n以上的機制是聽到 Netflix 的分享 How Netflix Is Solving Authorization Across Their Cloud [I] - Manish Mehta \u0026amp; Torin Sandall, Netflix\n總結 驗證比我想像中的複雜許多，Google 設計了 Zanzibar / Netflix 部分採用的 Open Policy Agent 機制，前者針對負責的巢狀規則與極大規模的驗證需求、後者則針對 Cloud Native 環境統一了架構層的驗證規則\n","date":"2023-01-28T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2023/2023-01-28-%E7%A0%94%E7%A9%B6%E5%BE%AE%E6%9C%8D%E5%8B%99%E4%B8%8B%E7%9A%84%E6%8E%88%E6%AC%8A%E8%A8%AD%E8%A8%88-google-zanzibar-%E8%88%87-open-policy-agent/","title":"研究微服務下的授權設計 - Google Zanzibar 與 Open Policy Agent"},{"content":"驗證 - 確認使用者身份、授權 - 允許某人針對某些資源操作某些行為，兩者在對於自建網站可能是等同一件事，當我讓 user A 登入通過驗證，那也就等同授權 user A 可以操作他自己的資源\n但今天如果驗證的服務、管理資源的服務、代替用戶操作資源的第三方服務都是獨立時，驗證與授權就有很大的區別，以下內容與截圖參考自\nOAuth 2.0 and OpenID Connect (in plain English) ID Tokens vs Access Tokens - Do you know the difference?! 關於登入與 OAuth 2.0 最陽春的登入機制莫過於 client 提供 account / password 給 server，server 進去資料庫比對，成功則設定 cookie or session 這樣的技術有幾個缺點\n安全性：如果用戶多個網站都用相同的帳密，則一個網站被攻破則每個網站都可能遭殃 維護性：每個網站都需要維護用戶的資料 第二個問題是如果某服務希望代替用戶操作另一個服務 (Delegated authorization)，如果透過帳密非常危險，也沒有權限的限制，例如 Yelp 曾經跟用戶要帳密去登入 Email OAuth 2.0 也就是在這樣的時空背景誕生\nOAuth 2.0 簡單介紹 Authorization server 與 Resource server 有一套驗證與授權範圍管理的機制，Client app 取得 Resource Owner (user 本人) 的授權後，就可用 Access token 代為向 Resource server 發起操作\n主要有四種流程\nAuthorization Code flow implicit flow account / password flow client credential flow 以下僅介紹第一種\nAuthorization Code flow 基本流程為\nUser 透過 Client App 指定要透過第三方登入 第三方 Auth Server 顯示授權頁面 User 同意授權後，Auth Server 夾帶 code 轉導到 Client App Client App 透過 code + client 專屬資訊向 Auth Server 換 access token Client App 拿到 access token，就可以向 Resource Server 發起操作 特別注意到 channel 的概念，這邊分成\nfront channel back channel front channel 指的是在前端，因為這部分的流程暴露在客戶端，安全性相對較低，例如瀏覽器插件的側錄等； back channel 指的是後端，因為 server 是在我們掌控所以安全性相對高\n所以任何東西從 front channel 轉到 back channel 都需要再次驗證，例如 front 回傳 code，此時 code 必須在 back channel 跟 Authorization server 換 access token\n如果 front channel 直接回傳 access token，則有可能被替換的風險\nOAuth 2.0 與 OpenID Connect 在一開始我們都會用 OAuth 2.0 驗證與授權一並處理，例如 Google OAuth 2.0 可以指定 Scope https://www.googleapis.com/auth/userinfo.profile\t，但隨著驗證的需求越來越明確，包含手機驗證登入等，所以就又基於 OAuth 2.0 擴展推出了 OpenID Connect\nOpenID Connect User 透過 Client App 指定要透過第三方登入 第三方 Auth Server 顯示授權頁面 User 同意授權後，Auth Server 夾帶 id token 回到 Client App Client App 驗證 id token 後就可以拿到用戶基本資料 如有額外需要，Client App 可以用 id token 呼叫 Auth Server API (/userinfo 如 Google https://openidconnect.googleapis.com/v1/userinfo、 Line https://api.line.me/oauth2/v2.1/userinfo) 拿額外的用戶資訊 乍看之下 OpenID Connect 跟 OAuth 2.0 流程很像，但有個最巨大的差異\nOpenID Connect 拿到的 id token 可以直接解析並讀取用戶資訊； 而 OAuth 2.0 拿到的 access token 並不是 Client App 要解讀，而是單純送給 Resource Server 驗證\n所以從驗證角度，Server 透過 OpenID Connect 可以直接解析 id token，而不用多打一次 Api 去要用戶的資料\n所以 OpenID Connect 有明文規定 token 必須是 jwt 格式，Client App 收到後要拿 Auth Server public key 檢查核發單位是否正確 (數位簽章的概念)\n而 OAuth 2.0 用的 access token 不一定要是 jwt，任意的 string 都可以，只要 Auth Server 跟 Resource Server 認得就好\n為什麼不拿 id token 當作 access token OAuth 2.0 有規範 sender contraint，限定 client 可以發送 access token，如果有實作那即使 token 被 hacker 偷走也沒關係，因為發送到 Resource server 會被擋下來 Access Token 有更多的檢查，包含 scope (ID token 沒有) / Aud / token format validation 為什麼不拿 access token 當作 id token access token 不是要給 client 解析，而是給 resource server，client 最好把 access token 當作任意字串 並沒有統一標準取得用戶資訊 使用 JWT 的一些資安考量 因為 Token 是採用 JWT，在核發與驗證上要特別注意，否則會有資安上的漏洞\n1. 務必檢查 iss / aud / exp 等基本訊息 RFC 7591 中的 payload 保留字段通常都是需要驗證的，包含\niss (issuer)：誰核發的 token aud (audience)：誰應該收到此 token sub (subject)：token 核發的目的 exp (expiration)：token 何時過期 nbf (Not Before)：token 何時開始生效 iat (issued at)：token 核發的時間 jti (JWT id)：jwt ID，只有在可能發生碰撞才需要 2. 務必移除 alg: none 支援 JWT 的合法性需要透過 signature 去驗證，而 Hacker 可能指定 alg: \u0026ldquo;none\u0026rdquo; 跳過驗證，如果實作沒寫好就會真的被跳過； 看了一下 Ruby 實作沒有這個問題，會去檢查 alg 有沒有在指定的範圍內\n1 2 3 def valid_alg_in_header? allowed_algorithms.any? { |alg| alg.valid_alg?(alg_in_header) } end 3. 小心 alg 被調整 同樣是 alg 被調整，如果從非對稱加密被改成對稱加密，實作不恰當可能就會被繞過，記得要分清楚非對稱加密的 key 與對稱加密的 key\nSingle Sign On (SSO) 與 OAuth 2.0 / OpenID Connect 關係 我們常會看到 SSO 與 OAuth 2.0 / OIDC 出現，SSO 主要是描述一種概念，用戶只需要一組帳密登入驗證機構，多個服務都用此驗證機構驗明身份\nauthentication method that enables users to securely authenticate with multiple applications and websites by using just one set of credentials.\n所以 OAuth 2.0 / OpenID Connect 都是 SSO 的一種實作，其他常見做法包含 SAML / LDAP 等\n參考自 How Does Single Sign-On Work?\n","date":"2023-01-20T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2023/2023-01-20-%E9%A9%97%E8%AD%89%E8%88%87%E6%8E%88%E6%AC%8A%E7%9A%84%E5%B7%AE%E5%88%A5%E6%B7%BA%E8%AB%87-oauth-2.0-%E8%88%87-openid-connect/","title":"驗證與授權的差別，淺談 OAuth 2.0 與 OpenID Connect"},{"content":"最近工作重心轉往 Golang 與 gRPC server 開發，最讓我感到神奇的是透過 proto 宣告中的 options，搭配不同的 protobuf plugin 可以 gen 出對應的檔案，包含 OpenAPI Doc / 不同程式碼語言實作的 gRPC server / JSON 檔，不禁讓我好奇這些神秘的 Protobuf options 與背後產生檔案的 Protobuf plugin 到底是怎麼運作 ?!\n以下文章將會透過一個簡單案例 - protoc-gen-http-client 利用 proto 產生對應 Golang http client request 的程式碼，會涵蓋\nprotobuf 與 plugin 的互動機制 如何從 command 讀取到 plugin 設定 如何設計 protobuf extension 以下內容主要參考自\nCreating a protoc plugin to generate Go code with protogen 1. Protobuf 與 plugin 的互動機制 prerequisite 首先我們要安裝 protoc 以及 protoc-gen-go\n安裝 protoc https://grpc.io/docs/protoc-installation/ 安裝 protoc-gen-go 方便開發 plugin https://pkg.go.dev/google.golang.org/protobuf/compiler/protogen#GeneratedFile $ go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.28 前者是 protobuf 的 compiler，後者是 protobuf golang plugin 負責從 protobuf 產出對應的 golang code\n確保 terminal 可以執行 $ protoc / $ protoc-gen-go\n互動機制 一般我們在使用 protoc 產生檔案指令是\n$ protoc \u0026ndash;go_out=paths=source_relative:. \u0026ndash;proto_path=. example/proto/*.proto\n指定 protoc 去載入呼叫對應的 plugin ，plugin 必須是\nshell 可以執行的 binary file 命名格式固定是 proto-gen-${plugin name} protoc 指令有幾個參數\n${plugin name}_out 解析要執行的 plugin，如透過 --go_out 指定了 proto-gen-go 的執行，並指定輸出的資料夾位置 ${plugin name}_opt plugin 參數 --proto-path= 指定 input proto 的資料夾位置 如果 plugin name 有多個字母，可以用 - 分隔如 protoc-gen-grpc-gateway 如此命名\n建立專案 protoc-gen-http-client 參考程式碼：phase1\n第一步，產生 protoc plugin 並成功產生空殼檔案，參考目錄\n1 2 3 4 5 6 7 8 protoc-gen-go-http-client ├── main.go // plugin code ├── example // 範例 proto │ └── proto │ ├── test.proto │ ├── test.pb.go // protoc-gen-go 產生的 │ └── test_http.go // 自製 plugin 產生的 └── go.mod // 記得 module 命名必須是 protoc-gen-xxxx 在開發上我們主要透過 golang program，並採用 protobuf golang package 協助開發\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 package main import \u0026#34;google.golang.org/protobuf/compiler/protogen\u0026#34; func main() { protogen.Options{}.Run(func(gen *protogen.Plugin) error { for _, f := range gen.Files { if !f.Generate { continue } generateFile(gen, f) } return nil }) } // generateFile generates a _http.pb.go file containing gRPC service definitions. func generateFile(gen *protogen.Plugin, file *protogen.File) { filename := file.GeneratedFilenamePrefix + \u0026#34;_http.pb.go\u0026#34; g := gen.NewGeneratedFile(filename, file.GoImportPath) g.P(\u0026#34;// Code generated by protoc-gen-go-http-client. DO NOT EDIT.\u0026#34;) g.P() g.P(\u0026#34;package \u0026#34;, file.GoPackageName) g.P() g.P(\u0026#34;func main() {\u0026#34;) for _, srv := range file.Services { for _, method := range srv.Methods { if method.GoName == \u0026#34;Get\u0026#34; { g.P(\u0026#34;// it\u0026#39;s get\u0026#34;) } } } g.P() g.P(\u0026#34;}\u0026#34;) } 簡單帶過程式碼\nL7 會拿到輸入的 proto file，我們可以一個一個檔案處理 L19~L20 是產生輸入檔案 L27,28 是針對 proto 裡面的 service / message 輪詢 透過 g.P() 或是 stdout fmt.Println 都會把 string 內容寫入輸出的檔案中 安裝並測試 開發後可以直接在專案目錄下安裝\n$ go install\n此時對應在 $GOBIN 應該會有對應的 binary file，如果 shell $PATH 有正確指定那 $ protoc-gen-http-client 可以正確執行\n此時 protoc 就可以載入 http-client 的 plugin，看到 .go file 產生\n$ protoc \u0026ndash;go-http-client_out=. \u0026ndash;go-http-client_opt=\u0026ldquo;paths=source_relative\u0026rdquo; \u0026ndash;go_out=. \u0026ndash;go_opt=paths=source_relative example/proto/*.proto\n2. 如何從 command 讀取到 plugin 設定 參考程式碼：phase2\n讓我們在執行 protoc 指令時，順便帶上指定參數來設定 http client request 的 url base-url\n主要程式碼改動透過 flag 取得參數\n1 2 3 4 5 var flags flag.FlagSet baseUrl := flags.String(\u0026#34;base_url\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;flags from command\u0026#34;) opts := \u0026amp;protogen.Options{ ParamFunc: flags.Set, } $ protoc \u0026ndash;go-http-client_out=. \u0026ndash;go-http-client_opt=paths=source_relative,base_url=api.com example/*.proto\n在 protoc-gen-go 中，可以指定 paths 參數，指定輸出的資料夾位置 (與 go_out 一起影響)，參考 Compiler Invocation\n3. 如何設計 Protobuf extension 參考程式碼：phase3\n接著我們要來增加 plugin 的 Protobuf extension，在 proto file 中指定 options 讓 plugin 產生對應行為\n我們主要在\nservice 中增加 method / path 的指定 message 中增加 field default value 指定 3-1 增加 proto extension 首先要針對 protobuf 不同的結構去 extend，從上到下有 file \u0026gt; service / message \u0026gt; method/field\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // options.proto // package name 要指向 proto 所在位置，而不是 golang 專案目錄 option go_package=\u0026#34;github.com/sj82516/protoc-gen-go-http-client/protos\u0026#34;; message HttpClientMethodOptions { string method = 1; string path = 2; } // extend google.protobuf.MethodOptions { HttpClientMethodOptions method_opts = 2050; } // extend google.protobuf.FileOptions { HttpClientFileOptions file_opts = 2048; } 接著在 example 中就可以使用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // test.proto import \u0026#34;example/http-client/options.proto\u0026#34;; message User { int64 id = 1[(field_opts).default=\u0026#34;1\u0026#34;]; } service HelloService { rpc GetUser(Hello) returns (User) { option (method_opts).method=\u0026#34;get\u0026#34;; option (method_opts).path=\u0026#34;/user\u0026#34;; }; } 這邊有幾點比較 tricky\nprotobuf import 跟 golang package 分開，聽起來很直覺但我一開始搞混了所以卡很久，尤其是要使用第三方的 protobuf extension 要手動下載 proto file 到自己的資料夾中，參考 gRPC gatewate 文件：we need to copy some dependencies into our proto file structure. 如果有用上 protoc-go-gen 則對應的 proto extension 同路徑要有 compiled .pb.go 檔案否則會出錯 資料夾結構如下\n1 2 3 4 5 6 7 8 9 10 protoc-gen-go-http-client ├── example │ ├── http-client // 任何第三方的 proto extension 都需要自己複製 │ │ └── options.proto │ └── proto │ ├── test.proto │ └── test.pb.go └── protos ├── options.proto └── options.pb.go // 需要 compiled 出 .pb.go，否則 只是剛好我的 protobuf plugin 跟 example 放在同一個資料夾下，但如果兩者是分開的專案也要按照相同的方式\n可以參考 protoc-gen-openapiv2\n3-2 調整 plugin 讀取 options 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // parse option options := method.Desc.Options().(*descriptorpb.MethodOptions) if options == nil { } v := proto.GetExtension(options, customProto.E_MethodOpts) if v == nil { } // wrap as client opts, _ := v.(*customProto.HttpClientMethodOptions) if opts.Method == \u0026#34;get\u0026#34; { g.P(fmt.Sprintf(\u0026#34;func %s() {\u0026#34;, method.GoName)) g.P(fmt.Sprintf(\u0026#34;res, err := http.Get(\\\u0026#34;https://%s%s\\\u0026#34;)\\n\u0026#34;, *baseURL, opts.Path)) g.P(fmt.Sprintf(\u0026#34;target := %s{}\u0026#34;, method.Output.Desc.Name())) ... } 截取部分程式碼，主要是透過 protogen package 就可以取得個別 proto 結構中的 options\n總結 透過 protoc plugin 讓 proto file 可以身兼多職，當作整個專案的核心定義檔，如架起 gRPC server 同時能順便產出對應的 api doc，透過自己撰寫個小 demo 更理解其中的原理還蠻有趣的\n","date":"2022-11-26T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2022/2022-11-26-%E9%96%8B%E7%99%BC%E7%B0%A1%E6%98%93%E7%9A%84-protobuf-plugin/","title":"開發簡易的 Protobuf plugin"},{"content":"在一月初時上 Teddy 的 DDD 課程時，第一次見識到 DDD 結合 Clean Architecture，可以將設計跟實作完美的結合，打造出彈性、易懂、好維護的程式碼，就一直對 Clean Architecture 很感興趣 (課程 ezKanban 原始碼)；\n後來在 2022 Coscup 上聽到強者 Jalex 分享 Clean Architecture in Go: The Crescendo Way 提供了另一種實作的方式；\n最後在看【Clean Architecture 實作篇】時，作者又提供另一種方式以六角架構為基底的實作\n在看了這幾種不同實作的角度後，整理一下實作 Clean Architecture 中不同層轉換的取捨\nClean Architecture 的解讀 【Clean Architecture】一書談了非常多的觀念，整理後自己的解讀是 架構是為了降低後續的開發、維運成本並最大化工程師的產能，而在這樣的原則下提出了以下的依賴原則\n離 IO 越遠的元件越是核心，而核心不應該因為外層的改變而受到影響，就好比說用戶不會在意資料庫是用 MySQL 還是 MongoDB (IO)，他只會在意他購買的品項與折扣對不對 (業務規則) 依賴的物件不可以跨層，所以 Adapter 不能直接存取 Entity 所以從內而外，可以分成 Entity (Domain Object) / Use Case / Adapter (Controller, Gateway, Repository - 常指 DB 儲存層) 等\n六角架構 由 Alistair Cockburn 提出的六角架構，剛好與 Clean Architecture 觀點呼應，外部 IO (Adapter) 如果要與內層 (Entity / Use Case) 互動必須通過 Port，這些 Port 會透過依賴反轉當作隔離層，避免內層被外層的變動而改變\nPort 方向依照核心的對應有兩種\nIn 從外層呼叫核心 Out 核心呼叫外層 (透過 interface) 跨層的溝通原則 在【Clean Architecture】一書中，提到如果跨層的話內層的物件是不可以直接被輸出到更外層，例如 Entity 只能在 Use Case 使用，如果要傳到 Controller , Gateway，必須再轉一層 DTO (data to object)，反之從外層往內傳也是，這是為了符合 SRP 單一職責\n例如一般的 web request，會從 Controller 取得參數 -\u0026gt; Use Case + Entity 處理業務邏輯 -\u0026gt; Controller 回傳結果，此時如果 Controller 在顯示結果直接依賴於 Entity，那 Entity 就會可能因為 API 要多回傳某個欄位而受到改動，違反最一開始的原則\n但是如果每一層之間都需要 DTO 轉換，那程式碼會很多重複宣告，因為我們透過重複取代耦合，在【Clean Architecture 實作篇】有整理幾種方式，供大家取捨\n1. No Mapping 跨層不轉換 先看跨層完全不轉換的方式，參考我用 golang 實作的版本 github/no-mapping，這邊我的 Entity 宣告混雜了 Controller 的 json tag 與 ORM 的 tag\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // Entity type Order struct { gorm.Model ID int `json:\u0026#34;id\u0026#34; gorm:\u0026#34;autoincrement\u0026#34;` Price int `json:\u0026#34;price\u0026#34;` Count int `json:\u0026#34;count\u0026#34;` Total float64 CreatedAt time.Time } // Use Case func (s CreateOrderService) Action(o *domain.Order) *domain.Order { withTax := 1.1 o.Total = float64(o.Count*o.Price) * withTax o.CreatedAt = now() s.saveOrder.SaveOrder(o) return o } 在後續的 Controller / Gateway 中，就直接使用 Entity\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // Repo 儲存資料 func (r OrderRepository) SaveOrder(o *domain.Order) { result := r.db.Create(\u0026amp;o) fmt.Println(result.Error) } // Controller 直接用 Entity 取得 API request 內容 // 並傳入 UseCase 中 func (c *OrderController) CreateOrder(ctx *gin.Context) { var srv = service.NewCreateOrder(c.repo) var order domain.Order if err := ctx.ShouldBindJSON(\u0026amp;order); err != nil { ctx.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err}) return } srv.Action(\u0026amp;order) ctx.JSON(http.StatusOK, gin.H{\u0026#34;total\u0026#34;: order.Total}) } 優缺點 如果是很單純的 CRUD，API / DB 的資料格式都一模一樣才會使用，減少不必要的跨層轉換；\n但缺點是如果 API / DB 格式開始分歧，Entity 就會受到污染，出現部分欄位只有在特定用途使用，而不是跟業務邏輯有關，就像是 CreatedAt 欄位只是為了 DB 紀錄，卻出現在業務邏輯上不太合理；\n再加上如果未來不用 json 而走 gRPC，那 Entity 就要重新改 tag，因為 IO 變化而改動 Entity 違反了 Clean Architecture 原則\n所以建議還是少用\n2. Two way mapping github/two-way mapping 則是 Controller / Gateway 獨立宣告自己的物件，建立出 Entity後再傳入 Use Case 中\n1 2 3 4 5 6 type Order struct { ID int Price int Count int Total float64 } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 // Repo 獨立 DAO 定義 type OrderDAO struct { Id int `gorm:\u0026#34;autoincrement\u0026#34;` Price int Count int Total float64 CreatedAt time.Time } func (r OrderRepository) SaveOrder(o *domain.Order) { // 用 Entity 去轉換 DAO orderDao := OrderDAO{ Price: o.Price, Count: o.Count, Total: o.Total, } result := r.db.Create(\u0026amp;orderDao) fmt.Println(result.Error) } // Controller 獨立 Request Object func (c *OrderController) CreateOrder(ctx *gin.Context) { var srv = service.NewCreateOrder(c.repo) // 獨立 DTO type CreateOrderRequest struct { Price int `json:\u0026#34;price\u0026#34;` Count int `json:\u0026#34;count\u0026#34;` } var o CreateOrderRequest if err := ctx.ShouldBindJSON(\u0026amp;o); err != nil { ctx.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err}) return } // DTO 重建 Entity order := domain.Order{Price: o.Price, Count: o.Count} srv.Action(\u0026amp;order) ctx.JSON(http.StatusOK, gin.H{\u0026#34;total\u0026#34;: order.Total}) } 可以看到 Entity 變得很乾淨，而 Controller / Gateway 要自己負責 DTO 的轉換，讓職責變得更明確\n優缺點 這算是蠻平衡的設計方式，適合用在 Use Case -\u0026gt; Adapter 這一層，可以看到 Jalex 大大在 DB 儲存層是用 Two way mapping 的方式 go-clean-arch\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 // DAO 另外定義 type repoGood struct { ID int `db:\u0026#34;id\u0026#34;` Name string `db:\u0026#34;name\u0026#34;` OwnerID int `db:\u0026#34;owner_id\u0026#34;` CreatedAt time.Time `db:\u0026#34;created_at\u0026#34;` UpdatedAt time.Time `db:\u0026#34;updated_at\u0026#34;` } // 直接拿 Good (Entity) 當作參數 func (r *PostgresRepository) updateGood(ctx context.Context, db sqlContextGetter, good barter.Good) (*barter.Good, common.Error) { where := sq.And{ sq.Eq{repoColumnGood.ID: good.ID}, } update := map[string]interface{}{ repoColumnGood.Name: good.Name, repoColumnGood.OwnerID: good.OwnerID, repoColumnGood.UpdatedAt: time.Now(), } // build SQL query ..... // execute SQL query ..... updatedGood := barter.Good(row) return \u0026amp;updatedGood, nil } 3. Full Mapping github/full-mapping 這次嚴格遵守 Clean Architecture，在 two-mapping 上增加跨層都要有 DTO 的轉換，所以 Controller -\u0026gt; Use Case 不能直接用 Entity，多宣告一個 Command Object\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 // CreateOrder Use Case 改吃 CreateOrderCommand 當作參數而非 Entity type CreateOrder interface { Action(command *CreateOrderCommand) *domain.Order } type CreateOrderCommand struct { Price int Count int } // New Command 可以負責驗證參數 func NewCreateOrderCommand(price int, count int) (CreateOrderCommand, error) { if price \u0026lt; 0 || count \u0026lt; 0 { return CreateOrderCommand{}, errors.New(\u0026#34;params error\u0026#34;) } return CreateOrderCommand{ Price: price, Count: count, }, nil } func (s CreateOrderService) Action(command *in.CreateOrderCommand) *in.CreateOrderOutput { o := domain.Order{ Price: command.Price, Count: command.Count, } withTax := 1.1 o.Total = float64(o.Count*o.Price) * withTax // 與 Adapter 互動也是透過 cmd 而非直接傳入 Entity cmd := out.NewSaveOrderCommand(o.Price, o.Count, o.Total, now()) s.saveOrder.SaveOrder(\u0026amp;cmd) output := in.NewCreateOrderOutput(o.Total) return \u0026amp;output } 外部使用上\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 // Controller 呼叫 Command Object func (c *OrderController) CreateOrder(ctx *gin.Context) { var srv = service.NewCreateOrder(c.repo) type CreateOrderRequest struct { Price int `json:\u0026#34;price\u0026#34;` Count int `json:\u0026#34;count\u0026#34;` } var o CreateOrderRequest if err := ctx.ShouldBindJSON(\u0026amp;o); err != nil { ctx.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err}) return } // 透過 new cmd 去互動 cmd, _ := in.NewCreateOrderCommand(o.Price, o.Count) output := srv.Action(\u0026amp;cmd) ctx.JSON(http.StatusOK, gin.H{\u0026#34;total\u0026#34;: output.Total}) } // Adapter 部分 type OrderDAO struct { Id int `gorm:\u0026#34;autoincrement\u0026#34;` Price int Count int Total float64 CreatedAt time.Time } func (r OrderRepository) SaveOrder(cmd *out.SaveOrderCommand) { orderDao := OrderDAO{ Price: cmd.Price, Count: cmd.Count, Total: cmd.Total, } result := r.db.Create(\u0026amp;orderDao) fmt.Println(result.Error) } 優缺點 多了一個 CreateOrderCommand 最大好處是可以進一步分散職責，由 Port 物件協助驗證資料，這樣 Use Case 可以專心驗證業務規則，而基本的資料驗證可以由 Port (也就是 CreateOrderCommand) 負責\n其他地方驗證資料的考量點\nUse Case: 要驗證資料同時驗證業務規則，這樣會比較複雜，讓 Use Case 專注業務規則會比較好 Controller: 不適合驗證資料，如果有其他地方要呼叫 Use Case 那驗證規則要在重複寫一次 資料驗證 vs 業務規則\n業務規則通常指的是與 Entity 本身狀態有關，例如 轉帳餘額不可為空，至於資料驗證比較是一般性的規則檢查例如 Email 格式、金額不可小於零等\n缺點就是如果是 CRUD 的話 DTO 會寫起來很瑣碎，建議是在 Controller -\u0026gt; Use Case 這一層使用\n小作弊 上述的案例我們只有管輸入，當 Controller 要呼叫 Use Case 時 輸入參數用獨立的 Command Object\n但是 Use Case 回傳值還是用 Entity，這還是違反了 Clean Architecture 物件不可跨層原則，這邊主要是方便使用，這也是【Clean Architecture 實作篇】作者在 p. 112 建議的做法\n如果 in / out port 的輸入輸出都要有 DTO，建議參考 Teddy 的實做 dddcleankanban\n1 2 3 4 5 6 7 8 // 在 Use Case 中回傳 output 而非 Entity output.setBoardId(input.getBoardId()) .setBoardMemberDtos(boardMemberDtos) .setWorkflowDtos(workflowDtos) .setCommittedWorkflowDtos(committedWorkflowDtos) .setCardDtos(cardDtosInBoard); return output 總結 總結整體架構大致是\nController 有獨立的 Request / Response Object Controller 會去建立 In Port Command，並呼叫 Use Case Use Case 實作 In Port Interface，外界是與 Interface 相依 透過 Out Port 與 Repository 溝通 Repository 定義 DAO 儲存資料到 DB 因為 return 都偷懶用 Entity，所以大家都與 Entity 相依 採用 Clean Architecture 可以讓每一層變得更獨立、更好測試，開發的模式也可以更固定，在團隊增加人數時也更好上手，增加生產力\n剛好公司的專案一邊是 Rails，框架本身就帶有很強烈的架構風格；另一邊微服務用 Golang 實作 Clean Architecture 試著讓框架與架構解耦，算是蠻有趣的衝突與相互印證\n","date":"2022-09-11T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2022/2022-09-11-%E6%B7%BA%E8%AB%87-clean-architecture-%E5%AF%A6%E4%BD%9C-port-mapping-%E6%96%B9%E5%BC%8F/","title":"淺談 Clean Architecture 實作 - Port Mapping 方式"},{"content":"平日在公司使用 RoR 開發，過往 code base 在跨 class 時為了方便會抽出一個名為 XXFunction::SharedMethod 的 module，讓多個 class 直接 include 使用，但後續在閱讀時發現 module 與 class 職責不清楚，導致閱讀時要不斷地多個檔案切換，最可怕是 instance variable 在 module / class 都會讀寫，強耦合的情況下任意的改動都有可能破壞原本的設計\n因此有了這篇的文章誕生，探討 Module 在 RoR 中該如何正確地使用，才可以再享有 code reuse 的便利卻不造成可讀性 / 維護性上的困擾\n以下文章大量參考\nGitlab 上的討論 Rails module using instance variable is harmful Good Module or Bad Module When To Be Concerned About Concerns Module 簡介 Module 在 Rudy 中是一個類似 class 的存在，是 class / method / constant 的集合但不可以被 initiailize，主要的功能有兩個\n1. Namespace 如果有兩個 class 在不同的 context 中卻有類似的命名，可以用 Module 當作前綴隔開，例如\n1 2 3 4 5 6 7 8 9 10 11 12 module FeatureA class Hello end end module FeatureB class Hello end end FeatureA::Hello FeatureB::Hello 2. Code Reuse 在 Ruby 中繼承只能夠 單一繼承，繼承設計的理念偏向是 is-a；\n如果想更廣泛的賦予 class 某種能力 could-do，那用 module 的方式會更加的適合，在 module 嵌入的數量上 Ruby 並沒有限制 (參考 類別（Class）與模組（Module）)\n最基本的例子是 Ruby Core Module Comparable，當 class include Module 後只要實作 \u0026lt;=\u0026gt; method，就可以使用 Comparable 定義的功能如 \u0026lt;、\u0026gt;、==、between? 等\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class SizeMatters include Comparable attr :str def \u0026lt;=\u0026gt;(other) str.size \u0026lt;=\u0026gt; other.str.size end def initialize(str) @str = str end def inspect @str end end s1 = SizeMatters.new(\u0026#34;Z\u0026#34;) s2 = SizeMatters.new(\u0026#34;YY\u0026#34;) s1 \u0026lt; s2 濫用 Module 的壞味道 介紹了 Module 的功用，來看一下在濫用的情況下造成的副作用\n臭味 1. 相依性變成隱式，降低易讀性 試想今天只是為了降低 class 的程式碼行數，而單純把 code 抽到 Module 會發生什麼？\n1 2 3 4 5 6 7 class AfterSignupController include Wicked::Wizard include SetAdmin include MailAfterSuccessfulCreate include FooBarFighters::MyHero # ... end 打開了一個檔案，結果發現 class 中所有的 method 都在別的 module 內，這時候要找到對應的 module 在哪就很麻煩\n更可怕的是當你找到後，你怎麼改保證在有 class 依賴的情況下，改動 Module 會不會壞掉？\nExtracting methods into a module to only turn around and include them is worse than pointless. It\u0026rsquo;s actively damaging to the readability of your codebase.\n臭味 2. 嚴重耦合，如果共用 Instance Variable 會讓情況更糟 分享一下目前遇到的案例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 module SharedFunction def update_method self.var += 10 end def print puts self.var + self.var2 end end class Hello include SharedFunction def initialize self.var = 0 self.var2 = 10 # 耦合點1: class 必須先 initial var，否則 update_method 就會出錯 # 耦合點2: 必需要知道 module method 呼叫順序，變成是要知道 module 所有細節才能使用，缺少封裝的意義 update_method print end def update_var # 耦合點3. var 這邊也有機會變動，未來在找 var 的值到底是什麼會很困難 self.var = 20 end attr_accessor var end 可以看出 module 在使用 instance variable 後會有很多耦合的地方，包含 initialzie 的時機 / 暴露過多細節等問題\n壞味道3. 多個 Module 間可能會有命名衝突 如果今天兩個 Module 命名重複怎麼辦？ 在 Ruby 中會依照繼承鏈決定套用到的 method，但可能不符合 caller 的預期，這邊可以參考前端 React 圈的討論 Mixins Considered Harmful\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 module A def method_a end end module B def method_b end end # 如果 module B 不小心也定義了 method_a，就會覆蓋過去 class C include A include B end 如何解決 建議 1. Composition over Inheritance Module 某種程度是多重繼承，也可以用 class.ancestors 看到 included Module 在繼承鏈上，要解決這樣的臭味可以改用 Composition\nComposition 是指說把類似的行為封裝成 class，直接在原本的 class 中使用，這帶來的好處是\nclass 有更好的封裝性，只有 public method 可以被使用 (在 Ruby 中不完全正確) class 可以獨立測試 例如以下，原本 tracking 相關的行為被抽到 module 中，要知道 notify_next! 定義在哪就必須找所有的 module 包含繼承的 class\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Todo \u0026lt; ActiveRecord::Base # Other todo implementation # ... include EventTracking end module EventTracking extend ActiveSupport::Concern included do has_many :events before_create :track_creation after_destroy :track_deletion end def notify_next! # end private def track_creation # ... end end 但現在 tracking 變成 class EventPlanning，好處是不用猜 notify_next! 在哪個 module，職責與細節都封裝在 EventPlanning 裡面，要改動只要確保相依的 class，就不用特別擔心，而且還有測試保護\n1 2 3 4 5 6 7 8 9 10 class Todo \u0026lt; ActiveRecord::Base # Other todo implementation # ... has_many :events before_create :track_creation after_destroy :track_deletion def notify_next! EventPlanning.new(self.events).notify_next! end end 建議 2. 設計 Module 時有明確的職責，並確保暴露的細節 千萬不要為了 reuse code 而放棄 Object Oriented 的思考，要確保每一個 Module 被設計的含義，具體來說就是\ninclude 的 class 必須實作什麼功能 (決定耦合點) Module 可以提供 class 怎樣的額外功能 讓我們看幾個漂亮的 Module 設計\n範例：Enumerable Enumerable 是提供集合類型的 class 有方便遍歷 (iterate) 的方法\ninclude class 必須實作 each Module 可以提供 class map / select / sort / count 等 iterator 該有的功能 1 2 3 4 5 6 7 8 9 class Foo include Enumerable def each yield 1 yield 1, 2 yield end end Foo.new.each_entry{ |element| p element } Enumerable 與 class 耦合點只有 each 這個 method，完全沒有 instance variable 等其他耦合，非常乾淨\n建議 3. 如果要用 instance variable 請確保只有 Module 自己使用 如果 Module 還是需要使用 instance variable 也沒關係，確保 class 不會用到即可\n範例：Sidekiq::Worker 讓我們看一個稍微複雜的案例\n1 2 3 4 5 6 7 8 9 10 class HardWorker include Sidekiq::Worker sidekiq_options queue: \u0026#39;critical\u0026#39;, retry: 5 def perform(*args) # do some work end end HardWorker.perform_async(1, 2, 3) 在 include Sidekiq::Worker，可以手動調整參數 sidekiq_options，這邊實作就會建立 instance variable\n1 self.sidekiq_options_hash = get_sidekiq_options.merge(opts) 但很明顯這個 variable 在 class 是完全不會用到，只有 module 內部使用，所以沒有問題\n另外當我們往下追 perform_async 時，可以看到\n1 2 3 def perform_inline(*args) Setter.new(self, {}).perform_inline(*args) end 這邊出現一個 module 內部定義的 class Setter，可以看到實際發送的邏輯在這邊，透過 class 封裝好處是不怕被其他人不小心複寫或改動\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 module Test def module_method puts private_method end private def private_method \u0026#34;module private\u0026#34; end end class A include Test private def private_method \u0026#34;class private\u0026#34; end end A.new.module_method // \u0026#34;class private\u0026#34; module 內的 method 會被覆寫，但如果改用 class 指定就不會\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 module Test def module_method puts ModuleClass.new.private_method end private class ModuleClass def private_method \u0026#34;module private\u0026#34; end end end class A include Test private def private_method \u0026#34;class private\u0026#34; end end A.new.module_method // \u0026#34;module private\u0026#34; 爭議 Rails ActiveSupport::Concern ActiveSupport::Concern 是 Rails 中蠻有趣的設計，類似於 Module 但有增加更多的 魔法，可以把太大的 orm class 以 module 的形式分拆出去，算是 orm 版的 Module，在使用的心法與上述雷同，就不贅述，也可以參考 Rails Concerns: To Concern Or Not To Concern\n基本上也都是提到 如果 module 與 class 產生了 circular dependency，如果有人沒注意到直接改 class，那 module 就可能會壞掉；解法同樣是降低依賴的機會，讓 module 做的事情簡單一些\n但有趣的是 DHH 本人看起來很喜歡這樣的寫法 ，或許這也是一種 convention over configuration 的體現 XD 但蠻多人抱持反對立場，就大家斟酌使用\n結語 Ruby 是一門有趣的物件導向語言，所有的東西都是物件，所以在設計上如果多使用物件導向的思維去設計，會讓程式碼更好維護\n另外發現很多人都會把 DRY (Dont repeat yourself) 當作聖旨，看到一點點重複就會忍不住抽共用，但沒有考慮到抽共用看起來避免了重複代碼，但這樣造成了意外的耦合，在理解 Clean Architecture 後會發現很多看似重複都是隱性的重複，其實沒有必要抽共用導致耦合的存在，這點未來有機會再延伸探討\n在使用 Module 上，請確保\n真的需要用 module 在用，否則用 composition + class 會讓生活更簡單 Module 設計上要確定暴露的細節與耦合點 使用 instance variable 要小心，請限縮在 module 內使用 ","date":"2022-08-27T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2022/2022-08-27-%E7%A8%8B%E5%BC%8F%E8%A8%AD%E8%A8%88%E8%AC%B9%E6%85%8E%E4%BD%BF%E7%94%A8-mixin-%E6%B7%BA%E8%AB%87-ror-module-%E4%BD%BF%E7%94%A8%E7%9A%84%E9%99%B7%E9%98%B1/","title":"【程式設計】謹慎使用 Mixin - 淺談 RoR Module 使用的陷阱"},{"content":"今天在解 549. Binary Tree Longest Consecutive Sequence II，在評論區發現有趣的解法 Merkle Tree，說來解法也很單純，就是 tree 節點會 (左子樹的 hash value + 右子樹的 hash value + 自己的 hash value) 再取一次 hash value 代表整棵 tree，所以需要 O(n) 的時間複雜度與空間複雜度，n 為節點數\n圖片來自 wiki\n#549 程式碼大致是，以下代碼的來源是 awice-Python, O(N) Merkle Hashing Approach\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def findDuplicateSubtrees(self, root): from hashlib import sha256 def hash_(x): S = sha256() S.update(x) return S.hexdigest() def merkle(node): if not node: return \u0026#39;#\u0026#39; m_left = merkle(node.left) m_right = merkle(node.right) node.merkle = hash_(m_left + str(node.val) + m_right) count[node.merkle].append(node) return node.merkle count = collections.defaultdict(list) merkle(root) return [nodes.pop() for nodes in count.values() if len(nodes) \u0026gt;= 2] 透過 hash value 比對兩個 subtree 是否相同，以上解法要小心有 hash collision 問題，最好重新檢查一下比較保險\nMerkle Tree 在現實中有什麼應用呢？ 以下內容參考自 Understanding Merkle Trees\n主要會出現在樹狀結構且需要快速找出這兩棵 Tree / Subtree 差異之處，例如\nGit 在 pull request 需要快速知道是從哪個 git commit tree 開始不同 / file tree 中有哪些檔案有異動需要同步 Blockchain 中需要知道交易鏈上的某筆交易是否存在於該區塊中 分散式資料庫中在同步資料時，要確認哪部分資料有落差需要同步，如 Dynamo DB 中 讓我們更深入看一下 Git 內部實作\nGit Internals - Git Objects https://git-scm.com/book/en/v2/Git-Internals-Git-Objects\n讓我們先來看一下 Git 內部怎麼儲存資料，Git 內部有一個 key-value database，裡面會儲存 hash key 與對應的內容 (Object)，包含的類型有檔案 (blob) / 資料夾 (tree) / commit 都會以這樣的方式儲存，不同的類型會 hash function 的參數會有所不同，但都是透過 SHA-1 產生\n以上的內容會儲存在 .git/object 的路徑\nBlob Objects 檔案包含文字圖檔等都是 blob 形式，我們可以透過\n$git hash-object 將指定的檔案或內容儲存到 git database 中，並取得 hash code\n$git cap-file 輸入對應的 hash code 可以取出對應的內容\n如官方案例\n1 2 3 4 5 6 7 8 9 10 $ mkdir test \u0026amp;\u0026amp; cd test $ git init $ echo \u0026#39;test content\u0026#39; | git hash-object -w --stdin d670460b4b4aece5915caf5c68d12f560a9fe3e4 $ find .git/objects -type f .git/objects/d6/70460b4b4aece5915caf5c68d12f560a9fe3e4 $ git cat-file -p d670460b4b4aece5915caf5c68d12f560a9fe3e4 test content 在 .git/objects 下可以看到 d6/70460b4b4aece5915caf5c68d12f560a9fe3e4，git 會把 40 碼 hash code 拆成 2 + 38，前 2 碼變成資料夾名稱＋後 38 碼變成檔案名稱，如果試著直接讀取 file 會發現無法識別，主要是 Git 會用 zlib 壓縮內容\nTree Objects 資料夾在 Git 中以 Tree 的格式儲存，其內容會儲存路徑下 tree / blob 的 hash code\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ mkdir -p test1/test2 $ echo \u0026#34;123\u0026#34; \u0026gt; test1/test2/test.txt $ git add . $ git commit -m \u0026#34;init\u0026#34; $ git cat-file -p master^{tree} 040000 tree b5a59142d85435f6a41a972e376a422fc6b2df93 test1 $ git cat-file -p b5a59142d85435f6a41a972e376a422fc6b2df93 040000 tree 33dacd7d9ac656ddebea4ecfc8ab9a87b37c2736 test2 $ git cat-file -p 33dacd7d9ac656ddebea4ecfc8ab9a87b37c2736 100644 blob d800886d9c86731ae5c4a62b0b77c437015e00d2 test.txt 其中 master^{tree}是代表 master branch 的目錄，內容儲存第一層的所有檔案與資料夾 test1，接著一層一層往下找就能找到 test.txt\n如果我們更新 test.txt 會發生什麼事？ 1 2 3 4 5 6 7 8 9 $ echo \u0026#34;456\u0026#34; \u0026gt; test1/test2/test.txt $ git cat-file -p master^{tree} 040000 tree ff1e53b4ff4c130ece8c9f12cdcc3f2613a779e7 test1 $ git cat-file -p b5a59142d85435f6a41a972e376a422fc6b2df93 040000 tree 33dacd7d9ac656ddebea4ecfc8ab9a87b37c2736 test2 $ git cat-file -p e9e2bd5dfa2916e3b1cba933bd9250a9822406e4 100644 blob 4632e068d5889f042fe2d9254a9295e5f31a26c7 test.txt 可以看到 test.txt 因為內容改變而 SHA key 也改變，同樣的 test1 、test2 對應的 tree object SHA key 已經變成了，所以只要底下的 blob 改變 tree SHA key也會跟著改變\n我暫時沒有找到直接把資料夾轉成 hash code 並儲存的方式，在文件中只有記載當在 staging 區增加檔案時，git 會幫忙建立 index 與 tree\n每次變動 git 都會儲存整份檔案內容，這樣 .git/objects 就會超肥？ 沒錯，會非常肥，所以 git 提供指令可以清除檔案太舊的資料，參考 How to shrink the .git folder，在 git 文件中有更詳細描述 10.7 Git Internals - Maintenance and Data Recovery\n1 $ git repack -a -d --depth=250 --window=250 Commit Objects 在 git 中 commit 儲存是樹狀結構，當執行 $ git commit 時，git 會產生 commit object 並記錄 commit 訊息 / 時間 / 作者與 parent commit，如\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 $ git add . $ git commit -m \u0026#34;init\u0026#34; $ git remove . $ git commit -m \u0026#34;delete\u0026#34; $ git log commit 8864a737bf54b251c878655c263dc9b1e16640e2 (HEAD -\u0026gt; master) ..... delete commit ff226f74f095fd1acb5c965583688be28ad3bbb4 .... init $ git cat-file -p 8864a737bf54b251c878655c263dc9b1e16640e2 tree 4b825dc642cb6eb9a060e54bf8d69288fbee4904 parent ff226f74f095fd1acb5c965583688be28ad3bbb4 author Yuanchieh \u0026lt;sj82516@gmail.com\u0026gt; 1654239635 +0800 committer Yuanchieh \u0026lt;sj82516@gmail.com\u0026gt; 1654239635 +0800 delete SHA key 如何運算 在文件最後有補充如何計算 blob 的 SHA key，這邊參考龍哥教學 【冷知識】那個長得很像亂碼 SHA-1 是怎麼算出來的？ 1 2 3 4 5 6 7 8 9 10 11 # 引入 SHA-1 計算函式庫 require \u0026#34;digest/sha1\u0026#34; # 要計算的內容 content = \u0026#34;Hello, 5xRuby\u0026#34; # 計算公式 input = \u0026#34;blob #{content.length}\\0#{content}\u0026#34; puts Digest::SHA1.hexdigest(input) # 得到 \u0026#34;4135fc4add3332e25ab3cd5acabe1bd9ea0450fb\u0026#34; 總結：在 Git 中檔案與 Commit 都是以樹狀結構儲存，只要節點下的子節點有變動，往上一路到跟節點的 Hash key 都會跟著改變，透過 Mercle Tree 就可以在 O(N) 找到變動的地方\n結語 刷題有時候有趣的不是單純答案寫出來，而是看到討論區有各種牛鬼蛇神的強大解法，許多資料結構或演算法在日常的系統中都扮演重要的角色\n","date":"2022-06-01T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2022/2022-06-01-%E5%88%B7%E9%A1%8C%E9%95%B7%E7%9F%A5%E8%AD%98mercle-tree-%E8%AD%98%E5%88%A5%E5%AD%90%E6%A8%B9%E7%9B%B8%E5%90%8C%E8%88%87%E5%90%A6/","title":"【刷題長知識】Mercle tree 識別子樹相同與否"},{"content":"當需要在某一陣列中，求某一段區間的數值和或是最小值，如果是靜態資料，也就是陣列內容不會再改變，我們可以用 prefix sum 在 constant time 取得結果\n但如果陣列的值會改變，就需要每次都重新計算 prefix sum，此時的時間複雜度會是 O(N)，有沒有更快的方法呢？\n這邊有兩個相似的樹狀結構 Segment Tree / Binary Indexed Tree (又稱 Fenwick Tree) 可以用 O(logN) 解決動態區間和的問題，其中 Segment Tree 可以更廣泛解決區間極值的問題\n相關題目\n307. Range Sum Query - Mutable 308. Range Sum Query 2D - Mutable 315. Count of Smaller Numbers After Self 327. Count of Range Sum Segment Tree 教學影片：Segment Tree Data Structure - Min Max Queries\nSegment Tree 與 BIT 的概念雷同，原本我用 prefix sum 遇到更新時要用 O(n) 整個重建，但如果我把區間切小，每次更新只要影響到部分區間，對應的讀取要篩選符合的區間讀取，妥協後 讀取與更新都控制在 log(N)，但區間該怎麼切以及如何實作呢？ 這就是 Segment Tree 與 BIT 不同之處\nSegment Tree 用陣列儲存區間值，需要兩倍額外記憶體空間，原陣列放在新陣列的最後，接著往前跟新區間 (parent = idx/2)，如下圖 (從影片截圖而來)\n所以區間是 2 -\u0026gt; 4 -\u0026gt; 8 這樣往上疊加\n初始化程式碼為\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class SegmentTree { public: SegmentTree(const std::vector\u0026lt;int\u0026gt;\u0026amp; nums) { offset_ = nums.size(); nodes_.resize(offset_ * 2, 0); // 先把原陣列放在最後 for (int i = 0; i \u0026lt; offset_; i++) { nodes_[i + offset_] = nums[i]; } // parent = 左 child + 右 child for (int i = offset_ - 1; i \u0026gt; 0; i--) { nodes_[i] = nodes_[i * 2] + nodes_[i * 2 + 1]; } } void update(int index, int val) { // 因為有移動，所以要加上 offset_ int nodeIdx = index + offset_; int diff = val - nodes_[nodeIdx]; // 更新時要更新全部的 parent while (nodeIdx \u0026gt; 0) { nodes_[nodeIdx] += diff; nodeIdx /= 2; } } private: int offset_; std::vector\u0026lt;int\u0026gt; nodes_; }; 透過 O(N) 即可完成初始化，我們將原陣列放在最後，並往上疊加出多個區間，Update 需要 O(logN)，因為要往前把相關的區間都要更新一次\n接著重點是 range query，傳入 left / right (閉區間) 要在 O(logN) 解決，重點，\n如果我們查找的範圍會涵蓋完整區間，往上找區間值 反之，則取得當下的值，並往下一個區間邁進 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 int sumRange(int left, int right) { int nodeLeftIndex = left + offset_; int nodeRightIndex = right + offset_; int count = 0; while (nodeLeftIndex \u0026lt;= nodeRightIndex) { // 如果是左指針，且指到區間右側，當下取值 if ((nodeLeftIndex \u0026amp; 1) == 1) { count += nodes_[nodeLeftIndex]; nodeLeftIndex++; } // 如果是右指針，且指到區間左側，當下取值 if ((nodeRightIndex \u0026amp; 1) == 0) { count += nodes_[nodeRightIndex]; nodeRightIndex--; } nodeLeftIndex /= 2; nodeRightIndex /= 2; } return count; } 如果想要查詢整段區間的指標移動，我們可以觀察出一個重點\n偶數 index 都在區間的左側 / 奇數 index 是在區間的右側\n因為我們是用 left / right 去找區間和，所以我們要找left 往右找與 right 往左找的共同區間，總共分成 4 種情況考慮\nleft 指向偶數：\n代表我們會拿整個區間，因為區間的左側是偶數，當目前 left 就指向偶數，代表要取出整個區間 left 指向奇數：\n代表我們不可以拿區間當作代表，因為奇數是區間的右側，再往下就到另一個區間，所以我們要直接取值 right 指向偶數： right 跟 left 邏輯剛好相反，我們只能拿 right 往左的區間，因為偶數是 parent 左側，再往下移就到下個區間，所以要拿當前值 right 指向奇數：\n因為奇數是區間的右側，代表我們可以拿整個區間為當前值 以上圖為例\n左指針指向 -2，此時是區間的右側，符合條件 2，拿完 -2 往下一個區間走 / 右指針只在區間右側符合條件4，往上一個區間 左指針持續指向區間左側符合條件1、右指針指向區間右側符合條件4，一路往上指到同一個區塊，直接拿完整段區間 (8~-5) 影片參考資料是左閉右開的計算方式，但這樣我覺得再取出區間和比較不好做，所以參考 leetcode 解答調整成目前的閉區間算法\n307. Range Sum Query - Mutable 完整解法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class SegmentTree { public: SegmentTree(const std::vector\u0026lt;int\u0026gt;\u0026amp; nums) { offset_ = nums.size(); nodes_.resize(offset_ * 2, 0); for (int i = 0; i \u0026lt; offset_; i++) { nodes_[i + offset_] = nums[i]; } for (int i = offset_ - 1; i \u0026gt; 0; i--) { nodes_[i] = nodes_[i * 2] + nodes_[i * 2 + 1]; } } void update(int index, int val) { int nodeIdx = index + offset_; int diff = val - nodes_[nodeIdx]; while (nodeIdx \u0026gt; 0) { nodes_[nodeIdx] += diff; nodeIdx /= 2; } } int sumRange(int left, int right) { int nodeLeftIndex = left + offset_; int nodeRightIndex = right + offset_; int count = 0; while (nodeLeftIndex \u0026lt;= nodeRightIndex) { if ((nodeLeftIndex \u0026amp; 1) == 1) { count += nodes_[nodeLeftIndex]; nodeLeftIndex++; } if ((nodeRightIndex \u0026amp; 1) == 0) { count += nodes_[nodeRightIndex]; nodeRightIndex--; } nodeLeftIndex /= 2; nodeRightIndex /= 2; } return count; } private: int offset_; std::vector\u0026lt;int\u0026gt; nodes_; }; class NumArray { public: NumArray(vector\u0026lt;int\u0026gt;\u0026amp; nums) { tree_ = new SegmentTree(nums); } void update(int index, int val) { tree_-\u0026gt;update(index, val); } int sumRange(int left, int right) { return tree_-\u0026gt;sumRange(left, right); } private: SegmentTree* tree_; }; /** * Your NumArray object will be instantiated and called as such: * NumArray* obj = new NumArray(nums); * obj-\u0026gt;update(index,val); * int param_2 = obj-\u0026gt;sumRange(left,right); */ Binary Index Tree 1994 年的論文 A New Data Structure for Cumulative Frequency Tables / 我覺得講得很好的影片 Fenwick Tree (Binary Index Tree) - Quick Tutorial and Source Code Explanation\n這張圖是從論文截圖而來，實作技巧非常巧妙，他利用 Last Significant Bit (LSB) 來決定區間的範圍，如果 LSB 是 xxx1，則只儲存當前一個數，如果 LSB 是 xx10，則儲存當前兩個數，以此類推，所以可以看到 1, 3 等只會儲存當前 1 個數、8 會儲存往前 8 個數\n所以更新時會需要更新所有相關區間，上面這張圖是代表當你更新 index i 時，需要往上調整的 bit index，例如更新 idx 1 時，因為 bit[2]、bit[4]、bit[8] 都有包含 idx 1，所以都要一併更新\n實作方面非常簡單，透過 2 補數 i \u0026amp; -i 即可取得 LSB\n1 2 3 4 5 6 7 8 9 // 1: 0001 //-1: 1111 // 1 \u0026amp; -1 =\u0026gt; 0001 // 6: 0110 //-6: 1010 // 6 \u0026amp; -6 =\u0026gt; 0010 int getParent(int i) { return i + (i \u0026amp; -i); } 讓我們看查詢會變得如何： 圖片表達如果你要某個 prefix sum，你必須往前輪詢的 index，例如要找 idx 1~9 的 prefix sum，則需要 bit[9] + bit[8]，搭配上一張圖 bit[9] 只有儲存 idx 9 這個元素，而 bit[8] 儲存了 idx 1-8 個元素\n實作方面同樣透過 2 補數，只是變成往下減\n1 2 3 int getNextInterval(int i) { return i - (i \u0026amp; -i); } 整體實作\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 class BIT { public: BIT(const std::vector\u0026lt;int\u0026gt;\u0026amp; nums) { int size = nums.size(); // index 從 1 開始比較好計算 bit_.resize(size + 1, 0); arr_.resize(size, 0); // 初始化只要往下一個 parent 加 // 後面 iterate 會疊上去 for (int i = 0; i \u0026lt; size; i++) { int bitIdx = i + 1; bit_[bitIdx] += nums[i]; arr_[i] = nums[i]; int parent = getParent(bitIdx); if (parent \u0026lt; bit_.size()) { bit_[parent] += bit_[bitIdx]; } } } void update(int index, int val) { int diff = val - arr_[index]; arr_[index] = val; index++; // 更新記得要全部包含的區間都更新 while (index \u0026lt; bit_.size()) { bit_[index] += diff; index = getParent(index); } } int prefixSum(int index) { int count = 0; index++; // 取值要往前推 while (index \u0026gt; 0) { count += bit_[index]; index = getNextInterval(index); } return count; } private: std::vector\u0026lt;int\u0026gt; bit_; std::vector\u0026lt;int\u0026gt; arr_; int getParent(int i) { return i + (i \u0026amp; -i); } int getNextInterval(int i) { return i - (i \u0026amp; -i); } }; class NumArray { public: NumArray(vector\u0026lt;int\u0026gt;\u0026amp; nums) { tree_ = new BIT(nums); } void update(int index, int val) { tree_-\u0026gt;update(index, val); } int sumRange(int left, int right) { return tree_-\u0026gt;prefixSum(right) - tree_-\u0026gt;prefixSum(left - 1); } private: BIT* tree_; }; 小結：BIT vs Segment Tree 整理一下兩者\n比較 操作 Segment Tree Binary Index Tree 記憶體空間 2 * n n 初始化 O(n) O(n) 查詢 O(logN) O(logN) 更新 O(logN) O(logN) 此外，使用上兩者有共同侷限 新增 / 移除元素需要重新初始化\n兩者差異 兩者都可以解決 #307 這道題目，但看似相同但還是有差異之處，簡單來說 Segment Tree 用途更廣，BIT 只能解決 Prefix Sum 計算\n例如 Segment Tree 還可以解決區間最小值/區間最大值，而 BIT 是做不到的，為什麼？\n因為 BIT 並不是儲存每一個值，而是在初始化就以區間的形式保存，如果是加法這種 invertible 算法，意即我區間儲存 (a+b), 我可以透過 (a+b) - a 還原 b 的值；\n但求極值是 non invertable，如果要用 BIT 求極值，那麼在區間計算時就用儲存極值，這樣更新時就會出錯\n1 2 3 4 5 原陣列：[3, 2] BIT: [, 3, 3(保存區間極值)] update (0, 1) BIT: [, 1, ?] =\u0026gt; 無法計算 所以 BIT 用途比較侷限，但優點是記憶體空間小，而且 bitwise 的計算速度會快更多\n有一篇論文寫可以用兩個 BIT 實作區間極值的查詢，結果還是比 Segment Tree 快上許多，可以參考看看 Efficient Range Minimum Queries - using Binary Indexed Trees\n","date":"2022-05-23T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2022/2022-05-23-%E7%AE%97%E6%B3%95segment-tree-%E8%88%87-binary-indexed-tree-%E8%A7%A3%E9%A1%8C%E6%95%B4%E7%90%86/","title":"【算法】Segment Tree 與 Binary Indexed Tree 解題整理"},{"content":"關於 MySQL Lock 來來回回寫了也有幾篇，每次看都有不同的發現，這次為了準備公司內部分享，重新再翻閱一次又找到一些新的有趣發現，以下將盡可能完整的解析 MySQL Lock 與 Index 的關係，怎樣的查詢會拿到怎樣的鎖，以及怎樣的查詢又可能互相 Deadlock\n文章主要受啟發於 解决死锁之路（终结篇） - 再见死锁，對應的實驗可以看程式碼 sj82516/mysql-deadlock-test，先說結論，在 DML (更新、刪除、插入)的操作中，Lock 會與套用的 Index 有關，在 MySQL 中 Index 有幾種\nClustered Index:\n通常是 Primary Key Unique Secondary Index:\n如果沒有 Primary Key，則 Unique Secondary Index 會是 Clustered Index，行為與 Clustered Index 雷同，不額外贅述 Non Unique Secondary Index 以下將重點分析 Clustered Index / Non Unique Secondary Index 對於 MySQL 操作有什麼不同的影響，在 Read Committed 簡稱 RC) 與 Repeatable Read (簡稱 RR) 下又有怎樣的不同行為，實驗預設是 5.7 + Repeatable Read (MySQL 預設)\nClustered Index 先來一題簡單的暖身題，以下兩個 Transaction 為什麼會 Deadlock 要滿足 Deadlock 需要有四個條件\nno preemption hold and wait mutual exclusion circular waiting 在上面的案例中，MySQL 在 RR 下要 update 會先取得 exclusive lock，兩個 Transaction 手上都拿了對方想要的資源卻也不都會先放開手上的鎖，導致 Deadlock 直接從圖片很容易看出彼此 Deadlock，但正式環境中多筆 Transaction 交雜，該如何找出 Deadlock 呢？\n1. 如何 Debug Deadlock MySQL 有幾個相關參數\ninnodb_deadlock_detect: 是否偵測 deadlock，預設開啟 innodb_lock_wait_timeout: 如果沒有開啟 Deadlock detect，建議設定較短的 wait timeout 否則會一直等到 innodb_print_all_deadlocks: 將所有 Deadlock 錯誤輸出至 error log 如果沒有輸出 Deadlock error，可以用 \u0026gt; SHOW ENGINE INNODB STATUS 輸出，其中有兩個區塊 deadlock 顯示最近一筆 deadlock 錯誤，transaction 則會顯示目前在等待 lock 的 transaction\n1.1 實際閱讀 Deadlock 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 LATEST DETECTED DEADLOCK ------------------------ 2022-04-26 09:33:29 0x40de5bc700 *** (1) TRANSACTION: TRANSACTION 9597, ACTIVE 3 sec starting index read mysql tables in use 1, locked 1 LOCK WAIT 3 lock struct(s), heap size 1136, 2 row lock(s), undo log entries 1 MySQL thread id 1034, OS thread handle 278338676480, query id 14980 172.17.0.1 root updating `UPDATE teachers SET teachers.age = 10 WHERE teachers.id = 5` *** (1) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 275 page no 3 n bits 72 index PRIMARY of table test.teachers trx id 9597 lock_mode X locks rec but not gap waiting Record lock, heap no 3 PHYSICAL RECORD: n_fields 6; compact format; info bits 0 0: len 8; hex 8000000000000005; asc ;; 1: len 6; hex 00000000257c; asc %|;; 2: len 7; hex 23000001c017d1; asc # ;; 3: len 1; hex 62; asc b;; 4: len 4; hex 80000010; asc ;; 5: SQL NULL; *** (2) TRANSACTION: TRANSACTION 9596, ACTIVE 3 sec starting index read mysql tables in use 1, locked 1 3 lock struct(s), heap size 1136, 2 row lock(s), undo log entries 1 MySQL thread id 1033, OS thread handle 278608463616, query id 14979 172.17.0.1 root updating `UPDATE teachers SET teachers.age = 6 WHERE teachers.id = 1` *** (2) HOLDS THE LOCK(S): RECORD LOCKS space id 275 page no 3 n bits 72 index PRIMARY of table test.teachers trx id 9596 lock_mode X locks rec but not gap Record lock, heap no 3 PHYSICAL RECORD: n_fields 6; compact format; info bits 0 0: len 8; hex 8000000000000005; asc ;; -\u0026gt; index，用 16 進位表示，這邊是指 id: 5 1: len 6; hex 00000000257c; asc %|;; 2: len 7; hex 23000001c017d1; asc # ;; 3: len 1; hex 62; asc b;; 4: len 4; hex 80000010; asc ;; 5: SQL NULL; *** (2) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 275 page no 3 n bits 72 index PRIMARY of table test.teachers trx id 9596 `lock_mode X locks rec but not gap waiting` Record lock, heap no 2 PHYSICAL RECORD: n_fields 6; compact format; info bits 0 0: len 8; hex 8000000000000001; asc ;; 1: len 6; hex 00000000257d; asc %};; 2: len 7; hex 22000001cc02e5; asc \u0026#34; ;; 3: len 1; hex 61; asc a;; 4: len 4; hex 8000000f; asc ;; 5: SQL NULL; *** WE ROLL BACK TRANSACTION (2) ------------ 這部分內容參考 MySQL死锁问题如何分析，重點看這一段\nRECORD LOCKS space id 275 page no 3 n bits 72 index PRIMARY of table test.teachers trx id 9596 lock_mode X locks rec but not gap waiting 專指 row lock\nRecord lock, heap no 2 PHYSICAL RECORD: n_fields 6; compact format; info bits 0 0: len 8; hex 8000000000000001; asc ;; -\u0026gt; index，用 16 進位表示，這邊是指 id: 1\n，我們可以從 log 中看出操作 / 手上的 lock / 等待的 lock / Lock 在哪一個 row\n2. 取得 Lock 的順序性 如果查詢的條件命中多筆，那 Lock 會怎麼取得呢？ 接著看以下案例 根據 MySQL 文件，會根據 Order By 的指定條件與 Index 本身順序性一行一行鎖起來\nIf an UPDATE statement includes an ORDER BY clause, the rows are updated in the order specified by the clause. This can be useful in certain situations that might otherwise result in an error. Suppose that a table t contains a column id that has a unique index. The following statement could fail with a duplicate-key error, depending on the order in which rows are updated\n從 Deadlock Log 可以清楚看到 id 2 / id 5 被互相鎖住\n1 2 3 4 5 6 7 8 9 *** (1) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 278 page no 3 n bits 72 index PRIMARY of table test.teachers trx id 9676 lock_mode X waiting Record lock, heap no 2 PHYSICAL RECORD: n_fields 6; compact format; info bits 0 0: len 8; hex `8000000000000002`; asc ;; ------ *** (2) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 278 page no 3 n bits 72 index PRIMARY of table test.teachers trx id 9675 lock_mode X waiting Record lock, heap no 3 PHYSICAL RECORD: n_fields 6; compact format; info bits 0 0: len 8; hex `8000000000000005`; asc ;; 2.1 建立 Index 時決定順序 當建立 MySQL Index 也可以指定順序，但需要注意 MySQL 5.7 會忽視 (全部都是 asc) 只有在 MySQL 8.0 以上才支援，所以以下案例只發生在 MySQL 8.0\n我們可以透過 USE INDEX() 指定執行時的 Index (5.7 文件) A key_part specification can end with ASC or DESC. These keywords are permitted for future extensions for specifying ascending or descending index value storage. Currently, they are parsed but ignored; index values are always stored in ascending order.\n2.2 恰巧兩個 Index 順序相仿 2.1 案例中我們很刻意讓 Index 同一個欄位欄位但順序剛好相反，但如果是兩個不同 Index 而資料寫入時讓順序恰好相反呢？ 在建立資料時，id 跟 name 的順序剛好相反，也就是\n1 id1 \u0026gt; id2 \u0026amp;\u0026amp; name1 \u0026lt; name2，例如 (1, \u0026#34;zz\u0026#34;) (2, \u0026#34;zx\u0026#34;) 這樣也會發生 Deadlock!\n3. 查詢沒有命中 : Gap Lock 如果查詢沒有命中，此時 MySQL 在 RR 情況下會取得 Gap Lock，所謂的 Gap 是在已存在欄位之間的縫隙，為了避免幻讀 MySQL 會鎖住 Gap 不讓其他 Transaction 插入資料\n這邊特別介紹兩個鎖定區間的 Lock，分別是 Gap Lock / Insert Intention Lock\nInsert Intention Lock 故名思義是是插入前會鎖定該區間 這兩種 Lock 特別在於不會排擠自己人，例如 Gap Lock 不會阻擋 Gap Lock / Insert Intention Lock 不會阻擋 Insert intention Lock，但是 Insert Intention Lock 跟 Gap Lock 互斥，原因是區間可能很大，為了提升性能，在同一個區間可以同時插入新資料，如果真的有違反 Unique Key 則會有原本的重複性檢查阻擋，Update 也是 有一個場景是「我們希望更新某一筆資料，發現資料不在則寫入」，如以下範例則會造成 Deadlock 因為 id 6 / 10 在 update 時都取得了 Gap Lock，接著要 Insert 取得 Insert Intention Lock 卻因為雙方都還握有 Gap Lock 而無法寫入，讓我們看具體的 Deadlock 細節\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 *** (1) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 279 page no 3 n bits 72 index PRIMARY of table test.teachers trx id 9700 `lock_mode X insert intention waiting` Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc `supremum`;; *** (2) HOLDS THE LOCK(S): RECORD LOCKS space id 279 page no 3 n bits 72 index PRIMARY of table test.teachers trx id 9699 `lock_mode X` Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc `supremum`;; *** (2) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 279 page no 3 n bits 72 index PRIMARY of table test.teachers trx id 9699 lock_mode X insert intention waiting Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; 可以看到雙方都在等 lock_mode X insert intention waiting，指定的 row 是 supermum 這是代表最後一個區間，區間大致長以下這般\n1 2 3 4 (negative infinity, 第一筆] (第一筆, 第二筆] .... (最後一筆, positive infinity) 所以同樣的 query 只是原始資料改變落在不同區間，就不會有 Deadlock 如以下 4. 範圍查詢：鎖定找過的每筆資料即使條件不合 接下來看一個 RR 蠻嚇人的一個特性，參考文件 15.7.2.1 Transaction Isolation Levels\n\u0026ldquo;When using the default REPEATABLE READ isolation level, the first UPDATE acquires an x-lock on each row that it reads and does not release any of them\n也就是說 where condition 假使是範圍搜尋，RR 會把搜尋到的範圍全部鎖死，直到 transaction 結束!\n讓我們看以下案例 Update 的條件沒有命中但是全部都被 Lock，要 update / insert 都不行，相對的\nRC 在檢查不符合條件就會 release，在做大規模的 Update / Delete 記得要用 RC 會比較好\nNon Unique Index 上面的範例都是 Clustered Index，MySQL 支援 Secondary Index，其中 Unique Secondary Index 與 Clustered Index 行為類似，就不另外贅述，但是 Non Unique Index 要稍微留意\n1. 查詢命中：依然會 Gap Lock 在一開始的範例，如果 Clustered Index 查詢有命中只會鎖那一行 (ex. update id = 1)，但如果 Non Unique Index 即使完全命中，也會連同 Gap 一起鎖起來 (Next Key Lock)\n這邊 Lock 比較多，需注意 Secondary Index 會被鎖之外，對應的 Clustered Index 也會被鎖，這邊 age 鎖定 10 以及前面的區間，所以要插入 age: 9 就會失敗；運用上面的技巧，age 切換到不同區間就可以成功插入\nForeign Key：會有 Share Lock If a FOREIGN KEY constraint is defined on a table, any insert, update, or delete that requires the constraint condition to be checked sets shared record-level locks\n更新欄位時 Foreign Key 也會被鎖住，之前有紀錄就不贅述 MySQL Deadlock 問題排查與處理\n總結與建議 幾點建議\n增加 Index 要仔細評估，Secondary Index 會造成寫入效能下降，體現於 lock 的使用 如果是用 ORM，記得檢查 Query 如果需要用 Secondary Index 改變欄位，建議可以用批次 (RoR 就是 find_in_batch) 或是先篩選出 Primary Key (預設 select 不會有 lock)，再使用 Primary Key 當作修改條件避免 Gap Lock 沒事就用 Read Committed 進階：為什麼不要預設 Read Committed ? 既然 Repeatable Read 會有這麼多效能疑慮，更新時連 where 條件不符合的行都會鎖、還會有不預期的 Gap Lock，那為什麼預設不要改成 Read Committed ?\n在 PostgreSQL 確實如此，PQ 9.3. Read Committed Isolation Level提到\nThe partial transaction isolation provided by Read Committed level is adequate for many applications, and this level is fast and simple to use\n我在查閱 MySQL 相關資料，也有查到 Percona 一篇文章提及 MySQL 預設應該要改成 Read Committed 比較好 MySQL performance implications of InnoDB isolation modes\nIn general I think good practice is to use READ COMITTED isolation mode as default and change to REPEATABLE READ for those applications or transactions which require it.\n那 MySQL 官方怎麼說，我找到一篇官方的 blog Performance Impact of InnoDB Transaction Isolation Modes in MySQL 5.7 建議到\nFor short running queries and transactions, use the default level of REPEATABLE-READ. For long running queries and transactions, use the level of READ-COMMITTED 裡面有另一篇文做了 Benchmark 非常有趣 MySQL Performance : Impact of InnoDB Transaction Isolation Modes in MySQL 5.7，從 MySQL 內部的 Lock 數量與操做 QPS 衡量不同 isolation level，我本來以為 Read Committed 在增刪查改會碾壓 Repeatable Read，但發現盡然沒有，反而因為 Read Committed 在每次 Read 都會產生新的 MVCC 版本而有更多的內部 Lock，非常有趣\n所以總結官方部落格建議，預設還是保留 RR 普遍效能反而更好，只有在長時間的 Job 再改成 RC 即可\n","date":"2022-04-25T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2022/2022-04-25-mysqllock-%E8%88%87-index-%E9%97%9C%E4%BF%82%E5%92%8C-deadlock-%E5%88%86%E6%9E%90/","title":"【MySQL】Lock 與 Index 關係和 Deadlock 分析"},{"content":"DDIA - 《Designing Data-Intensive Applications》，這本書值得有一個專門的縮寫 XD 在幾年前剛出社會時有先硬啃了大半部分，在往後的工作上這些觀念不斷的被使用上，一直很想再重新更深入理解這本書；\n剛好最近要開始準備公司內部分享，分享一些關於資料庫的事情，重新翻閱這本書，並寫下讀書心得，以下內容包含圖片都是整理自相關資料\n以下將描述\n最簡單實作資料庫的方式 SSLTable / B Tree 儲存方式與比較 Column Storage Storage and Retrieval 資料庫最基本的核心功能\n給你一些資料，幫我保存 - storage 我晚點跟你拿這些資料，記得給我 - retrieval 如果僅考慮這兩個功能，如何用最簡單的方式實作呢？\n一. 最簡單的資料庫 - Log Structured Storage 今天實作一個 key-value DB，我們用兩個 bash command 就可以完成\n1 2 3 4 5 6 7 8 #!/bin/bash db_set() { echo \u0026#34;$1,$2\u0026#34; \u0026gt;\u0026gt; database } db_get() { grep \u0026#34;^$1,\u0026#34; database | sed -e \u0026#34;s/^$1,//\u0026#34; | tail -n 1 } 呼叫 db_set 時，很單純一直把值 append 到檔案的最後；如果要取出，則從檔案的最後開始比對 key，回傳第一筆匹配的值，這樣就能解決同個 key 被更新多次的狀況\n1 2 3 $ db_set 42 \u0026#39;hello word\u0026#39; $ db_get 42 hello world 如果我們考量寫入與讀取的效能\n寫入：非常好，因為是順序性寫入，基本上沒有比 append 更快的寫法，在非常多的應用程式中，也都用上 append only log，如 MySQL binlog 讀取：因為要從檔案最末端開始讀取，時間複雜度為 O(n)，非常慢 1. 改善讀取效率 - 加上 Index 我們可以透過在 memory 中維護一份 Hash Table，在寫入時順便儲存 { key:檔案位置 }，這樣就能在查詢時用 O(1) 的時間找到 key\n但這帶來另一個限制，Hash Table 必須能完整放入 Memory，如果今天 key size 超過，則 Index 無法被建立就會回到 O(n) 的查詢複雜度\n2. 改善儲存效率 - Compact 思考另一個儲存問題，今天如果是同一個 key 被反覆儲存多次，以目前 append-only 的設計，他會被儲存很多筆，每次讀取只拿最後一筆，前面幾筆的資料空間就浪費了\n所以通常會搭配 Compact 設計，重新整理 log 把舊資料刪除，釋放儲存空間，實作上 會開新的檔案合併舊的檔案內容，並不會直接修改舊檔案，這樣做的好處是寫入、讀取不會中斷，等到新的檔案完成後，再移除舊檔案\n3. 為什麼要用 append 而不是直接 update ? 如果我直接改原本的資料，是不是就節省了 compact 的過程？原因在於硬碟順序寫入效能會比較好，即使 SSD 也是，這點後續補充\n4. 無可避免的缺點 - range search 如果要範圍搜尋，Hash Table 無法做到這件事，同樣的 disk 儲存方式也沒有排序，所以只能全部 Scan\n5. Real World Sample: Bitcask 以上的方式聽起來簡單的過分，但這也是真實有在採用的作法，如 Riak 內的儲存引擎 Bitcask，Riak 是分散式 Key-Value DB\nBitcask 是用 Erlang 實作，原始碼 有點看不懂，但有另一篇文章可以從 high level 角度去查證 Bitcask: A Log-Structured Hash Table for Fast Key/Value Data\n儲存方面，bitcask 維護一個 active file 持續寫入，其餘會有多個 older file 只讀不寫，背景運行一個 merge process 持續合併多個 older file\n每筆資料有前綴 metadata，這些資料欄位的尺寸都是固定的，但是 key value 的長度可以是變動，透過 keysz / valuesz 知道對應長度；\n其中 CRC 是 checksum，可以檢查資料寫入是否有誤\nbitcask 中的資料結構 keydir 就是 hash table，保存 key 對應到 value 的儲存位置，這樣就能夠一次 Disk 查詢取出 value\n6. 不同面向考量 資料庫不單是讀寫，還需要考量其他面向的問題，讓我們一一檢視這樣的簡單設計\nCrash Recovery: 基本上不會有問題，因為檔案都是 append only，如果有問題可以透過 CRC 檢查，只是 keydir 要重建 Restore: 直接複製檔案過去就好 Heavy Load \u0026amp; High Volume: 當資料量變大或是 loading 變大時，預期 bitcask 的效能不會有太大差異，因為都是很簡單的 disk 與 memory 操作 二. SSTable 與 LSM Tree 上面提到 Log Structure 遇到 range search 效能會很差，那如果我們把資料排序後再寫入呢？\n在計算機科學中，負責快速插入、查找、範圍搜尋的資料結構可以用 平衡樹，插入與搜尋都是 O(logn)，如果是範圍搜尋則為 O(logn, k)\n實作上，當資料寫入時\n會先暫存到 memory 中，資料結構為平衡樹，此稱為 memtable 當 memtable 超過門檻，寫入硬碟，稱為 SSTable (Sorted String Table)，後續資料寫入新的 memtable SSTable 同樣會在背景 compact，如 Log Structure 到底儲存用 sorted 後的結果有什麼巨大的好處？又會有帶來什麼影響呢？\n讓我們思考以下幾個細節\n1. 寫入最一開始是儲存在 memory 中，如果 crash 會不會掉資料？ 會，所以寫入 memory 時同時會用 append log 方式寫入資料庫，借用 log structure 的智慧，當 crash recovery 從 log 復原即可\n2.sorted 後儲存結果的幾個好處 除了剛剛說的支援更快的範圍查詢外，保存在 memory 中的 index 數量可以更少，讓 dataset 支援容量更大，如上面圖示，SSTable 會以多筆資料為一個 block保存，今天我要找 2，index tree 內沒有 2 沒關係，我可以找到 1 的檔案位置，接著把 block 讀取出來，這樣我就能透過 1 找到 2，能夠這樣找是因為 儲存有按照 index 排序\n3. compact 過程會不會很麻煩? 不會，如果有學過 merge sort，這個過程就是把兩個小的 block 從頭到尾 iterate 過就解決了，時間複雜度為 O(n+m)\n4. 查找不存在的 key 效率不好 因為要找過每一個 SSTable 才能確定資料不存在，這邊可以利用 Bloom Filter 快速判斷，可以參考我之前的筆記 Sketch Data Structure - Bloom Filter 介紹與實作\n5. Real World Sample: LevelDB / RocksDB / BigTable Google LevelDB 是用 SSTable 概念實作的 Key-Value DB，可以參考他的說明 LevelDB impl.md，其中可以注意 compact 過程是有分等級的 (所以才叫 level DB)，避免一次性大量的 compact 發生導致硬碟效能吃不消，其中翻了一下 memtable 沒有看到是用什麼方式實作\nRocksDB 則是 Facebook 基於 LevelDB 所開發的，說明蠻完整的 RocksDB Overview\nThe memtable is an in-memory data structure - new writes are inserted into the memtable and are optionally written to the logfile (aka. Write Ahead Log(WAL)). The logfile is a sequentially-written file on storage. When the memtable fills up, it is flushed to a sstfile\n下方有提到一些 memtable 實作與比較，預設是 skiplist，感覺蠻有趣的，未來可以再深入研究\n另外還有 Google 的Bigtable: A Distributed Storage System for Structured Data，裡面也有提到很多相關的內容，未來待讀項目之一 (挖坑)\n以上 Log Structure 與 SSTable 都可稱為 LSMTree，也就是 Log Structure and Merging Tree，透過 log 方式儲存並有持續 merge 的行為\n三. B Tree B Tree 在資料庫儲存上是非常受歡迎的選項，如 MySQL / PostgreSQL 等，B Tree 也是平衡樹的一種，每一層保存指定數量的節點 (branching factor) 代表不同的範圍，並保存指向下一層的指針，最後在葉子節點 (leef node) 保存資料\n在 B Tree 的保存中，是以固定尺寸的 Page 為單位，剛好對應到硬碟儲存方式也是以固定尺寸的區塊儲存，如果資料沒有塞滿 Page 則會造成一些破碎\n高度為 4 + brancing factor 為 5 + Page size 4 KB 的 B Tree 就能儲存 256 TB\nB Tree 與 SSTable 相似之處在於資料保存於硬碟都是排序過，但是 B Tree 會不斷修改已經持久化的檔案，尤其是當 Page 內資料超過需要拆分 Page 並重新平衡時，會有比較多的硬碟操作，而 SSTable 只會一直寫入等到 compact 階段才合併產生新的檔案\nB Tree vs SSTable 常理來說 SSTable 寫入會比較快，因為就是 append 上去而 B Tree 需要先寫 WAL 並等到 Page 刷新到硬碟上，這點在高寫入的系統下尤為重要； 另外 SSTable 會有 compact 過程，相比 B Tree Page 設計可以更有效使用磁碟空間\n而 B Tree 的好是讀取較快，因為 SSTable 的同一個 key 可能散落在多個 file 中需要每個都檢查；同時當遇上 compact 時效能會有比較大的衝擊，而 B Tree 相對會比較穩定\nColumn Storage 上述偏 OLTP 的資料庫設計，資料是以 row 的方式儲存，但是在 OLAP 專門做分析上，往往我們需要的是 query 非常多的資料筆數但只分析其中一兩個欄位，例如撈出過去一年的銷售總額，如果資料是以 row 方式儲存，要把一整年的資料都拿出來過濾、篩選再總和，十分耗費資源\n在 OLAP 中，既然我們常常以 column 為主，那改用 column 來當作儲存的依據是不是會比較好？ column storage 的概念就這樣延伸出來\n這樣做的最大好處非常好壓縮，從欄位資料的 cardinality 來看，往往數量不多，例如數百萬筆資料中國家種類就那兩百個，所以可以用一些壓縮的技巧如 bitmap encoding，用 bitmap 代表某個特定值儲存，在讀取時可以用 bitwise 操作，對於 CPU 效率會好上許多；\n所以儲存空間小、運算也很快\n限制與應變 因為現在是每一個 column 都獨立儲存，那如果我想要讀取某一個 row 的所有 column 怎麼辦？\n所以在儲存時每一個 column file 的 row order 都必須一樣，這樣才能還原同一筆 row 的全部 column\n如果在儲存時希望有排序，例如分析資料通常會按照日期排序，可以套用之前 SSTable 概念，先用 memtable 排好序，寫入時再分成多個 column file 儲存\n要注意 column-family database 與 column storage database 是不一樣的詞彙，如 Cassandra 在文件表明是以 row-based 儲存，參考 Apache Cassandra is a highly-scalable partitioned row store，但他被歸類在 column-family 中，至於為什麼這樣歸類可能是跟他的用途跟印象比較有關，網路上隨便一查都有一些錯誤的資訊，要在小心留意 (會不會我才是錯的 ?! 如果是歡迎留言讓我知道)\n結論 知道資料庫怎麼儲存好像不會變成資料庫大師 XD 但對於未來在評估不同的資料庫時，又多一個可以驗證真偽的工具，尤其是在大營銷時代各種新的技術名詞持續被發明，但資料庫的本質就還是 storage and retrieval\n","date":"2022-04-02T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2022/2022-04-02-ddia03-%E8%B3%87%E6%96%99%E5%BA%AB%E5%84%B2%E5%AD%98%E5%8E%9F%E7%90%86%E7%A0%94%E7%A9%B6/","title":"【DDIA】03 - 資料庫儲存原理研究"},{"content":"從應用程式的角度，執行任務可以分成 I/O bound 與 CPU bound，而 I/O 處理相比於 CPU 運算速度慢好幾個層級，所以當 I/O 還沒準備好時會「主動讓」給其他任務，盡可能地讓 CPU 保持忙碌，等到 「I/O 準備好再重新回到 CPU 執行」\n在之前理解 Nodejs non-blocking I/O 時，官方文件 Overview of Blocking vs Non-Blocking 寫到\n1 Any code that is expected to run in a concurrent manner must allow the event loop to continue running as non-JavaScript operations, like I/O, are occurring. Nodejs sdk 有提供 non-blocking I/O 機制，底層 libuv 會處理 event loop 與其餘 I/O 的執行，「當 I/O 執行時可以讓其他的 JavaScript code 繼續運行不被 block」\n所有用「」括起來的地方都是我所想不透的，為什麼 I/O 運行時其他 JavaScript Code 可以繼續運行？是因為 Nodejs 有某種特殊的方法知道目前的程式在等待 I/O 所以主動切換不同的任務執行？如果不是 Nodejs runtime 切換，那會是作業系統切換的嗎？該怎麼切換？\n又為什麼 I/O 的處理不需要 CPU 的介入嗎？ CPU 怎麼知道 IO 處理完了該繼續往下執行？是不是我的 IO 全部改成 asynchronous 效能就突飛猛進？那為什麼 async io 並不是每個程式語言執行時的預設支援？\n同樣最近在看 Golang goroutine Go: Goroutine, OS Thread and CPU Management時又遇到相同的問題，當讀到 goroutine 等待 I/O 回應時 M 會解除 P 並進入等待，讓其他的 M 執行；如果是遇到 network 相關的 I/O 則推至 network poll 等待 network 完成\n從應用程式開發者的角度，我們只要知道有神秘的小精靈會幫我們完成 I/O，JavaScript callback / goroutine system call 可以在非同步的狀況下拿到 I/O 回傳的結果就好\n但往下思索，這一切的黑幕背後有滿山滿谷的疑惑\n作業系統與 I/O 裝置 為了讓開發者可以專注於軟體開發，應用程式多半運行在作業系統之上，透過作業系統提供的統一介面抽象化硬體，並確保單一應用程式不會霸佔硬體資源，所有與 I/O 裝置設備通訊都必須經過作業系統的操控 system call，而這神秘的小精靈就躲在這個環節中\n從 high level 的角度來看，大致如下 分成兩條路線：應用程式主動呼叫與 I/O 裝置主動觸發\n應用程式呼叫 system call，此時送出 interrupt 切換到 kernel space 執行 system call 操作(如讀寫) I/O 裝置對應的 File，觸發 kernel module 運作，這邊專指 device driver 的部分 當 I/O 裝置完成特定動作，如網卡接收到封包 / 硬碟讀取完資料，會直接透過硬體打出 interrupt 訊號給 CPU CPU 會找到 OS 註冊對應的 interrupt handler 處理，類似於 API server 註冊 api route 等 request 近來就到對應的 handler，mapping 過程稱為 ISR ISR 會找到對應的 device driver 處理 補充：這邊我們只探討跟 I/O 相關的議題，所以 interrupt handler 指的是就是以 kernel module 存在的 I/O device driver，其他還有如 timer interrupt handler 等 所以真正有趣的地方在於\nI/O device driver 如何在準備資料的時候釋放 CPU 資源，並在資料準備完成後透過 interrupt 重新向 OS 排程並回傳資料\n以下將以 Linux 為主，探索作業系統與 I/O 裝置的互動\nLinux Kernel module 實作 參考內容 The Linux Kernel Module Programming Guide，現在學習 Linux 真的很幸運有 Jserv 大大的貢獻，用影片與共筆分享在成大教授的課程，並維護這份易讀好懂的 Linux Kernel 開發教學，以下將先以實作切入\nkernel space、user space 與 kernel module 先前提到作業系統是為了統一管理硬體而存在，避免應用程式霸佔資源而使其他應用程式無法使用，在執行層面 Linux 拆成 kernel space 與 user space，kernel space 可以操作所以的資源，而一般應用程式執行於 user space 當中，這樣的保護是基於 CPU 所提供，CPU 有不同的模式可以用型，kernel space 有最高權限 (supervisor mode) 而 user space 則是最低權限 (protect mode)\nkernel module 是在 Linux runtime 可以動態開關而不需要重新編譯 kernel image 的一段程式，可直接在 kernel space 執行，常見為 I/O device driver\n又因為 kernel module 是屬於 kernel 的一部分，所以他們的資源是共享的如記憶體，同樣的如果 kernel module 有問題連帶整個 kernel 都會 crash，以下實驗建議另外開 VM 嘗試，我自己是用 Macbook M1 + Ubuntu 20.20 VM 執行\nhello world - kernel module 參考資料 lkmpg - 4.2 Hello and Goodbye\n讓我們先從最簡單的 hello world 開始，先了解最基本的 kernel module 安裝、卸載與執行過程\n1. hello world 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 /* * hello-1.c - The simplest kernel module. */ #include \u0026lt;linux/kernel.h\u0026gt; /* Needed for pr_info() */ #include \u0026lt;linux/module.h\u0026gt; /* Needed by all modules */ int init_module(void) { pr_info(\u0026#34;Hello world 1.\\n\u0026#34;); /* A non 0 return means init_module failed; module can\u0026#39;t be loaded. */ return 0; } void cleanup_module(void) { pr_info(\u0026#34;Goodbye world 1.\\n\u0026#34;); } MODULE_LICENSE(\u0026#34;GPL\u0026#34;); 編寫 kernel module 時，有以下三點必備\n當 module 安裝時要做什麼：init_module 當 module 移除時要做什麼：cleanup_module 指定 module 的 license：MODULE_LICENSE 需要注意的是，這個 program 在輸出時是用 pr_info 而非 printf，原因是參考 C 編譯過程 在第三階段 Assembly 產出 object code，如果有外部函式庫呼叫會在第四 Linking 階段補上缺少的 object code；\n但是 kernel module 的第四階段不同，他只能解析 kernel 所註冊的 symbol，也就是 kernel 本身提供的 system call，可以在 /proc/kallsyms 查看\n2. 條列 / 安裝 / 移除 kernel module 編譯用的 MakeFile 參考上附連結，產出 module object code 後，可以透過以下指令操作 kernel module\n1 2 3 4 5 6 // 條列 $ sudo lsmod // 安裝 $ sudo insmod {module.ko} // 移除 $ sudo rmmode {module} 可以透過 $ sudo journalctl --since \u0026quot;1 hour ago\u0026quot; | grep kernel 查看近一小時 kernel module 打印結果\ndevice driver 所有的裝置在 linux 中都是 file，資料結構可以參考 include/linux/fs.h，首先要先釐清這邊的 File 只存在於 kernel，雖然中文可能都被翻成文件或檔案，但實際上與檔案系統中的檔案概念是不同的! 後者通常是叫做 inode\n所以每一個裝置在 linux 中都是以一個 file 存在，通常儲存於 /dev/ 底下，而 device driver 則是 user space 應用程式與 device 溝通的管道，流程大致是\ndriver 向 kernel 註冊取得屬於自己的裝置編號與指定檔案 driver 定義檔案操作 1. 註冊裝置 參考資料 lkmpg - 5.6 Device Drivers 先觀察系統中既有的裝置\n1 $ ls -l /dev/ 可以看到類似的輸出\n1 2 3 brw-rw---- 1 root disk 3, 1 Jul 5 2000 /dev/hda1 brw-rw---- 1 root disk 3, 2 Jul 5 2000 /dev/hda2 brw-rw---- 1 root disk 3, 3 Jul 5 2000 /dev/hda3 注意到 \u0026ldquo;3, 1\u0026rdquo; 這類型的字串，前面是 major number 後面是 minor number，分別代表 指定 driver, 裝置 id，每個 device driver 都會被分配一個 id，而可能有多個裝置都是由同一個 driver 所驅動，所以有第二個 minor id 讓 driver 區分不同的硬體\n接著注意到最前的字元，可能會看到 d / b / c 三種，d 代表 directory 目錄，b 代表 block、c 代表 char；\nblock device 是指說操作會以 block 為單位，所以有時操作會被 buffer 後才執行，例如硬碟儲存裝置，可以最佳化讀寫的效率；\nchar device 則沒有 buffer，可以任意讀寫不同的大小，幾乎大多數的裝置都是 char device\n2. 註冊檔案操作 當我們想指定 device driver 如何操作檔案時，會定義 file_operations，指定當檔案被讀取 / 寫入時要對應觸發的 handler function，可以看到以下 struct 中幾乎都是定義 function pointer\n1 2 3 4 5 6 7 8 struct file_operations { struct module *owner; loff_t (*llseek) (struct file *, loff_t, int); ssize_t (*read) (struct file *, char __user *, size_t, loff_t *); ssize_t (*write) (struct file *, const char __user *, size_t, loff_t *); ssize_t (*read_iter) (struct kiocb *, struct iov_iter *); ssize_t (*write_iter) (struct kiocb *, struct iov_iter *); ..... 補充 file 相關資料 Linux 系統程式設計 - fd 及 open()、close() 系統呼叫，file 在 Linux 中必須先被 open() 才能執行後續的讀寫操作，open() 時會把檔案稱作 open file，此時會回傳 file descriptor，資料結構就是上面的 file_operations 紀錄每種動作對應的處理方法\n3. 範例程式碼 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 static struct file_operations chardev_fops = { .read = device_read, .write = device_write, .open = device_open, .release = device_release, }; static int __init chardev_init(void) { major = register_chrdev(0, DEVICE_NAME, \u0026amp;chardev_fops); if (major \u0026lt; 0) { pr_alert(\u0026#34;Registering char device failed with %d\\n\u0026#34;, major); return major; } pr_info(\u0026#34;I was assigned major number %d.\\n\u0026#34;, major); cls = class_create(THIS_MODULE, DEVICE_NAME); device_create(cls, NULL, MKDEV(major, 0), NULL, DEVICE_NAME); pr_info(\u0026#34;Device created on /dev/%s\\n\u0026#34;, DEVICE_NAME); return SUCCESS; } /* Methods */ /* Called when a process tries to open the device file, like * \u0026#34;sudo cat /dev/chardev\u0026#34; */ static int device_open(struct inode *inode, struct file *file) { static int counter = 0; if (atomic_cmpxchg(\u0026amp;already_open, CDEV_NOT_USED, CDEV_EXCLUSIVE_OPEN)) return -EBUSY; sprintf(msg, \u0026#34;I already told you %d times Hello world!\\n\u0026#34;, counter++); try_module_get(THIS_MODULE); return SUCCESS; } /* This function is called whenever a process which has already opened the * device file attempts to read from it. */ static ssize_t device_read(struct file *file, /* see include/linux/fs.h */ char __user *buffer, /* buffer to be filled */ size_t length, /* length of the buffer */ loff_t *offset) { /* Number of bytes actually written to the buffer */ int bytes_read = 0; /* How far did the process reading the message get? Useful if the message * is larger than the size of the buffer we get to fill in device_read. */ const char *message_ptr = message; if (!*(message_ptr + *offset)) { /* we are at the end of message */ *offset = 0; /* reset the offset */ return 0; /* signify end of file */ } message_ptr += *offset; /* Actually put the data into the buffer */ while (length \u0026amp;\u0026amp; *message_ptr) { /* Because the buffer is in the user data segment, not the kernel * data segment, assignment would not work. Instead, we have to * use put_user which copies data from the kernel data segment to * the user data segment. */ put_user(*(message_ptr++), buffer++); length--; bytes_read++; } pr_info(\u0026#34;Read %d bytes, %ld left\\n\u0026#34;, bytes_read, length); *offset += bytes_read; /* Read functions are supposed to return the number of bytes actually * inserted into the buffer. */ return bytes_read; } 結論關鍵的程式碼\n透過 register_chrdev 註冊裝置與指定的 file，系統會返回 major number 註冊 file_operators，可以挑選需要的 handler 註冊 可以注意一下 device_read 註解，裡面有個 system call put_user，因為 driver 是在 kernel mode，而 read 是從 user space 觸發，當今天 driver 想要回傳資料給 user space，不能直接寫入記憶體，而是需要透過 put_user 將記憶體從 kernel space 複製到 user space 執行上的細節就不贅述，有興趣可以看參考資料\nIO blocking / non-blocking 參考資料 lkmpg - 11 Blocking Processes and threads\n當應用程式決定操作 I/O 時，device driver 可能面臨資料尚未準備好的情況，此時會透過 O_NONBLOCK flag 決定是否為 block 應用程式，如果 non-blocking 則直接回傳錯誤 -O_NONBLOCK 讓應用程式晚點重試 (polling)，這也就是 Nodejs 所透過的方式，non-blocking I/O (這句話不全然對，還是要看 libuv 針對不同 I/O 的實作，但 non-blocking 就是指這種情況沒錯)\n或是應用程式選擇 block mode，此時 device driver 還在等待資料的同時，可以選擇透過 wait_event_interruptible 主動交出 CPU 控制權，避免無謂的佔用 CPU 資源； 同時在 wait 之前會先註冊對應的 wake_up 事件，或是收到 signal 會再叫醒原本沈睡的 process\n*圖片參考資料 https://medium.com/@clu1022/%E6%B7%BA%E8%AB%87i-o-model-32da09c619e6\n重新回來看 blocking 與 non-blocking 的圖，這邊是從應用程式角度出發，blocking I/O 會在 kernel device driver 沒有資料時等待，但從 kernel 角度，如果有指定 interruptable 則作業系統會切換到不同的 process 去，所以也不會有資源浪費的問題 (撇除 context switching 開銷)\nsystem call 參考資料 lkmpg - 10. system call\n前面實作了簡單的 kernel module，並看到 device driver 可以在 file 發生變化時產生對應的行為，但一般來說應用程式與 kernel 的互動是透過封裝過後的 system call\nLinux kernel 會有一張 table sys_call_table 儲存支援的 system call 與對應的 address， 當應用程式需要操作硬體需要指定 system call，例如 open() 開啟檔案 / read() 讀取檔案等，在暫存器寫入指定資料後透過特殊指令 interrupt 通知 CPU 要切換 kernel space 執行 (在 intel 中是 0x80)，進階資料可以參考 CPU protection ring\n1. 從 printf 觀察 system call 參考 lkmpg 5.2 Functions available to modules 透過最簡單的 c program printf 來看 system call 的執行\n1 2 3 4 5 6 7 #include \u0026lt;stdio.h\u0026gt; int main(void) { printf(\u0026#34;hello\u0026#34;); return 0; } 打包後透過 strace 查看 system call 狀況\n1 2 $ gcc -Wall -o hello hello.c $ strace ./hello.o 打印出蠻多東西，包含一些 memory allocate 的指令等，最終可以看到 write 的 system call 呼叫\n1 2 write(1, \u0026#34;hello world\u0026#34;, 11hello world) exit_group(0) 2. 修改 system call 如果我們希望修改 system call，理論上可以直接改 sys_call_table 的 mapping，但基於安全性考量這是無法直接在 runtime 操作的，原因是避免 hacker 直接修改 system call\n第二個嘗試可以透過 $sudo grep sys_call_table /proc/kallsyms 找出 sys_call_table 所儲存的實際記憶體位置並替換，但這目前也不行，同樣是因為安全性考量， Linux kernel 在每次 boot 時會動態擾亂 kernel code 跟 data (稱為 KASLR)，增加 hacker 嘗試攻擊的難度\n一個可行的做法是透過 kprobe，這是一個 kernel debug 的工具，當 CPU 執行到中斷點時會保存暫存器狀態，並執行 kprobe 指定的指令，可以透過這種方式去動態調整 system call 執行\n但上面的方法在 production 很危險，試想如果有多個 kernel module 去調整 system call，在 restore 時可能會發生意外，不論是復原錯誤或是執行到已經移除的 kernel module，所以建議是直接重新編譯 kernel\nCPU 與 Hardware interrupt 上面大致描述綠線的走向，從 user space 呼叫 system call，觸發對應 device driver 所指定的 file operation\n接下來看藍線的部分，當 I/O device 收到外部訊號如網卡收到封包、鍵盤被按下按鈕，如何送出中斷給 CPU 並進入後續的處理\n具體內容請參考宅色夫的 Linux 核心設計: 中斷處理和現代架構考量，這邊僅大致提一下流程\nI/O 裝置送出 Interrupt Request (IRQ) Hardware controller 整理後送出 interrupt vector 到 CPU CPU 切換模式立即處理 interrupt，透過 ISR 找到對應的 Interrupt Handler Interrupt handler 中有分成 top half / bottom half，top half 是不會被其他 Interrupt 中斷，所以一定會當下完成；而 bottom half 則會被排入 softiqr 重新排成，soft 在 OS 中有時是代表不確定何時會被完成 實作面的部分參考 lkmpg 15.2 Detecting button presses，透過 request_irq 註冊 interrupt request，當指定的 I/O 發生 interrupt 時就會呼叫註冊的 funtion\n1 2 3 ret = request_irq(button_irqs[0], button_isr, IRQF_TRIGGER_RISING | IRQF_TRIGGER_FALLING, \u0026#34;gpiomod#button1\u0026#34;, NULL); 更多的討論可以參考文件，不同的 CPU 與作業系統有不同的考量，考慮到排程 realtime OS 的設計又會有不同\n延伸：Linux 的 Asynchronous I/O 現在流行的 network I/O 處理方式是透過 I/O multiplexing，如 Linux 的 epoll，而 asynchrous I/O 看起來十分的迷人，應用程式發出請求後就直接等到作業系統通知，中間完全不用等待，但為什麼目前沒有被大規模採納呢？\n*圖片參考資料 https://medium.com/@clu1022/%E6%B7%BA%E8%AB%87i-o-model-32da09c619e6\n參考資料 Efficient IO with io_uring，AIO 在 Linux 2.5 就已經加入了，但持續被詬病例如針對 buffer i/o 還是會變成 synchronous 且 API 難以使用，在 Linux 5.1 後加入了 io_urning新的 AIO API 並持續優化\nlibuv 有開始討論導入 io_uring 的部分，有人提供 benchmark 在讀取檔案部分可以比原本的 thread pool 設計更快\nSO: Is there really no asynchronous block I/O on Linux? 裡面有提供很多的相關連結，資料庫如 PostgreSQL / RocksDB 嘗試用 io_uring 提升硬碟讀寫效能，在 networking 方面也有一些嘗試，在另一篇文章 Epoll vs. io_uring 效能測試與比較看起來效能提升不少，CPU 使用率低且能處理更多的 request，之後有機會再深入研究\n總結 突然間不知如何總結，研究的過程比想像中發散，看了很多文件 XD\n大抵上從 high level 角度理解了整個 I/O 發生的過程，認識到了「應用程式讀寫 IO 的過程」，中間涉及到 system call / kernel module 的執行，以及 OS context switch 的過程，或許我真正想釐清的是 並非把 I/O 變成 non-blocking / asynchronous 系統效能就會無腦提升，memory 的 copy / interrupt handler 處理等還是會佔用 CPU 時間，可以參考另一篇 cloudflare 的整理，真正想做到效能提升有時還是需要 I/O 裝置的升級\n研究的過程還有很多沒解釋清楚的地方，例如 File 的資料如何被 I/O 裝置讀寫，需要在涉獵更多硬體相關的知識，或許該來研究樹莓派了 XD\n總之，也算是稍稍釐清困擾自己多年的疑惑，希望也可以分享給對於應用程式與 I/O 裝置互動有疑問的人，我在每一段都盡可能留下參照的 lkmpg 章節，強烈推薦有興趣可以讀完整篇，對於 Linux 作業系統有基本的認知\n如果有任何不清楚或寫錯的地方，再麻煩留言指教\n","date":"2022-02-06T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2022/2022-02-06-i/o-%E5%90%8C%E6%AD%A5%E8%88%87%E9%9D%9E%E5%90%8C%E6%AD%A5%E5%BE%9E%E7%A1%AC%E9%AB%94%E4%BD%9C%E6%A5%AD%E7%B3%BB%E7%B5%B1%E5%88%B0%E6%87%89%E7%94%A8%E7%A8%8B%E5%BC%8F/","title":"I/O 同步與非同步：從硬體、作業系統到應用程式"},{"content":"最近想了解一下作業系統處理 I/O 的過程，翻閱到 CloudFlare 一系列關於作業系統與 Network Packet 處理的實驗與文章，蠻有趣的稍微整理了一下\n關於 Linux 封包處理 主要在量測 Linux 極限可以處理多少 packet per second / 以及如何持續優化\nLinux 單機處理 1M udp packet per second 文章出自：How to receive a million packets per second，在硬體規格\n1 2 3 4 1. six core 2GHz Xeon processors. 2. With hyperthreading (HT) enabled that counts to 24 processors on each box 3. multi-queue 10G network card by Solarflare 4. with 11 receive queues configured. 可以撐到每秒接收一百萬個 udp 小封包(32 bytes)，發送跟接收端在同個區網的不同機器上\n中間的嘗試迭代過程很有趣\n1. 裸測 僅能到 0.19~0.35 M，發現 kernel 可能隨機分配 process 到不同 CPU 上，透過 taskset 指定 CPU 運行，穩定在 0.35 M 上不去，發現只有單一 CPU 忙碌到 100%\n2. NUMA 架構優化 多核心處理器為了避免多核心在存取 Memory 的貧頸，會採用 NUMA 架構，讓一至多個 CPU core 共用一塊 memory 組成一個 NUMA node； 圖片出處 @lkmem/SkWYoA-TU NUMA 好處是如果 CPU 都只存取 local memory 效率會非常好，如果要存取到不同區域的 memory 或是任務在不同 NUMA node CPU 中 context switch 則效率會很差，作者進行以下比較：\n如果 RX queue 和應用程式是在同一個 NUMA node 的不同 core 上，效能是最好的 如果是在不同的 NUMA node 上，則效能差且不穩定 如果是在同個超執行緒(HT)上，則效能只剩一半 3. 增加 NIC 的 RX queue 數量 過往 NIC 只有一個 RX queue 用來存放接收到的封包，而一個 RX queue 只能由一個 CPU 讀取限制了多核心機器的效能，所以 NIC 目前都會有多條 RX queue\n但是有了多條 queue，NIC 需要把同一個 connection 的多個 packet 送到同一個 queue 中讓同一個 CPU 處理，否則會遇到 packet order 亂掉的問題，此時可以透過 hashing 解決，hash function 為 {ip, port}\n後來因為實驗的 NIC 無法調整 hash 機制，所以改用增加 IP 的方式，但目的也是要把 packet 分散到多個 RX queue 上，此舉增加到 0.47M\n4. 加開 recv thread 原本是用 single thread 接收，嘗試開啟 multi thread 但反而性能下降，原因是多個 thread 共用同一個 file descriptor 需要額外 lock\n但好在 Linux 在 3.9 後加入了 SO_REUSEPORT 可以讓多個socket descriptor 綁定到同一個 port 上，所以讓多個 thread 可以分擔讀取的工作，但作者同樣觀察到 SO_REUSEPORT 這一層同樣有分散不平均的問題，只有幾個 CPU 特別忙碌\nOne of the features merged in the 3.9 development cycle was TCP and UDP support for the SO_REUSEPORT socket option; that support was implemented in a series of patches by Tom Herbert. The new socket option allows multiple sockets on the same host to bind to the same port, and is intended to improve the performance of multithreaded network server \u0026hellip;..\n\u0026hellip;. Incoming connections and datagrams are distributed to the server sockets using a hash based on the 4-tuple of the connection—that is, the peer IP address and port plus the local IP address and port.\n小結 單機 Linux 優化後可以做到每秒接收 1M UDP packet\n記得將 packet 分散到 RX queue 接收端應用程式要打開 SO_REUSEPORT 並透過 multi thread 讀取 要有多的 CPU 負責從 RX queue 讀取 如果是 NUMA 架構，RX queue / 應用程式的 CPU 要在同一個 Node 性能比較好 為什麼我們需要 Linux kernel 處理 TCP stack? 文章出自：Why we use the Linux kernel\u0026rsquo;s TCP stack\n為什麼我們需要 OS 試想一個問題，如果我們一台 server 上只跑一個專用的應用程式，那為什麼我們需要先額外安裝一個上千萬行的 OS 去代理執行我們的應用程式？\nOS 主要提供幾個好處：\n抽象化硬體、提供統一介面：\nOS 會抽象化底層的硬體，讓應用程式專注於開發而不用管實際執行的硬體為何，帶來更好的移植性 資源管理：\nOS 可以透過排程讓一個硬體於多個應用程式間切換使用，而不會有一個應用程式霸佔整個硬體資源，例如網卡可同時處理 server request 也可用於 ssl sesson 為什麼需要 userspace TCP stack 如果透過 OS 則讀寫硬體資源時需要涉及 user space / kernel space 資料的複製與 context switch，這會帶來額外的 latency / CPU performance 影響\n例如 Google、Cloudflare 這種網路流量非常大的公司，就會有動機去客製化 TCP stack，所謂的 userspace TCP stack 就是 繞過(bypass) OS Kernel直接存取網卡，這張網卡就專門被單一的應用程式所使用，OS 的監控工具 (iptable/netstat) 等都無法使用，其他應用程式也都不能\nCloudflare 有提到透過 Linux iptable 大概可以處理每秒一百萬的 packets，而 Cloudflare 在遭遇攻擊時流量會在 單台 server 每秒三百萬的 packets，這也是他們需要繞過 kernel 自行處理的動機\n但目前沒有穩定的 open-source userspace TCP stack 可以使用，要自己維護開發，除了執行外周邊的 debug、monitor 工具都要自己想辦法成本很高，所以 Cloudflare 採取半繞過的方式，只有 \u0026ldquo;RX queue\u0026rdquo; 會繞過 kernel，這帶來的好處是享有一定的效能提升且其餘 packets 處理能仰賴既有的工具如 iptables/netstat\n優化 latency 文章出自：How to achieve low latency with 10Gbps Ethernet\n前面優化了 throughput，現在要來看 latency 如何被優化，量測的基準線從平均 47.5 us 開始\n1. 提高 read 的頻次 Linux 3.11 開始增加了 SO_BUSY_POLL 的 socket 選項，在指定的時間內 kernel 就會去讀取 packet 提升讀取的頻次，透過增加 CPU 使用率降低 latency 7 us\n同樣的道理可以套用在 user space ，透過 non-blocking read fd.recvmsg(MSG_DONTWAIT) 再往下降低 4 us\n2. pin process 透過指定 process 在特定的 CPU 上執行減少 context switch，降低 1 us；\n但如同先前提到，預設一個 RX queue 只能有單一 CPU 讀取，如果剛好應用程式也在最忙碌的 CPU 上，則延遲反而會增加 2 us\n3. 將 RX queue 綁定到指定 CPU 為了避免上面問題，可以透過 Indirection Table 調整 RX queue 如何分配到 CPU 讀取，作者限定只有在同一個 NUMA node 的 CPU 才能讀取 RX queue\n同樣的做法也可以用 Flow steering 實作，指定特定的 flow 到特定的 RX queue 上面，額外用途是確保在高流量情況下指定的封包還能被特定的 CPU 處理，如 SSH/BGP，也可以用來防堵 DDoS 攻擊\n1 client$ sudo ethtool -N eth2 flow-type udp4 dst-ip 192.168.254.1 dst-port 65500 action 1 在傳送端也可以指定類似的事情，調整 TX queue 透過哪個 CPU 發送的機制\n小結 再通過一些優化，從原本 47 us 降到 26 us；如果是透過 bypass kernel 則近一步下降到 17 us\n","date":"2022-02-02T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2022/2022-02-02-%E7%AD%86%E8%A8%98cloudflare-%E5%84%AA%E5%8C%96%E5%B0%81%E5%8C%85%E6%8E%A5%E6%94%B6%E7%9A%84%E9%81%8E%E7%A8%8B/","title":"筆記：CloudFlare 優化封包接收的過程"},{"content":"在 2022 年一開始就上到如此扎實、有收穫的課程真的是太讚了，之前在同事的介紹下認識了 Event Storming 並接觸到一些 DDD 的名詞，開始在網路上透過影音課程 Domain-Driven Design Fundamentals、鐵人賽系列文章學習，在上課前也大概翻了一下 DDD 藍皮書，以上的教材都很不錯，也把名詞解釋跟 DDD 流程大致梳理一遍，但是蓋上書本、關掉網頁，我要怎麼導入？\n例如說 DDD 提到開發人員要找領域專家一起找出 Domain Model，並將 Domain 劃分出適合的 Bounded Context，但什麼是適合？我該怎樣知道不適合？當我找到適合的\u0026quot;感覺\u0026quot; A-ha Moment 是怎樣會發生？\n在翻閱《Clean Architecture》一書也有同感，文章寫得非常清楚，也透過 Uncle Bob 的文筆重新認識了軟體架構，但然後呢？我該怎麼實作？\n在 DDD 一書闡述了軟體的建立需要攜手 Domain Expert 定義 Domain Model，並確保程式實作要跟 Model 保持一致，在圖表中拆分了 Entity / Value Object 等，以及提供很多元件間溝通的 Patten，但沒有說實作起來會怎樣 (講白了書本中沒有太多的程式碼) 在 《Clean Architecture》 中描述的 SOLID / 分層原則 / 依賴性原則，但沒有說元件所謂的職責怎麼劃分，有很多實作的細節跟取捨並沒有太多案例\nTeddy 將兩者結合，用自己設計的 Kanban 系統，帶著學員跑過一次 DDD 建模的過程，並透過《Clean Architecture》實作，他說他很接近理想的軟體\n改動的成本只跟需求範圍有關，跟系統存在的時間無關\n透過一個階段一個階段的討論與檢討，先試錯再回頭解釋名詞，這樣的學習方式我覺得比看書、看影片還要好很多，以下我將用時間軸分享 Teddy 上課的內容以及我們小組討論的演進，會有大量錯誤然後被糾正 ~打臉~ 的案例 XD\n內容有點細，不會完整介紹 DDD 跟 Event Storming，只會分享在實作上錯誤與老師的糾正\n推個課程頁面 領域驅動設計與簡潔架構入門實作班，實際參與小組討論會清晰很多\nDomain Driven Design DDD 基本介紹 1. 什麼是 Domain Domain 可以被拆分為 Problem Domain 以及 Solution Domain，Problem Domain 是指實際發生在世界上的問題，也就是公司所面臨的商業問題，例如外出想要叫車、肚子餓想要點外賣等，但是世界上的問題太多了，不會每一個都關注，所以總合來說\n利害關係人所在乎的實際問題就是你的 Domain\nUber 在乎用戶外出的通勤問題 / Food Panda 在乎用戶肚子餓想點外賣果腹的問題，各自有各自的 Problem Domain，並對此提出各自的解法 Solution Domain，之後討論將 Solution Domain 限縮在軟體設計上\n回歸開發的根本，為什麼我們需要一直跟 PM / 利害關係人 (後續以領域專家代稱) 不斷釐清問題，因為問題才是驅動一切的本質，如果今天不用寫一行程式碼就能解決問題，那何必動手，釐清問題是開發的第一步\n2. 什麼是 Domain Model 有了 Problem Domain，過往的習慣是直接進入開發設計，也就是對應到 Solution Domain，但這兩個 Domain 間有一個很大的鴻溝 (附圖路徑 A)，開發人員以為自己聽懂了需求就開始實作，而領域專家也以為開發人員真的懂了，最後等系統驗收時才發現根本做錯了\nDDD 提倡領域專家與開發人員應該要先達成共識，將 Domain 建立出對應的模型 Model，讓雙方將共識轉換為圖形，並在過程中建立 Ubiquitous Language 共同語言，等確認後開發人員才去實作 (附圖路徑 B)\n比對路徑 A 、B，看似 B 繞了點遠路，但先透過 Domain Model 闡述 Solution 的長相並抽離實作，才能用最小的成本，讓領域專家及早確認解法的方向正不正確\nQ: 是不是所有問題都要建 Domain Model?\nA: 不是，只有當商業邏輯足夠複雜才需要，單純的 CRUD、報表系統是不需要的\nEvent Storming 是一個建立 Domain Model 的方法，從 High Level 到 Low Level 持續演化的過程\n3. Domain Model 要越真越好嗎 所謂的「真」是指實際世界的真實性，Teddy 上課舉例：「樣品屋是不是越像真的實體屋越好？」\n答案是「不一定!」，重新思考 Domain Model 是 Solution Domain 的一環，所以真正的重點是源頭的 Problem Domain，今天建商蓋樣品屋是希望用戶購買實體屋，但如果今天建商蓋完就銷售一空，那根本連樣品屋都不用蓋\n只要 Model 能夠剛好解決問題就好，over design 或 under design 都是不好的，這點在後續的實際操作會再補充\nEvent Storming 以 Kanban 系統 ezKanban，Teddy 擔任領域專家 (兼任開發人員XD) 帶大家跑過 Event Storming，以下介紹 Kanban 的核心商業邏輯\nVisualize：視覺化表達 Limit WIP：每一個工作階段的進行中工作 (WIP) 有數量限制 Manage Flow：管理工作流程 建模工具使用 Miro 進行順序按照階層從 Big Picture 開始與領域專家討論核心的用戶行為，接著到 Process Modeling 補充更多的規則、行動，最後才是 Software Design 1. Big Picture: 找出 Domain Event 先拉出一條箭頭表達時間軸，先後順序，將 User Story 中的關鍵事件 (Domain Event) 先條列出來，所謂的事件是會改變系統狀態，例如電商系統用戶加購物車、結帳等，反之讀取不是事件，因為不會改變系統狀態，不管 Data 讀取幾次都不會有變化\n事件是代表發生過的事情，所以都會用過去式表達，例如看板系統中的\nWorkflow Created Workflow Deleted Stage WIP set 等等 在討論過程中，務必記得\n不要出現技術用語，例如資料庫怎麼儲存、實作要套什麼 Design Patten 不要被 UI 綁架了! 因為我們是看著 Teddy 已經實作好的 ezKanban，所以 Domain Event 很多，實務上看各自系統規模，而且 Event Storming 是持續演進，不用求一步到位 上圖是我們小組建立 Domain Event，其中有三點錯誤\na. 時間軸順序性 我們一開始把 User Created / User Logined 放在同一列上，同一列在 Event Storming 代表這兩個事件是在差不多時間點發生，但理論上 User Create 必然在 User Logined 之前，所以不該放在同一列上\nb. 不要被 UI 綁架 這是一個非常有趣的案例，在 ezKanban 中你必須在畫面上點擊「Layout Edit」才能編輯工作流程的框架，所以我們組內討論很直覺的想這個事件很重要，所以就補了一個 「Layout Mode Entered」，但老師說不對!! 被 UI 綁架了，這只是一個實作細節，我今天可以換一個按鈕或 UI 流程做到同一件事，這屬於應用層面的邏輯\nDomain Event 又可細分成兩種 Core Domain Event / Application Domain Event，前者是滿足核心商業邏輯的事件、後者是應用程式執行時需要的事件，今天以看板系統，用戶哪會關心什麼 Layout Mode，那是你實作的事情，所以不該把這個 Event 放在上面\n我自己後來在反思這件事情，我嘗試用這樣的邏輯去釐清\n我能不能用不同的 UI Flow 取代當前的事件？ 例如在 Board 頁面就區分出「版位調整」/「工作調整」，如果可以的話，那就是應用事件 今天我實作在多平台 Web / App / Server 都需要這個事件嗎？如果要那有可能是核心，如果只有 Web 要實作其他平台不用，那肯定是應用事件 這麼在意是不是 Core Domain Event 的用意是讓 Domain Model 保持簡潔，太多細節參雜會混淆討論\nc. 不要被 DB 綁架 - 任務驅動而非指令驅動 在條列 Domain Model 時，我與組員都有一個疑問 「一個 Stage 有 10 個屬性都能夠 CRUD，那我要全部列出來嗎？例如 Stage 的標題、說明欄等，那我是要寫一個 Stage Updated 就好還是要寫 Stage Name Updated？」\n除了不要被 UI 綁架外，也不要被資料庫綁架了! 今天用戶才不管你怎麼 CRUD，他真正在意是能不能完成他手中的任務，例如說今天我去 ATM 領錢，系統顯示\n說法 A: 您已提領 1000 元 說法 B: 已更新您的帳戶餘額為原餘額減去1000 元 相信你提款後看到說法 B 應該會傻眼，請以「用戶想要完成什麼任務」的角度描述 Domain Event\nd. 用說故事的方式去釐清 Model 表達力 我在操作 ezKanban 先跑了建立工作流程，最後才發現 ezKanban 有 Team 的概念，Team 可以邀請其他 User 當作 Member 共享 Project，因為我比較晚看到所以 Team 相關的事件放在時間軸後面\n最後 Teddy 問說：「這樣的 Model 表達了什麼含意？」Model 的意義在於圖形化我們想要解決的問題與發生順序，也就是用戶的使用歷程與每一個使用案例，今天用說故事的方式會像\n故事 A:「用戶今天想要使用看板系統，他先註冊了帳號並登入，接著建立專案，並開始設定自己的工作流程\u0026hellip;\u0026hellip; 最後他邀請他的成員加入，大家一起操作看板」 故事 B: 「用戶今天想要使用看板系統，他先註冊了帳號並登入，接著建立團隊，邀請他的成員加入，大家一起操作看板，接著建立專案，並開始設定團隊的工作流程\u0026hellip;\u0026hellip;」 兩個故事都說得通，但 Teddy 覺得故事 B 更符合他的使用場景，團隊的建立會在工作流程之前，所以調整後的 Domain Event 大致長這樣 2. Big Picture - 劃分 Bounded Context 洋洋灑灑列出 Domain Event 後，接下來要來分群，想像成是買房畫隔間，設計圖應該要能清楚辨識出這是一個商務辦公室還是一個小家庭自住用，透過隔間去表達系統的意圖 *附圖從 Google 搜尋，不用懂室內設計但也大致能看出左右兩張圖的不同\n我們小組一開始思考方式是「不然就每個物件一間，Workflow 一間 / Board 一間等」，如圖示 Teddy 看到我們如此設計，他反問說「你們的隔間有反應出 \u0026ldquo;看板系統\u0026rdquo; 這個意圖嗎？如果從外部使用者的角度，他看到 Workflow / Board / Stage 這麼多細節對嗎？」\n正確的隔間應該是 區分成三塊：\nCore Domain：Kanban 是我們的核心領域，千萬不要外包 Generic Domain：User / Team 屬於通用性功能 Support Domain：圖上沒有，如金流、外部通知系統，這種的找外包即可，不一定要自己開發 如何知道自己的隔間是否正確？回到商業邏輯與說故事的方式，劃分 Bounded Context 後是否能正確拆解出公司的核心與非核心商業邏輯\n大家最關心的微服務，通常是一個 Bounded Context 一個部署的元件\n3. Big Picture - 加入 Role / Externel System 加入觸發的角色 / 外部系統，這一步驟相對單純，角色是系統中動作的執行者 HotSpot 是當討論過程卡住、發生意見不同、命名不同時可以貼著註記，避免討論被卡住，最後可以看哪一個區塊是大家最沒有共識的\n4. Process Modeling - 加入 Command / Read Model 這一步相對單純\nCommand 是觸發 Domain Event 的動作，基本上就是把完成式改成現在式表達 Read Model 則是完成 Command 所需的參數 a. 建立事件是否要傳入 id 有趣的小細節是 Create Board 時我們把 board_id 也寫出來，小組討論時會覺得 DB 會自動產生 id 所以不用寫到 read model，但 Teddy 說 「錯，我哪管你 id 是 DB 產生還是前端用 UUID 產生，這是實作細節，Board 就是需要 id 識別」，再次強調，不要陷入 UI/DB 的細節\n5. Process Modeling - 加入Policy Policy/Rule/Process 是指 事件完成後觸發的流程，千萬不要跟動作需要完成的驗證搞混，例如說\n輸入 Email 註冊時，Email 必須符合格式 =\u0026gt; 這是驗證 密碼輸錯三次後，需要封鎖帳號 =\u0026gt; 這是 Policy 驗證是實作細節在 Event Storming 中不表達，Policy 才是我們現階段要關注的，以下是舉例「全家推出領取包裹後，可以八折買拿鐵」 這邊偷夾帶一張白色的便利貼，Teddy 說他確實也遇到有些驗證是重要的商業邏輯，他自己變形增加了白色便利貼，表示 Command 的驗證規則\n到這一步我們可以發現 Event Storming 很好的描述了一個故事 「User want receive package, we have to check his id card, after user have received package, he can purchase latte 20% off」\n這就是 Event Storming 的魅力\n6. Software Design - 加入 Model 接下來進入實作細節，Model 可以想做是物件，比對 Command 應該是哪一個物件負責，條列後最終把 Model 的大致關係也描繪，大致如下 因為 Stage / Swimlane 一者是橫向一者是直向，兩者可以互相包含，所以圖表有點複雜，圖表複雜代表程式碼實作一定也不好寫，該怎麼優化呢？\n抽一個 Lane 代表 Stage / Swimlane，這樣關係就單純很多 更精準表達商業邏輯，另外一個重點是 Workflow 與 Stage 關聯而不是 Lane，因為 Workflow 預設第一層必須是 Stage，透過圖表表示出核心的設計邏輯 a. Model 拆分很看商業邏輯 同學在課堂上詢問：「如果我有多個支付，我是不是要把每個 CreditCardPay / ApplePay 都寫成一個 Model？」 Teddy 反問：「為什麼需要？如果 Pay 只是一個付款手段，如果沒有很大的差異，一個 Pay 表達實作時再拆分就好」\n我接著提問：「那為什麼 Stage / Swimlane 要拆分？他們也只是一個橫的一個直的」\nTeddy 表示 因為在看板的 Domain 中，直的代表工作階段 / 橫的代表工作流程是截然不同的，這是非常核心的商業邏輯，所以 Model 要拆到多細，要不要真的把每個實作的物件都表達，完全看領域專家與開發人員是否很重視每個元件的獨立性與表達性\n7. Software Design - 找出 Aggregate 最後一步! 也是很抽象的一步，將 Model 分群，有些 Model 明顯是其他 Model 的附庸 ，例如說訂單細項 Order Item 是訂單 Order 的附庸，每次 Order Item 的改動會影響 Order 的計算，會進階影響例如折價券等計算，所以 Order Item 最好不要直接調整，由 Order 統一調整才能確保資料的一致性\n要不要把 Model Aggregate 成一塊有兩個作用力\n資料一致性 併發操作 Teddy 帶我們思考的方式是\n今天我改動 Model A，那我可不可以同時改動 Model B ? 我在 Workflow name 的時候，可以同時操作 Board 嗎？\n切記不要用 Database Relation 思考，例如 User 刪除 Order 也要跟著被刪除，這樣 Aggregate 永遠切不開，更直觀地說 要包 Transaction 操作的都就放在同一個 Aggregate\n找出 Aggregate 後，需要找一個 Model 當作 Root，往後 Aggregate 內的 Model 都由他來控制，外人不可直接操作內部 Model，任何跨 Aggregate 操作只能保證最終一致性\n延伸變體：還是很想把讀取也放入 Event Storming 可以嗎？ Teddy 上課一直強調讀取隨便寫就好，怎麼髒都沒關係因為 Command 才會影響系統狀態 XD Query 效能問題等又是 DB 細節，本身也不是 Event Storming 該關注的層級\n但協作上大家還是想把 UI 放到討論中 / Read Model 如果有前端可能也希望在寫得細一點，這些都可以自己延伸，Teddy 也分享有些流程 UI 可以幫助說明的話他也會放進去，例如 Command 後回傳另一張 Read Model / UI 圖示放在 Read Model 旁邊等 延伸問題：開發只要這一份文件就好了嗎？ 文件的維護也是我課前很想了解的部分，在 DDD 藍皮書中不斷強調 Domain Model 跟程式碼實作要高度一致並持續演進，但工作多年有良好維護文件習慣的人還真的是少數，尤其是文件越多份維護的成本就更高，所以我課後就問 Teddy 他們團隊開發是不是只有這一份文件，他說基本上是，資料庫部分因為他是走 Event Sourcing 所以不用另外的 Schema 設計文件，如果不是頂多再一份 DB Schema 文件就足夠了\nDDD - Tacticle Design \u0026amp; Strategic Design 跑過一次 Event Storming 再回來看 DDD 廣為人知的兩張圖就比較能理解了，這邊只額外補充兩件事\n1. Entity 與 Value Object 區分 前者是具有 id 在 Conext 下有唯一識別性的物件，後者是內容重要但是不是同一個物件不太關心，例如說紙鈔，大多系統下我們只關心紙鈔的面額，甭管你是左邊的 100 元還是右邊的 100 元，所以是 Value Object / 但如果是印鈔系統，同樣是 100 元還是要用 id 區分不同的紙鈔，這時就是 Entity 了\n2. 如果有驗證需求，可以考慮用 Value Object 一個實作的小細節，如果以往都是用欄位儲存一個基礎型別，例如 string email，要增加驗證就會很瑣碎，封裝一個 Email 物件並在 constructor 統一驗證會比較簡潔\n結語 重點小整理：\n以商業核心為出發 用說故事的方式跑 Event Storming 不要落入實作細節 如果有需要，可以自己變體適應組織 上完課自己重新覆盤，沒想到花這麼多時間才整理完，DDD 用靜態的學習方式真的很抽象，大腦一直轉不過去，上完 Teddy 的課才覺得那扇門被打開了一個縫，看到理想軟體開發的光明 XD\n這一篇主要整理 DDD，下一篇預計整理 Clean Architecture 與如何跟 DDD 結合，希望這些紀錄對大家有幫助，最後在幫 Teddy 老師推廣一下他的課程，只有實際跑過一次Event Storming 才能真正學會，如果有什麼建議或糾錯再麻煩留言\n","date":"2022-01-14T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2022/2022-01-14-%E9%A0%98%E5%9F%9F%E9%A9%85%E5%8B%95%E8%A8%AD%E8%A8%88%E8%88%87%E7%B0%A1%E6%BD%94%E6%9E%B6%E6%A7%8B%E5%85%A5%E9%96%80%E5%AF%A6%E4%BD%9C%E7%8F%AD%E4%B8%8A%E8%AA%B2%E7%AD%86%E8%A8%98%E8%88%87%E5%BF%83%E5%BE%97%E9%97%9C%E6%96%BC%E6%95%8F%E6%8D%B7-ddd-%E8%88%87-event-storming-%E4%B8%8A/","title":"「領域驅動設計與簡潔架構入門實作班」上課筆記與心得：關於敏捷、 DDD 與 Event Storming - 上"},{"content":"第四章：分解資料庫 上一章是用系統的層級討論如何拆分，但系統最難拆分的部分無疑是 資料庫，所以作者花了很大的篇幅描述多種資料庫拆分的方式\n分解模式 模式一：共享資料庫 所有的服務共享一個資料庫，這是一個不太好的選擇，因為無法保證誰控制了資料，每個服務都有讀寫、修改 Schema 的能力，這暗示了缺乏商業邏輯的凝聚力 模式二：資料庫視圖 如果仍希望多個服務間共享一個資料庫，可以透過視圖 (View Table) 減輕耦合上的困擾，因為視圖可以有獨立的權限管理並隱藏底層實際儲存的邏輯； 視圖的功能取決於資料庫本身的限制，要小心資料是否過時等問題 模式三：資料庫包裝服務 將資料庫隱藏在服務後面，從資料庫依賴轉變成服務依賴，這可以當作轉移的第一步，先避免資料庫耦合持續惡化，透過一層服務封裝與隱藏實際的資料庫邏輯\n書中案例是銀行的權限管理，多個服務因應不同的場景持續堆疊新的權限管理在資料庫中，透過拆分出權限服務，將權限調整限縮單一服務，後續再考慮拆分獨立資料庫，有點像上一章提到的抽象分枝\n跟視圖相比的好處是不需要映射現有的資料表，且可以透過程式邏輯控制更複雜的操作，只是上游介接的服務需要調整 模式四：資料庫服務介面 如果客戶端是 BI 工具需要直接透過 SQL 端點讀取，最好是建立專門的資料庫作為公開的讀取端點，對映引擎負責同步資料，可以透過批次處理、串流同步工具 (作者推薦 Debezium)，處理的複雜度仰賴於內部資料庫與對外資料庫的差異，如從 Cassandra 映射到 SQL DB 先分割資料庫還是程式碼 先分割資料庫 要小心跨資料庫 Join、一致性與完整性問題，如果要特別注意資料一致性問題可以採取此方法，缺點是短期效益不大，依然要面臨單體式架構\n可以透過程式中的 Storage Layer 將 Domain 對應到獨立的 Table / Database\n先分割程式碼 先分割程式碼更容易了解新服務需要的資料，也能夠提早獨立部署新服務，但有可能拆分完程式碼就沒有繼續拆分資料庫\n資料一致性 如果需要針對不同的 Domain 操作可以在 SQL 資料庫中用 Transaction 包起來確保原子性，但是分散式架構就需要用別的方式確保原子性\n方法一：二階段提交 透過一個協調者，與多個服務溝通並確保大家一起 commit 或一起 rollback，演算法複雜且效率差，如果居中的協調者失敗則會出現非一致性問題 方法二：Saga 避免長時間鎖定資料，將每個執行步驟模組化獨立運作，個別服務都要處理異常事件時的補償行為，參考 Saga 分散式交易模式，但要小心 Saga 模式的複雜度很高 第五章：成長過程中的痛苦 導入微服務會遇到很多的挑戰，尤其是當團隊人數與微服務數量持續增加時，可能會開始出現以下問題\n1. 大規模所有權 當要改動程式碼時，會依照程式碼所有權概念而有所不同，如果是強大的程式碼所有權則需要再修改前告知持有者；弱所有權或是集體所有權則是直接修改不需要額外通知\n通常一開始都是集體所有權，大家分工開發而沒有特定的任務，但是當人數超過 100 人，通常好的團隊都是強所有權，讓每個團隊專注於特定領域\n2. 重大變革 當微服務改動時，如果沒有謹慎思考破壞原本的介面，則相依的服務也會跟著被破壞，例如提升 API 版號時，記得要公告並保留一定的支援時間\n3. 報告 過往單體式資料庫要拉報表相對簡單，但如果拆分微服務，不同團隊採用不同的資料庫，要確保最終再產生報告時有同步相同的資料\n4. 監控和疑難排查 分散式系統的監控與排查難度會比單體式高非常多，可以使用 ELK / Fluentd 等技術彙整，並增加系統的可觀測性，Observability 的概念算是把 Monitoring 再往前推進一步，可以參考 喬叔帶你上手 Elastic Stack - 探索與實踐 Observability：01 - 前言 \u0026amp; 淺談 Observability；在追蹤部分，可以把每一個呼叫都產生對應的關聯 ID，追蹤在不同微服務間的通信，可以參考開源工具 Jaeger\n5. 當地開發者經驗 當有更多服務時，要在本地端開發變成一場夢魘，雖然說有 Docker 等容器化技術，但如果像 JVM 需要吃大量資源，可能會耗盡開發者的本地端資源，如果是使用 k8s 可以考慮用 Telepresence，部署完整服務在雲端，開發者可以運行特定服務在本地端其餘依賴雲端的服務\n6. 全域與本地優化比較 雖然說微服務可以讓各個團隊彈性的採用技術，但從整體的角度太過分散的技術棧可能會導致費用上或是維護上的困難；但如果太集中化管理，強迫各個團隊採用相同的技術又失去了微服務的部分好處\n這可以用前幾章描述的可逆與不可逆決策，如果是不可逆如雲端供應商等，則需要跨團隊的討論，但可逆的決策如採用函式庫、嘗試新的程式語言則可以放手給團隊決定，最好是可以讓每個團隊出一名技術負責人組成跨部門小組，同時兼容全域與本地優化的可能\n總結 以上是從書中摘錄的內容，因為還沒有太多的實際經驗，只能照本宣科，無法像安德魯大叔寫出抽象化程度夠也包含足夠扎實的經驗分享，期許自己未來有更多經驗能夠重新補充文章內容\n好的架構設計是持續演進的，建議是從 Monolithic -\u0026gt; Modulized Monolithic -\u0026gt; Microservices，如果無法模組化單體式，那只會得到更糟糕的微服務，微服務本身透過獨立的服務形成高強度、難以違背的模組化，問題就回到程式設計的最源頭如何設計出高內聚+低耦合的模組，往下預計還有幾個知識點的延伸 DDD / Event Storming / Clean Architecture / Observability (遠望\n","date":"2021-12-05T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-12-05-%E5%96%AE%E9%AB%94%E5%BC%8F%E7%B3%BB%E7%B5%B1%E5%88%B0%E5%BE%AE%E6%9C%8D%E5%8B%99%E8%AE%80%E5%BE%8C%E5%88%86%E4%BA%AB-%E4%B8%8B/","title":"《單體式系統到微服務》讀後分享 - 下"},{"content":"微服務因應容器化技術、持續整合與交付工具成熟，成為顯學好一段時間了，但這不代表我們就應該導入微服務，終究微服務只是一項技術(或說是架構)，如果沒有設定一個正確的目標，並用適當的指標時時關注，那導入任何新技術最後都會是一場災難，微服務更是； 我很喜歡這篇分享，先檢視團隊的 CI/CD、monitoring 機制是否健全再來思考微服務 https://www.facebook.com/hatelove/post/10223238347681539\n這部影片是 Sam Newman (本書作者)與 Martin Fowler 對談，，主要從寫書的動機開始，後面談到了為什麼要導入微服務、導入理由以及資料庫、團隊如何應對等，基本上跟書的結構互相呼應，GOTO 蠻用心的影片一個段落就會打 Tag 以及統整，後續要回顧很方便；後來經歷才知道 Sam Newman 之前 在 ThoughtWorks 工作 12 年\n以下針對每章節做一些分享與總結，並融合一些看過的資料\n第一章：足夠的微服務 微服務是泛指圍繞業務領域建模的可獨立部署之服務，這邊的重點有兩個\n圍繞業務領域 可獨立部署 1. 圍繞業務領域 Conway\u0026rsquo;s Law: 任何設計系統的組織，都將不可以避免的產生以組織通訊架構為副本之設計\n組織的架構某種程度跟程式架構雷同，都應該追求高內聚低耦合，否則會因跨組別的高溝通成本，而讓組織的生產力下降，進而讓大家選擇對自己最方便的解法而非全局最佳解\n把康威定律轉成工程師的比喻會是\nIf you have four groups working on a compiler, you\u0026rsquo;ll get a 4-pass compiler\n過往組織的分組是透過職能，如 RD 組成一個 RD 部門負責、IT 部門負責部署與維運、業務部門負責產品端的其餘事項，所以業務端發起需求後，需要先與 RD 部門溝通、RD 部門開發後請 IT 部門部署，層層的跨部門溝通讓新功能上線很慢，此時組織的重點是以職能內聚而分業務內聚，延伸分享之前整理的筆記 Fred聊聊SOLID設計原則，SOLID 原則中的 Single Responsibility 可以從業務角度去思考\n因應軟體需要更高頻次的功能部署，開始以業務領域來做組織的架構，例如 Amazon 的 two pizza team 追求you build it, you run it的精神，讓每個團隊從需求設計到上線反應速度更快\n2. 獨立部署 組織依照業務領域切開後，會需要可以隨時按照團隊的節奏部署服務，並確保其餘服務不受影響，這也就是獨立部署的重要性\n在設計服務，要確保服務的低耦合，否則會陷入更新一個服務要連帶更新其餘多個服務，變成可怕的分散式單體式架構，這就延伸到開頭分享的 FB 文章 - 團隊是否有良好的 CICD、是否有良好的 Log 架構與 Tracing 系統\n3. 自身擁有的資料 微服務不應該共享資料庫，如果其餘服務需要資料，應該要透過服務介面而非直接存取資料，避免程式碼拆分最後卻在資料庫耦合，消抹了獨立部署的功用\n微服務好處 主要體現在靈活性\n部署獨立性：改善系統規模與強健性 (Robust)、可混用不同技術 更好分工：不同團隊專注不同的 code base 單體式架構 1. 單一程序 將所有程式碼放入單一程序部署的系統中\n2. 模組化單體式 將單一程序模組化，讓每個模組都能獨立運作，只是部署會統一部署，甚至可以讓不同模組使用不同的資料庫；非常推薦 Shopify 的分享 Under Deconstruction: The State of Shopify’s Monolith，預計之後再展開討論\n耦合與內聚 內聚是指「程式碼同變動、共留存」，當今天改 A 功能時，僅需要確保 A 功能相關的服務改動，降低變動成本\n耦合是指「資訊隱藏」，把相對經常性變動的部分與靜態的部分分開，有個穩定的模組邊界，又可細分成實作耦合、時間耦合、部署耦合、領域耦合\n小結 第一章定義了微服務、單體式架構，並分析背後設計原理與最終反應的架構優劣，但需要小心 單體式不等同於傳統，不要污名化單體式，應該要用更客觀的角度權衡架構選擇\n最後作者推薦 DDD 協助業務領域的定義與拆分\n第二章：遷移計畫 微服務是個技術選擇而非目的，再導入前應該先謹慎思考\n想要解決的問題是什麼？想要解決的問題跟公司利益是否一致？用戶是否會因此受益？ 是否比較過替代方案？是不是其他「無聊的技術」就能解決問題？ 如何衡量導入微服務的成效？怎麼再導入過程知道方向是否正確？ 為什麼選擇微服務 如果是因為下列原因而想要導入微服務，可以先思考看看替代方案是否可行\n導入目的 微服務的潛在優點 替代方案 增加團隊自主性 拆分微服務讓團隊的規模小、擁有相對權力，可以更有效的工作 分配責任有很多方式，並不一定要改架構，可以將程式碼所有權分屬不同的團隊，像是採取模組化單體式結構，例如 shopify 縮短上市時間 單獨針對微服務進行變更與部署 在思考生產力問題時，應該要先檢視開發流程的貧頸在何處，作者提到他的顧問生涯中發現是需求傳遞過程耗費最多時間而非開發，所以應該先審視與量測軟體開發的每個步驟 有效擴展負載 單個微服務可以獨立擴展，更動態的調整個別服務的規模 單體式的水平擴展依然是很有效的擴展方式，如果當下遇到效能貧頸，直接擴展而非導入微服務可能是相對快速又有效的選擇 增進強健性 拆分微服務後，可個別依據業務需求而區分出核心與非核心的微服務，進而提供不同的強健性調整 透過 Load Balancer、Event Queue 搭配單體式系統的水平擴展，一樣可以增進強健性，把資源投注在診斷系統原因可能會更有幫助 擴展開發人員 在《人月神話》中只有把項目拆分成可獨立作業的項目，增加人力才有辦法加速開發否則多餘的人力是沒有搬著的，微服務透過良好的介面隔離實作，讓團隊要加人相對容易 重點擺在團隊與服務的所有權要保持一致，可以透過模組化單體式分工合作 擁抱新技術 微服務因為是獨立部署，不同服務間可以採取完全不同的技術架構 這是微服務很明顯的優勢，替代方案可以考慮向 JVM 支援 Java, Scala, Kotlin 等同家族語言，或是像 Graal VM \u0008同一個 Runtime 支援多種語言 微服務何時是不好的主意 模糊領域：錯誤定義服務邊界會很可怕，因為會造成跨服務的連鎖改動 新創公司：可能有點爭議，但這呼應到上一點，如果公司的商業模式還沒有穩定，那自然就無法切出正確的服務邊界 客戶安裝與軟體管理：如果是要把軟體打包給庫戶，使用微服務可能會不太好，要考量到客戶的管理能力 沒有充分理由時 組織變革 作者較哨如何在組織內導入新的技術，分享 John Kother 博士的組織變革步驟，大致上溝通、小幅嘗試、看到初步效益、融入組織 再產生變革時，要注意到變革成本，Jeff Bezos 將變革分成兩類：第一類是必然、不可逆的，第二類是可逆、可變化的，第一類需要長期討論、謹慎的思考，第二類可以讓高判斷力的個人或組織迅速做出； 沒有經常下決策的人通常會誤把第二類當作第一類，讓所有決策都停滯不前 這也呼應到之前工作時公司導入敏捷開發，固然看到快速迭代、持續驗證與微調的優勢，但內部也時常在爭論如小規模實驗是否真的有效、用戶會不會因為實驗而大量流失等等，現在反思起來就是沒有把第一類與第二類決策溝通清楚的緣故\n如何開始 作者推崇透過 DDD 幫助定義服務邊界，並採用 Event Storming 讓大家對於模型有共同的理解\n接著往內檢視團隊的組成，團隊成員是否有足夠的技能？例如說微服務要透過事件溝通導入 Kafka，那團隊是否有 Kafka 熟悉的人才？如果沒有要花多少時間跟資源調整？\n如何確定轉移是否有效 可以設立定期檢查點，再一定時間檢查定性與定量措施，定量措施包含發布週期、故障率、部署次數、性能監控等，但要小心定量的指標會變成陷阱，例如團隊追求部署次數而盲目更新 (就跟 RD 貢獻度看程式碼行數壹樣的可笑)，所以需要搭配定性措施，訪談成員對於轉移過程的感受\n第三章：分割單體式架構 當通過上面兩章的討論，確定要導入微服務時，回首現實要面對的時龐大、架構混亂的單體式架構\n1. 重組單體式系統 傳統程式碼通常以技術分類而非業務領域分類，例如 Model / Controller / View 等資料夾拆分，要重新以業務領域拆分並找出對應的程式碼會是一項大工程，此時可以參考市面上重構與管理 legacy code 的方法\n2. 模組化單體式 可以考慮將功能建立獨立的模組，例如 Java 的 Jar 檔、Ruby 的 Gem 等，拆出獨立的模組未來要獨立部署成微服務也會比較容易\n3. 漸進重寫 試著先重構現有的程式碼，接著在重新實現功能，如果原本程式碼中的邏輯沒有先梳理清楚就貿然重寫，只會把過時且複雜的邏輯重新復刻，並拖累轉移的速度\n遷移模式 模式一：絞殺榕 (Strangler Fig) 由 Martin Fowler 在看到絞殺榕生長聯想的系統遷移模式 StranglerFigApplication，一開始種子在樹上生長，接著落地後持續成長，最後母樹會死亡\n套用類似的邏輯在軟體系統上，最直覺的做法是重寫一個新系統直接遷移就好，但往往事情更複雜，一次性重寫新系統需要更多的時間並冒著更大的風險；\n學習絞殺榕，讓新系統可以相容於舊系統，接著再逐步抽離新系統，最後完成轉移汰換舊系統，讓每一個步驟更小並降低風險，如果中間犯錯可以很快退返，或是終止轉移也不會有任何問題 方法適用於上層的服務，如果是系統內比較底層的服務會比較難套用 (後續介紹)，實作上可以在單體式架構前多一個代理器如 HTTP Proxy 或是 Message Queue，遷移步驟大概是\n當新系統實作尚未完成時，原本的 Request 持續流向\u0008舊系統 新系統完成後可先部署，但此時還未對外開放 透過金絲雀部署，先放手部分流量到新系統 確保沒問題，流量完整切換至新系統，汰換舊系統 在導入代理器時，要注意 Smart Endpoints and Dumb Pipes，不要讓代理器承擔過多的職責，例如說因為舊系統支援 soap 但新系統要支援 grpc，與其讓代理器判斷 soap -\u0026gt; 舊系統 / grpc 走新系統，還不如直接對外公開兩個協定，而舊系統再把 soap 格式轉換到新系統的 grpc 延伸閱讀 Microservice Principles: Smart Endpoints and Dumb Pipes\n模式二：抽象分支 當要抽離的組件是系統的底層，例如使用者通知功能，如果要直接改寫會必須把實作端與呼叫端一次改動，此時最好是\n為要替換的功能建立抽象 讓舊系統的其他功能依賴介面而非實作 提供微服務的實作 切換抽象並使用實作 刪除舊實作 在安德魯大叔的部落格中有提到相同的概念 微服務架構 #2, 按照架構，重構系統，身份驗證組件幾乎是所有功能都會相依的組件，要從單體式架構抽出來前，記得先抽象化，確保功能引用都依賴抽象化，最後才\u0008替換微服務實作，這樣最後失敗要切換回去也非常方便，這又呼應回 Martin Fowler 在絞殺榕文章說的\nwhen designing a new application you should design it in such a way as to make it easier for it to be strangled in the future.\n果然大師們說的道理都是相通的\n模式三：平行模式 如果是非常核心、重要的功能如金流，在重寫成新微服務時，可以採用平行模式雙邊寫入，每日進行比對直到沒有出錯時在切換，降低微服務上線的風險\ngithub 有開源一個 Ruby gem scientist，專門做平行化實驗的比較 裝飾者模式 如果當前系統無法被修改，卻需要攔截呼叫並額外增加行為，可以考慮用裝飾者模式，透過代理服務器在呼叫完原系統後，額外呼叫微服務；但要小心違反 dump pipe 的風險\n模式四：變更資料擷取 同樣是不修改當前系統，也不想用裝飾者模式，但同樣希望在行為變動時觸發微服務，如會員註冊後，要打印會員卡，可以透過資料儲存的方式，例如資料庫的 trigger、定期 polling 資料庫；但要小心耦合在資料庫 模式五：使用者介面組成 UI 端也可以把頁面拆解成不同的組件，例如 Micro Frontend，因應 Web 標準的發展支援 custom element，讓不同的團隊可以採用不同的技術實做不同的組件，透過 DOM event 跨組件通信\n總結 前三張談了組織與程式碼架構，評估導入微服務的優劣、大方向上導入的方式，後面兩張會繼續談最難拆分的資料庫搬移以及導入後隨著組織成長會遇到的困境\n","date":"2021-12-03T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-12-03-%E5%96%AE%E9%AB%94%E5%BC%8F%E7%B3%BB%E7%B5%B1%E5%88%B0%E5%BE%AE%E6%9C%8D%E5%8B%99%E8%AE%80%E5%BE%8C%E5%88%86%E4%BA%AB-%E4%B8%8A/","title":"《單體式系統到微服務》讀後分享 - 上"},{"content":"最近因為資料分析開始大量使用 Python，與同事協作在同一個 Package 下拆分不同 Module 實作，當我想直接執行 Module 遇上了 「ModuleNotFoundError: No module named」的錯誤，讓我開始想瞭解 Python 的 Package 系統運作的原理\nPython Package 載入方式 官方文件：6. 模組 (Module) 寫得很詳盡，模組的載入順序是根據 sys.path，而 sys.path 依序由以下組成\n當前路徑 環境變數 PYTHONPATH site-package 其中 site-package 是使用手動建置 package 所在位置 (pip install, python setup.py install)，可能會有非常多組，每一位 user / venv 下又有對應的 site-package，參考 How do I find the location of my Python site-packages directory?可以找出對應的路徑\nGlobal: $ python -m site User: $ python -m site --user-site 所以 sys.path 決定了 package 載入的順序，例如當前路徑下 package 名稱與核心模組重複，那會以當前路徑優先載入；\n所以能透過動態改變 sys.path 決定載入順序\n為什麼直接執行 Package 下的 Module 會失敗 在 Python 中，import 可以選擇完整的 Module 路徑或是相對路徑，而路徑會受到執行檔案的 sys.path 與 __package__ 的影響，相關的 magic method 為\n__name__：Module 的完整名稱 __package__：決定相對路徑 import 的解析路徑，如果檔案是 Package，則 __package__ 會等於 __name__；\n如果是 Module 則 __package__ 是所屬的 Package 名稱；\n如果 Module 是 top-level modules，也就是 __name__ == __main__，則 __package__ 為 None 以下參考自 Relative imports in Python 3，\u0008假設目前的專案目錄是\n1 2 3 4 5 main.py mypackage/ __init__.py mymodule.py myothermodule.py 在 myothermodule.py 中，載入 mymodule，可以透過完整路徑 import mypackage.mymodule 或是相對路徑 import .mymodule 的方式，但如果直接執行 $python myothermodule 會分別遇到以下錯誤\n完整路徑 1 ModuleNotFoundError: No module named \u0026#39;mypackage\u0026#39; 相對路徑 1 ImportError: attempted relative import with no known parent package 完整路徑的錯誤原因是因為 sys.path 中沒有 mypackage，sys.path 是加入script 當前的資料夾 (./mypackage)而不是 ./，所以 mypackage 是無法被載入的；\n相對路徑的錯誤則是因為當前 myothermodule.py 是 top-level module， __package__ 被設定為 None，所以相對路徑解析會失敗\n如何解決 分別針對完整路徑與相對路徑提出解決方案\n1. 完整路徑 既然完整路徑是因為 sys.path 沒有包含到 package 的上層路徑而沒有被載入，那就加上去即可\nPython 鼓勵但不強制 import 都要放在檔案開頭\n1 2 3 4 5 6 7 8 9 10 11 12 13 import sys from pathlib import Path # if you haven\u0026#39;t already done so file = Path(__file__).resolve() parent, root = file.parent, file.parents[1] sys.path.append(str(root)) # Additionally remove the current file\u0026#39;s directory from sys.path try: sys.path.remove(str(parent)) except ValueError: # Already removed pass import mypackage.mymodule # 成功 Import 直接安裝 package 另一個作法是透過 setuptools 直接安裝相依的套件，這樣就能從 site-package import，但這改動相對麻煩些，且變成套件要額外管理反而麻煩\n2. 相對路徑 使用 -m 執行 直接指定完整的 package.module 路徑 $ python -m mypackage.myothermodule，此時 __package__ 會被正確解析成 mypackage，參考 PEP366\nBy adding a new module level attribute, this PEP allows relative imports to work automatically if the module is executed using the -m switch\n手動指定 手動實作 PEP366 的提案，參考PEP366_boilerplate.py，加入對應的 sys.path 並指定 __package__，達到跟 -m 相同的效果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import sys, importlib from pathlib import Path def import_parents(level=1): global __package__ file = Path(__file__).resolve() parent, top = file.parent, file.parents[level] sys.path.append(str(top)) try: sys.path.remove(str(parent)) except ValueError: # already removed pass __package__ = \u0026#39;.\u0026#39;.join(parent.parts[len(top.parts):]) importlib.import_module(__package__) # won\u0026#39;t be needed after that if __name__ == \u0026#39;__main__\u0026#39; and __package__ is None: import_parents(level=...) 結語 第一次接觸 Python 覺得頗神奇，有許多 magic number 以及獨樹一格的 package 管理方式，包含 virtual env 的使用等\n","date":"2021-11-22T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-11-22-python-%E7%9B%B4%E6%8E%A5%E5%9F%B7%E8%A1%8C-package-%E4%B8%8B%E7%9A%84-module-%E7%9A%84%E9%8C%AF%E8%AA%A4/","title":"Python - 直接執行 package 下的 module 的錯誤"},{"content":" 天瓏購買連結\n最近在幫公司處理 Data Pipeline，將資料送往 BigQuery 儲存，並開始了 SQL 煉獄，必須說平常開發寫的 Query 複雜度都不太高，比較注重資料表的設計與效能，而報表相關則需要更大量的關聯、去重、查詢效能等，所以特別買了《Effective SQL》拜讀一番，裡頭提供很多寫 SQL 的優化，以及關聯後各種的 Edge Case，如果你對於寫的 SQL 沒有足夠自信，那很推薦入手\n以下將整理幾點我覺得特別有啟發性的\n關聯運算 在關聯運算中，包含了八種運算\n選擇：透過 where / having 過濾 投影：透過 select / group by 選擇回傳欄位 連接：透過 join 連接多張資料表 交集：透過 interset 找出兩個集合的重疊 (MySQL 不支援)，也可以用 INNER JOIN，例如「找到同時買過 bike 與 sakteboard 的客戶」 笛卡爾積：兩個集合的所有組合列舉，使用 CROSS JOIN 聯集：合併兩個欄位相同的集合，透過 UNION 除法：被除數集合中帶有全部除數集合的列，例如「某應徵者符合所以的工作條件」 差集：一個集合減去另一個集合，可以透過 EXCEPT (MySQL 不支援)，但可以用 OUTER JOIN 再去檢查 null 值模擬 作法 23: 找出不相符或不存在的紀錄 https://www.db-fiddle.com/f/4WX7yN4GWRrA1wX7zb8AXv/2\n主要可以使用 3 種方式\n使用 In 使用 Exists 使用 Left Join 並用 Where 前兩種搭配 subquery，exists 通常性能比 in 好因為只要 subquery 至少存在一列就能 return\n作法 24: 使用 Case 解決問題的時機 https://www.db-fiddle.com/f/knMFW8pMRiVa6yg22i59DB/0\n紀錄一下 Case 中 when 可以使用 subquery，這讓可能性增大很多，例如標記商品的熱銷程度，可以在 when 中加入 subquery 查詢販賣總數而不用先 outer join 再 select 一次\n作法 25: 解決多條件問題的技巧 當資料表需要關聯後再用多種條件篩選，要小心下條件的位置避免篩選錯誤，例如「要找出買過 skateboard 又同時買過 helmet / knee pad 的用戶」，需要使用多次 INNER JOIN 才能夠篩選出同時滿足多條件的查詢，要小心不能直接用 IN 會變成 \u0026ldquo;OR\u0026rdquo; 的條件\n1 2 3 4 5 6 7 -- 這只會找到有買過任一商品的用戶 select users.id from users where users.id in ( select orders.user_id from orders inner join products on products.id = orders.product_id where products.name in (\u0026#39;skateboard\u0026#39;, \u0026#39;helmet\u0026#39;, \u0026#39;knee pad\u0026#39;) ); 可以適時用 function 簡化重複的 SQL https://www.db-fiddle.com/f/vpB9hZgNGhnFPZo2FQrLhn/0\n作法 26: 需要完全符合時使用除法 https://www.db-fiddle.com/f/sA2VfuFSxW9Lr4krdcUg29/1 假設是一個求職網站，用戶需要找「滿足特定技能組合的職缺」，就需要用到除法的概念，可以用兩種方式\n1. 雙重否定： 先看最內層 - 找出用戶所有的與所需技能樹相符合的技能，第一個否定式是找出所需技能樹中用戶有哪些不足的 (not exists)，第二層否定式 用戶沒有 (所需技能樹中不再(用戶所需的技能樹))\n1 2 3 4 5 6 7 8 9 select * from users where not exists ( select * from prefered_skills where not exists ( select * from users as u2 inner join user_skills on user_skills.user_id = u2.id where u2.id = users.id and user_skills.skill_id = prefered_skills.skill_id ) ) 邏輯上有點繞，假設所需技能樹需要 js / aws，而 user 只會 rails / aws，第一層會先篩選出 aws (rails 不再所需技能樹內)；第二步會篩選出 js (因為用戶沒有該技能)，第三步計算出 (用戶有缺少所需技能樹) 所以被過濾掉；\n不過以上除了邏輯比較繞，還有一個缺點是 如果所需技能樹為空，則會回傳所有的行，因為第一層否定會是 all true\n2. 使用 group by 與 having： 這個方法比想像中簡單，透過 LEFT JOIN 找出 user 目前與 prefered_skills 有幾個重複技能，最後直接比 count 是否相同就知道\n1 2 3 4 5 6 select users.id, count(prefered_skills.skill_id) as prefered_skill_count from users inner join user_skills on user_skills.user_id = users.id left join prefered_skills on prefered_skills.skill_id = user_skills.skill_id group by users.id having (select count(*) from prefered_skills) = prefered_skill_count; 作法 33: 不用 GROUP BY 找出最大或最小值 https://www.db-fiddle.com/f/8TMwJykwSRc4Li4J9hbdyb/0 這個提議很有趣也很實用，通常看到 MIN/MAX 很直覺就是用 GROUP BY 找出，但是 GROUP BY 後只會保留聚集的欄位而其他欄位資訊都會消失 (除非用 primary 當作 GROUP BY 條件但通常不會這麼用)，這邊作者給出另一個很棒的替代方案，使用 LEFT JOIN 找出極值\n例如說今天有一個酒單 [類別, 產地國家, 酒精濃度]，我們希望「找出同一類別中最烈的酒同時顯示產地國家」 第一種是錯誤示範，會出現ER_WRONG_FIELD_WITH_GROUP 的錯誤\n1 2 select max(alcohol), category, country from beers group by category; 可以用 LEFT JOIN 方式，找到同一個種類中比當前欄位更高的酒精濃度，如果找不到 (where .. is null) 代表當前欄位就是最烈的啤酒\n1 2 3 select * from beers left join beers as beers2 on beers.category = beers2.category and beers.alcohol \u0026lt; beers2.alcohol where beers2.category is null; Subquery vs JOIN： 可以看到 Subquery 在很多時候可以與 JOIN 互相替換，不論是在篩選所關聯的資料表、計算加總等，那究竟兩者誰比較好？ 網路上普遍說 JOIN 比較好，因為 DBMS 執行時比較容易優化，且 Subquery 每次都會執行； 但在這本書中卻沒有明講，有時候 Subquery 涉及的欄位少、可以透過索引加速時 (ex. 基於索引的 COUNT) 反而有可能會更快，最後看來還是要實際跑跑看用 EXPLAIN 才知道\n窗口函式 早期 SQL 沒有相鄰列的概念，只能用 GROUP BY 做彙整，而窗口函式提供了以當前行計算的類似加總方法，例如在某條件下該行的加總(SUM)、排行 (RANK)、前一筆 (LEAD) 等，可以做到以往很難實踐的功能例如同月跨年的營收成長比例\n透過 partition by month 讓窗口依照 month 分類，同時用 year 當作排序，取 lag 也就是前一筆的 revenue 就可以得到「去年同月」的資料\n1 2 3 select year, month, revenue - lag(revenue, 1) over (partition by month order by year asc) as increase from revenues; 常用的還有 ROW_NUMBER / ROW_NUMBER 是單純的行數， RANK 則是按照順序排名，兩者的概念很接近，只有在有重複數值時 RANK 會出現相同的排名；但是當 RANK 相同排名出現後會斷層，如果希望是連續排名可以用 DENSE_RANK\n另外還有 RANGE，可以取前後一個範圍區間做動態彙整\n總結 SQL 寫起來也頗有挑戰性，要想辦法兼顧簡潔與性能需要一定的時間學習，還有一些進階的議題如階層化顯示等就先略過\n","date":"2021-11-14T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-11-14-effective-sql%E8%AE%80%E5%BE%8C%E5%88%86%E4%BA%AB/","title":"《Effective SQL》讀後分享"},{"content":" 天瓏購買連結\n不論是因為隨著產品變化而導致過往的程式設計不適合，或是單純設計人員沒有遵守基本設計原則而導致的設計錯誤，如果沒有持續的回顧並重構，會讓技術債越疊越高，而技術債最終體現在變更成本 (cost of change)，要加新功能越來越難加、或是加了莫名其他的地方開始壞掉，讓產品開發越來越艱難\n但是識別程式壞味道有時沒這麼直覺，而《設計重構》用很清楚的方式整理，從發生壞味道的潛在原因、透過 Java SDK 的錯誤設計、最後給出實際範例，並指出適合的重構方式；而設計是需要因地制宜，所以有些壞味道本身是刻意的，這本書也適當的舉出一些反例，讓整個論述更加的完整\n整本書僅 248 頁很推薦入手，總共整理 25 個壞味道，搭配 Design Pattern / Refactoring 系列的書可以相互佐證 以下紀錄一下我特別有感，以及實際遇到的程式壞味道\n抽象 抽象是為了精簡和泛化來簡化實體，具體的實現手法有\n提供清晰的概念邊界與唯一身份 映射領域實體 確保內聚性與完整性 賦予單一而重要的職責 避免重複 3.1 缺失抽象 很多時候我們會用基本型別來表達一個應該被抽象化的概念，例如書籍中的 ISBN，ISBN 又有 10 碼 / 13 碼的區別，本身數字的組合又有個別的意義，如果我們只用基本型別 String 儲存，當遇到商業邏輯需要檢查 ISBN 碼、透過 ISBN 碼解讀資訊時，就會讓處理邏輯四處重複\n解決方式是增加一個 ISBN class，讓原本的 Book 去引用 ISBN，讓檢查與解讀的 method 隱藏在 ISBN class 中\n3.2 命令式抽象化 物件導向的設計原則是識別真實世界的事務，並用抽象化表示他 (映射領域實體)，具體來說就是抽象的物件應該要表達事物的行為與其狀態，如果是習慣程序型思維而非物件型思維，容易產生資料與行為處理分開的場面，如 Data class 儲存所有的參數，DataHandler class 負責接收 Data class 的做參數並定義很多 method\n修改的方式是找到適合的物件，讓資料與行為都分類到所屬的物件，用來表達一個清晰且完整的概念，如果是像 Data / DataHandler 或許可以考慮合併一起\n過往在寫 js 時 Object Listeral 真的太方便了，往往 class 都變成單純的 method 而沒有狀態，直接吃 object 當作參數做運算，這讓程式碼的抽象化不完全，導致可理解性、可重用性低很多\n反例：設計模式中有幾種都有命令式抽象化的壞味道，例如\n策略模式：每個策略都是獨立物件，在動態執行時當作參數傳入改變行為 狀態模式：將物件的不同狀態宣告成物件，在執行時切換不同狀態，方便讓每個行為可以有對應的操作 命令模式：將每個命令都宣告成物件 以上的物件宣告都有點違反原先的物件抽象化原則，但為了提高可重用性、可擴展性，這樣是可以接受的取捨\n封裝 隱藏實作細節，實作關注點分離與資訊隱藏\n4.4 未利用封裝 當我們在一段邏輯中開始用大量的 if/else 、switch 檢查型別並給予不同操作時，就可能落入了未利用封裝的壞味道中，這會有幾個問題\n顯式型別檢查讓程式碼與具體型別耦合，降低可維護性，如果新增型別就需要改程式碼 如果呼叫方漏檢查型別，可能會有不預期的行為出現 透過多型可以很好的解決問題，多引入一個超型別，接著各個類別實作相同的介面即可\n模組化 透過集中和分解建立高內聚、低耦合的抽象\n5.3 循環依賴式模組化 如果依賴關係圖有環出現，也就是超型別依賴於子型別，這會導致修改類別時不小心影響到其他的類別\n出現的原因可能是\n職責拆分錯誤 把自己當作參數傳遞 實作 callback 時 依賴層級太多沒有注意到 例如說有一個雲端文件系統需要把文件加密，而加密需要文件的內容，變成 SourceDocument class 跟 DESEncryption class 相互依賴 解決辦法可以讓 SourceDocument 依賴於 IEncrytion，而 DESEncryption 實作 IEncrytion，這樣就沒有循環依賴以及 SourceDocument class 耦合於 DESEncryption 實作的問題 層次結構 (heirarchy) 透過分類、合併、替換和排序等手法層次化抽象，例如動物學分類\n6.7 叛逆型 heirachy 子型別可以從超型別繼承行為，或是自行複寫行為，但如果子型別未照超型別介面實作，會破壞 is-a 的繼承關係，進而違反里氏替換原則 (LPS)，這種違背超型別的承諾就是叛逆型層次結構\n可能是因為超型別涵蓋太廣，或是子型別單純想重用某些行為而必須違背其餘行為的設計\n例如有一個超型別是 CallSite，其中一個子型別 ConstantCallSite 為了提供不可變的特性，所以當呼叫 setTarget 時會拋出錯誤 解決方法可以把子型別共用的行為再多抽一層超型別出來，共用相同的行為，其餘的各自型別處理 總結 從之前學習 SOLID 原則，到從反向用程式碼臭味來看為什麼需要 SOLID 算是一個蠻有趣的對照，再搭配 Refactoring 讓自己對於抽象的物件導向「抽象」、「封裝」、「繼承」有更多的了解與反思\n","date":"2021-11-06T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-11-06-%E8%A8%AD%E8%A8%88%E9%87%8D%E6%A7%8B%E8%AE%80%E5%BE%8C%E5%88%86%E4%BA%AB/","title":"《設計重構》讀後分享"},{"content":"公司目前主要資料庫是 MySQL，為了資料分析有開啟新的 Read Replica 避免影響到正式環境，因為一些分析工具 (Metabase) 的限制，所以與同事在思考 Read Replica 如果只同步指定的資料表，同時保留寫入其他資料表的彈性該有多好 ?!，查了一下 RDS 發現這是可以的 How can I perform write operations to my Amazon RDS for MariaDB or MySQL DB instance read replica?，當時覺得十分的衝突，為什麼 \u0026ldquo;Read\u0026rdquo; Replica 可以 Write，也借此回到源頭理解 MySQL Replication 設定與機制，才發現這一切都是很合乎情理的\n以下都是以 MySQL v5.7 為主\nMySQL Replication 閱讀過 Chapter 16 Replication，簡單整理幾個重點\nSource DB / Replica DB 都要指定 server_id Source DB 必須開啟 binlog，有三種格式可以選，後續補充 Replica DB 可以透過 replication_do_table 指定資料表同步 Replica DB 可以指定多個 Source DB GTIDs 建議開啟，主要是幫助 binlog 的每一個操作都打上 id，方便確認同步進度；不開啟則是用 file 同步，需要紀錄同步的檔案名稱與位置，相對複雜些 binlog 會紀錄在 master 上的所有操作，而 replica 很單純就是拿到 binlog 把同樣的操作套用在自己身上，用這個角度思考，就可以理解為什麼 replica 同時保留寫入的功能很正常，只要不影響 master binlog 上的操作即可\nbinlog format 1. statement binlog 直接紀錄 master 上 SQL 語句，所以 binlog 本身非常容易閱讀，在做系統操作檢查時，也可以很清楚看到每一個 SQL 操作 (audit)，好處是相對資料量小、好閱讀；缺點是有些 statement 是 undeterminsitic，也就是在 replica 上重新執行會有不同的結果，例如 UUID() / USER() 等一系列操作，有趣的是 RAND() / NOW() 這些看起來就是會有問題的反而不會有問題\n2. row 不紀錄每一個 SQL，而是把 SQL 會影響到的欄位以 Row 的格式紀錄，例如 UPDATE 更新了 10 筆欄位就紀錄 10 筆，好處是重新執行保證結果一致 / 壞處是資料量很大、格式不易閱讀需要工具 decode、沒辦法看到原始的 SQL\n3. mixed 結合 statement / row 的優點，只有遇上 undeterministic 的 statement 才會用 row 紀錄，這也是 RDS 預設的格式\n實驗 完整實驗可以參考我的 Repo play-with-mysql-replication，主要驗證了\n不要在同步的資料庫新增資料，否則會終止同步 在非同步的資料庫做任合操作都沒問題 在同步的資料庫，增加 Index、增加新的欄位、UPDATE 已經同步的資料都沒問題 如果是修改同步的資料庫欄位格式，例如 varchar(255) =\u0026gt; varchar(200)，在不超過欄位尺寸下，statement 同步正常、row 不能同步 RDS 上行為類似，差別在預設就不能有 Multi Source 的功能\n結語 使用託管服務如 RDS 代為管理 MySQL 蠻有趣的，大幅降低了管理的麻煩，但如果有什麼功能面的問題，回歸工具本身去探索反而可以看得更全面；\n不過 RDS 還有而外跟 Aurora 做結合，可以製作 Aurora Read Replica，不確定有沒有別的行為差異，之後有機會再來研究\n","date":"2021-10-10T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-10-10-mysql-replication-%E8%88%87-rds/","title":"MySQL Replication 與 RDS"},{"content":"一開始學寫程式，很習慣依照執行順序，把低層次的物件寫死在高層次的物件中，更甚者直接讀取資料庫沒有任何的抽象化，例如\n1 2 3 4 5 6 7 8 9 10 11 12 class PaymentService { constructor() { this.orderCollection = mongoClient.collection(\u0026#39;order\u0026#39;); this.s3Service = new S3Service(); } async pay(orderId) { const order = this.orderCollection.find({ id: orderId }); ..... this.s3Service.uploadResult(....); } } 可能會覺得資料庫、雲端服務本身不太會有變動，所以寫死沒差，但除了服務被綁死外，另一個難題是寫測試，沒辦法將第三方服務 mock 寫出乾淨的 unit test，或是必須 mock 整個第三方 module 在寫測試前就先花了一小時在 mocking 非常浪費時間\n當我們把外部相依抽出來，透過建構式注入，就解決了以上的問題，但帶來的新問題是呼叫方需要花很多時間先建構出需要的服務，例如\n1 2 3 4 5 function main() { const orderStorageService = new OrderStorageService(); const s3Service = new S3Service(); const paymentService = new PaymentService(orderStorageService, s3Service); } 在每一個使用 paymentService 的地方，都需要手動建立相依的服務，即使有用 Factory Pattern 再多一層抽象化，管理起來也是十分的麻煩\n此時可以用 InvertifyJS 解決依賴注入的麻煩\n原始碼 sj82516/inversify-js-example\n靜態相依 InversifyJS 概念大概是\n初始化 Container，將可以被注入的 Class 都打上標記，可以把 Container 當作是 Namespace 在要注入的地方，透過標記決定初始化並注入相依的 Class 如果要動態取得物件，可以從 Container 拿取 讓我們先看一個簡單的範例，假設我們有一個 Payment Service，會依賴於第三方付款平台 PaymentGatewayService 以及基本的 LogService 蒐集 log\n靜態相依是只說 Payment Service 在初始化就決定相依的物件，而不會動態的決定，第一步將 LogService 標記 @injectable，需注意要先定義 interface，接著把實作設定為 injectable，讓服務相依於抽象介面而不是實作，符合 ISP - 介面隔離原則，例如說 LogService 實作上可以是儲存於本地端檔案、上傳到 Slack 等等\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 export interface LogService { log(message: string): void; error(message: string): void; } export const LogServiceTypes = { slack: Symbol(\u0026#34;slack\u0026#34;), local: Symbol(\u0026#34;local\u0026#34;) } export type LogServiceTypeValueTypes = typeof LogServiceTypes[keyof typeof LogServiceTypes]; @injectable() export class LocalLogService implements LogService { static NORMAL_FILE = \u0026#39;./normal.log\u0026#39; static ERROR_FILE = \u0026#39;./error.log\u0026#39; async log(message: string) { await fs.appendFile(LocalLogService.NORMAL_FILE, message); } async error(message: string) { await fs.appendFile(LocalLogService.ERROR_FILE, message); } } @injectable() export class SlackLogService implements LogService { async log(message: string) { console.log(\u0026#34;send log to slack\u0026#34;, message) } async error(message: string) { console.log(\u0026#34;send error log to slack\u0026#34;, message) } } LogServiceTypes 是為了讓使用者在指定 logService 時有型別的提示 LogServiceTypeValueTypes 主要是為了指向 LogServiceTypes 的 values type 在 PaymentGatewayService 做類似的事情，接著定義我們 PaymentService\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @injectable() export class StaticPaymentService { constructor( @inject(\u0026#34;slackLog\u0026#34;) private logService: LogService, @inject(\u0026#34;stripePaymentGateway\u0026#34;) private paymentGatewayService: PaymentGatewayService ) { } pay(client: Client, order: Order): string|void { const totalFee = this.paymentGatewayService.totalFee(order) if (totalFee \u0026gt; client.balance) { return this.logService.error(`${client.name} doesn\u0026#39;t have enough money`); } this.logService.log(`${client.name} paid ${totalFee}`) return this.paymentGatewayService.generateLink(client, totalFee); } } 重點在 constructor 中主動宣告了相依的物件，這邊我們指定要注入 slackLog、stripePaymentGateway\n最後是定義這些資源的地方 invertify.config.ts\n1 2 3 4 5 6 const paymentServiceContainer = new Container(); paymentServiceContainer.bind\u0026lt;LogService\u0026gt;(\u0026#34;localLog\u0026#34;).to(LocalLogService); paymentServiceContainer.bind\u0026lt;LogService\u0026gt;(\u0026#34;slackLog\u0026#34;).to(SlackLogService); paymentServiceContainer.bind\u0026lt;StaticPaymentService\u0026gt;(\u0026#34;staticPaymentService\u0026#34;).to(StaticPaymentService); paymentServiceContainer.bind\u0026lt;StripePaymentGatewayService\u0026gt;(\u0026#34;stripePaymentGateway\u0026#34;).to(StripePaymentGatewayService); paymentServiceContainer.bind\u0026lt;PaypalPaymentGatewayService\u0026gt;(\u0026#34;paypalPaymentGateway\u0026#34;).to(PaypalPaymentGatewayService); 我們定義一個 paymentServiceContainer，接著將物件都註冊到 Container 之中，方便後續的調用，上一步 inject() 的名稱 slackLog 就是在這邊定義，可以自由替換，只要單個 Container 中不重複就好\n最後是 PaymentService 的初始化與調用\n1 2 const staticPaymentService = paymentServiceContainer.get\u0026lt;StaticPaymentService\u0026gt;(\u0026#34;staticPaymentService\u0026#34;); staticPaymentService.pay(client, order); 這樣就完成了 小結 透過以上的案例，可以發現 InvertifyJS 做的事情也不複雜，讓開發者顯式的註冊對應的物件與其介面，接著在需要注入的地方直接用註冊名稱呼叫，如果需要動態初始化物件與其相依的服務使用 container.get(\u0026quot;name\u0026quot;) 即可\n讓呼叫端要做的工作少了非常多\n動態相依 如果我們希望動態一些，在初始化時才決定要選擇，可以採用工廠模式\n1 2 3 4 5 6 7 8 9 10 11 export class DynamicPaymentService { constructor( private logService: LogService, private paymentGatewayService: PaymentGatewayService ) { } pay(client: Client, order: Order): string|void { .... } } 我們不用在宣告的地方加上 Injectable，而是透過 Container 建立 Factor\n1 2 3 4 5 6 7 8 9 10 11 12 paymentServiceContainer.bind\u0026lt;PaymentGatewayService\u0026gt;(\u0026#34;PaymentGatewayService\u0026#34;).to(StripePaymentGatewayService).whenTargetNamed(\u0026#34;stripe\u0026#34;) paymentServiceContainer.bind\u0026lt;PaymentGatewayService\u0026gt;(\u0026#34;PaymentGatewayService\u0026#34;).to(PaypalPaymentGatewayService).whenTargetNamed(\u0026#34;paypal\u0026#34;) type PaymentServiceFatory = (logType: LogServiceTypeValueTypes, paymentPlatform: string) =\u0026gt; DynamicPaymentService; paymentServiceContainer.bind\u0026lt;PaymentServiceFatory\u0026gt;(\u0026#34;DynamicPaymentService\u0026#34;).toFactory\u0026lt;DynamicPaymentService\u0026gt;((context: interfaces.Context) =\u0026gt; { return (logType: string, paymentPlatform: string) =\u0026gt; { let paymentGatewayService = context.container.getNamed\u0026lt;PaymentGatewayService\u0026gt;(\u0026#34;PaymentGatewayService\u0026#34;, paymentPlatform); let logService = context.container.get\u0026lt;LogService\u0026gt;(logType); return new DynamicPaymentService(logService, paymentGatewayService); }; }); 看起來有點嚇人，分成幾段\n\u0026lt;(logType: symbol, paymentPlatform: string) =\u0026gt; DynamicPaymentService\u0026gt; bind 裏面放的是回傳的型別，我們的 Factor 是有兩個參數 logType / paymentPlatform 並且回傳 DynamicPaymentService 的函式 (\u0026quot;DynamicPaymentService\u0026quot;) 在 Container 他對應的名稱是 \u0026ldquo;DynamicPaymentService\u0026rdquo;，可以用 String，更謹慎可以用 Symbol .toFactory\u0026lt;DynamicPaymentService\u0026gt; 定義 Factory 回傳 DynamicPaymentService 型別的物件 (context: interfaces.Context) =\u0026gt; {} 透過當前上下文取得相關資訊，並返回實際 Factory 函式的實作 為了更方便取得對應的服務，可以加上 whenTargetNamed 直接指定名稱 最後使用\n1 2 3 const factory = paymentServiceContainer.get\u0026lt;PaymentServiceFatory\u0026gt;(\u0026#34;DynamicPaymentService\u0026#34;); const paymentService = factory(LogServiceTypes.slack, \u0026#34;paypal\u0026#34;); paymentService.pay(client, order); 結語 Inversify 還有其他有趣的功能，包含使用 tag / 定義 middleware 等等，整體看起來是個擴充性非常良好的設計，有機會再來研究內部的實作\n","date":"2021-09-05T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-09-05-%E4%BD%BF%E7%94%A8-inversifyjs-%E9%81%94%E5%88%B0-iversion-of-control-%E6%8E%A7%E5%88%B6%E5%8F%8D%E8%BD%89/","title":"使用 InversifyJS 達到 Iversion of Control 控制反轉"},{"content":"開發上為了方便，常常使用別人開發好的套件，但是最近遇到幾次衝突，發現套件的版本管理沒有想像中簡單，以下將釐清 npm / gem+bundler 在套件的\n安裝 載入方式 子套件的版本衝突 做更深入的了解與比對 實驗方式會準備 test_module_1 / test_module_2，分別使用 depend_module version 1 / version 2，最後同時使用 test_module_1 \u0026amp; 2\n1 2 3 4 - test_module_1 |- depend_module@1.0.0 - test_module_2 |- depend_module@2.0.0 TLDR；NPM 可以在不同模組引用不同的版本；而 Gem 不行；Golang 可以透過 replace 指定多個版本\nNPM NPM install 節錄 NPM 7.x npm-install 官方文件部分內容\n$npm install 主要是協助安裝 package 所相依的 packages，所謂的 package 是\n資料夾中有 package.json gzip 壓縮的 tar 檔 (1)，也就是把有 package.json 的資料夾壓縮 指向 (2) 的 url，例如 $npm install https://github.com/indexzero/forever/tarball/v0.5.6 指向 (1) 的 git remote url 被發佈到 registry 的 (3)，例如 npm install git+ssh://git@github.com:npm/cli#semver:^5.0 等 在 package 路徑下執行 $npm install，則會安裝 packages 在 node_modules 中；-g 會安裝到 global 環境下；--production 則不會安裝 devDependencies 下的 package\n如果要安裝同一個 package 不同版本並同時使用，在 npm 6.9.0 以後可以用 alias\n1 2 npm install jquery2@npm:jquery@2 npm install jquery3@npm:jquery@3 載入 Nodejs require 節錄自 Nodejs v16.4.2 Modules: CommonJS modules，這邊先探討 Commonjs Module 而不是 ESM\n在 Nodejs 中，每一個 file 就是一個獨立的 module，可以透過 require 來引入，當 require(X) 在 Path Y 下發生時，會嘗試以下步驟\n如果 X 是 core module，則返回 core module 並結束 如果 X 是 / 開頭，則將 Y 設定為 filesystem root (沒用過) 如果 X 是 ./、 ../ 、 / 開頭，則嘗試載入對應路徑的檔案或資料夾 如果 X 是 # 開頭，則往上層找到最近有 package.json 的地方 (稱為 scope)，並走 ESM 載入方式 找到 scope，比對 package.json 中定義，看是不是要載入自己 不斷地往上一層路徑找到 node_modules，並查詢有沒有對應的 package 實際載入的過程挺複雜的，只要載入一次後 package 就會被 cache 起來，可以從 require.cache 中看到被 cache 的狀況，所以如果一個套件被多個套件引用不同的版本，有可能因為 cache 而導致某些套件使用錯誤的版本嗎？ 看起來是不會的，因為文件提到 module cache 是基於他們被解析的 filename，因此不同版本的 package 會安裝在不同的 node_modules 下，所以解析的 filename 自然也不同\nModules are cached based on their resolved filename. Since modules may resolve to a different filename based on the location of the calling module (loading from node_modules folders), it is not a guarantee that require(\u0026lsquo;foo\u0026rsquo;) will always return the exact same object, if it would resolve to different files.\n版本實驗 本地端使用 Nodejs@14.15.4 / NPM@6.14.0，我發佈了\ndepend_module 1.0.0 / 2.0.0 yuan_test_module_1 require depend_module@1 yuan_test_module_2 require depend_module@2\n最後 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 require(\u0026#39;yuan_test_module_1\u0026#39;) require(\u0026#39;yuan_test_module_2\u0026#39;) console.log(\u0026#34;require tow modules\u0026#34;) console.log(require.cache) ----- 輸出 depend_module version 1 test_module_1 depend_module version 2 test_module_2 require tow modules [ \u0026#39;/Users/zhengyuanjie/Desktop/package/test/index.js\u0026#39;, \u0026#39;/Users/zhengyuanjie/Desktop/package/test/node_modules/yuan_test_module_1/index.js\u0026#39;, \u0026#39;/Users/zhengyuanjie/Desktop/package/test/node_modules/depend_module/index.js\u0026#39;, \u0026#39;/Users/zhengyuanjie/Desktop/package/test/node_modules/yuan_test_module_2/index.js\u0026#39;, \u0026#39;/Users/zhengyuanjie/Desktop/package/test/node_modules/yuan_test_module_2/node_modules/depend_module/index.js\u0026#39; ] 出乎我意料之外，node_modules 的結構是\n1 2 3 4 5 6 |- node_modules |- depend_module (version 1) |- yuan_test_module_1 |- yuan_test_module_2 |- node_modules |- depend_module (version 2) 我沒有預期第一層結構中會有 depend_module，以為都會是在個別的 test_module 下，再回來看 npm 文件說明\npackage{dep} structure: A{B,C}, B{C}, C{D}，沒有版本衝突，則預設都安裝在最上層 1 2 3 4 A +-- B +-- C +-- D A{B,C}, B{C,D@1}, C{D@2}，D@1 安裝在最上層，D@2 則安裝在 C 下面 1 2 3 4 5 A +-- B +-- C `-- D@2 +-- D@1 這樣可以做到預設共享相同版本的 module 避免重複下載，卻也不用擔心多個版本衝突的問題\n環狀相依 如果 package A require package B 而 package B 又 require package A，變成環狀相依的情況，在 Nodejs 中這樣不會拋出錯誤，只是行為可能不如預期\n官網的案例中\nmain.js require a.js / b.js，因為 a.js 先被 require 則先被載入 在 a.js 中，執行到 require b.js，則開始載入 b.js 在 b.js 中，執行到 require a.js，則為了避免無限迴圈，此時會回傳步驟(2)載入到一半的 a.js，接著繼續完成 b.js 載入 回到 a.js，此時的 b.js 引用是完整的，繼續載入 a.js 回到 main.js 中，此時在 main.js 中 a.js / b.js 都是完整的引用 1 2 3 4 5 6 7 8 9 $ node main.js main starting a starting b starting in b, a.done = false b done in a, b.done = true a done in main, a.done = true, b.done = true Gem / Bundler 接著來看 Ruby 生態如何用 Gem Bundler 管理套件，以下內容主要參考 Understanding How Rbenv, RubyGems And Bundler Work Together，首先先安裝 rbenv，用於管理主機上多個 Ruby 版本\nRubyGem 在 Ruby 1.9 後就整合了，gem 安裝路徑可以透過 $gem env 中的 \u0026ldquo;INSTALLATION DIRECTORY\u0026rdquo; 路徑，不同版本有被分開不同的路徑安裝，但不像 npm 會在每個專案下都有獨立的 node_modules\n載入 有三種方式\nload:\n類似於 require，但會重複載入 require:\n到 $LOAD_PATH 下檢查是否有載入對應的 gem，沒有則去系統安裝路徑載入 gem 下的 lib，並加到 $LOAD_PATH 中，詳見龍哥的 Ruby 語法放大鏡之「你知道 require 幫你做了什麼事嗎?」 require_relative:\n接受相對路徑載入 所以 RubyGem 管理相當單純，把 Gem 都安裝在統一的安裝路徑下\nBundler Bundler 負責幾項工作\n讀取 Gemfile 並安裝對應適合的版本 產生 Gemfile.lock 確保在不同環境下還原時版本一致 因為不能載入多版本，所以 Bundler 會在所有版號中找到適合的，例如 module_a 需要 module_c \u0026gt; 1.0.0，而 module_b 需要 module_c \u0026lt;=1.1.0，則最後會安裝 module_c@1.1.0 符合兩者需求 實驗 實驗方式相同，但是發現 Ruby 不能載入多個版本，會出現\n1 `raise_if_conflicts\u0026#39;: Unable to activate yuan_test_gem_2-2.0.0, because depend_gem-1.0.0 conflicts with depend_gem (= 2.0.0) (Gem::ConflictError) 使用 Bundler 會因為找不到適合版本而拋錯\n1 2 There was an error parsing `Gemfile`: You cannot specify the same gem twice with different version requirements. You specified: depend_gem (~\u0026gt; 1.0) and depend_gem (~\u0026gt; 2.0). Bundler cannot continue. Golang mod 參考Using Different Versions of a Package in an Application via Go Modules，Golang 可以在 mod file 中指定 replace，就可以在程式中使用多個版本，也可以指向自己的 fork，相當的方便，官方文件：Requiring external module code from your own repository fork\n結語 比對不同的實作蠻有趣，目前看來 npm 會 install 在當下路徑，換來好處是可以載入多個版本的相同模組；而 gem 沒有這樣的彈性 至於載入多版本的相同模組我覺得是蠻現代的需求，不確定為什麼 Ruby / Gem 沒有提供這樣的特性，背後的脈絡不知是如何考量\n","date":"2021-07-10T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-07-10-nodejs-/-ruby-/-golang-%E5%A5%97%E4%BB%B6%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86%E5%B7%AE%E7%95%B0%E6%AF%94%E5%B0%8D-npm-%E8%88%87-bundler/","title":"Nodejs / Ruby / Golang 套件版本管理差異：比對 NPM 與 Bundler"},{"content":"公司使用 MySQL，近日遇到一些讀取的瓶頸，有了使用 AWS Aurora 的討論，剛好上一週看到 Exploring performance differences between Amazon Aurora and vanilla MySQL 才赫然發現 AWS Aurora 底層儲存架構會影響使用場景，跟預期的自駕 MySQL 搭配 Replica 有不同的效果\n以下這篇將探討\nMySQL 儲存的基本原理 Aurora 的儲存架構 資料源自於\nExploring performance differences between Amazon Aurora and vanilla MySQL Amazon Aurora: Design Considerations for High Throughput Cloud-native Relational Databases MySQL 儲存的基本原理 關聯式資料庫最基本要保障 ACID，其中 Durability 是最基本的核心功能，保證寫入成功的資料不會因為系統 crash 等問題而遺失，在 MySQL 寫入的流程大概是\n將更新寫入 Redo Log buffer (WAL)，等待 commit 確定就刷新到硬碟保存紀錄避免遺失資料 如果資料有被讀取到 memory 中，則更新 Page 內容並標記為 dirty 背景運行的程序在 checkpoint 時機觸發時將 dirty page 與 redo log 的內容更新於硬碟上的 data file 其中有幾個目的\n減少 disk I/O 並保障連續性寫入：\n如果每一筆資料進來就馬上更新硬碟上的儲存，這會拖垮 DB 效能，所以 MySQL 讀寫時是以 page 為單位，如果發現更新的 page 在記憶體中會標記為 dirty page，後續再 checkpoint 統一更新到硬碟中 \u0008Redo Log 避免系統 Crash 而資料遺失：\n為了避免資料遺失，所以會先寫入 Redo Log，所以又稱作 Write Ahead Log 先寫入 log 再操作，Redo Log 是一個順序性持續寫入的 Log，所以寫入效能比隨機性更新還要好，當系統 crash 後，會來檢查 Redo Log 中是否存在沒有更新到資料庫硬碟的資料；\n具體寫入硬碟時機看 innodb_flush_log_at_trx_commit，預設 1 為每一筆都及時寫入硬碟中 定期同步到硬碟中：\n每個操作都有先寫入 Redo Log，但這是為了災難復原而用，資料庫的資料會以 Page 形式儲存於硬碟中，所以定期到 checkpoint 會把 dirty page 從記憶體更新到硬碟中 Binlog Binlog 適用於系統異地恢復/建立 Replica，Binlog 不像 Redo Log 會保存所有的操作，只會保存對於資料有異動的行為，例如 update / delete 等\n寫入時機在 commit 完成時，會在 commit 結束前 / lock 釋放前寫入硬碟，避免資料異常，具體的寫入頻率要看 sync_binlog，預設為 1 代表每一筆 commit 就寫一次，可以設定為 n 代表 n 筆 commit 才寫入但就會有遺失資料的風險\nUndo log 如果 isolation level 開到 repeatable read，則 MySQL 採用 MVCC，讓每個 transaction 只會讀取到自己 transaction 開始前的最新資料，而不被並行的 transaction 所影響\n之所以需要 undo log 是當 transaction rollback 時，需要知道自己要回滾的狀態，所以 undo log 必須保存到 transaction 執行完畢才可以刪除，長度會是 執行最久的 transaction\nAurora 架構 Aurora 是 AWS 基於雲端建構的關聯式資料庫服務，兼容於 MySQL / PostgreSQL，主要想解決幾個問題\n容錯能力：\n硬碟可能會壞 / 主機可能會有問題 / 甚至 Data Center 都會出意外，尤其是在雲端分散式系統中，機器數增加帶來更高的出錯機會，Aurora 每一個 replica 都會對應 3 個 AZ 各 2 個 node 總共 6 個 node 儲存資料，大幅增加容錯能力 擴展性：\n傳統的資料庫架設於單一主機上，讀取會受限於 Disk I/O 的貧頸，Aurora 將 Compute / Storage 分離，可以針對需求獨立升級，並增加跨區域、跨 AZ 的 Read Replica 但有這麼多好處卻不用受到太多的性能影響，聽起來有點美好的不切實際，例如備份到多個儲存 node 就需要擔心資料一致性/效能的問題，除了原本的 Disk I/O 又多了一段 Network I/O，Aurora 在不同的地方作出了對應的調整\n2. DURABILITY AT SCALE 2.1 Replication and Correlated Failures Aurora 跟很多分散式儲存的服務很像，採用多數讀寫的機制，只要確保\nVw + Vr \u0026gt; V Vw \u0026gt; V / 2 V 代表節點數量，如果 Vw 寫入節點數量超過 1/2 節點都成功才成功，且 Vr 讀取數量是讀取多數則\n最基本的數量會取 3，則 Vw / Vr 只要有 2 個 node 存活則可以繼續運作，容錯率是 33%，但文件中說到 Aurora 選擇 6 個 node分散在 3 個 AZ，這樣可以預防一個 AZ 以及一個額外錯誤下讀取還不會中斷\n2.2 Segmented Storage 為了降低同時機器故障而導致破壞多數的可能，要盡可能降低 MTTF(平均錯誤時間)與 MTTR(平均復原時間)，Aurora 以 10GB 當作一個區段，產生 6 個備份並分散到 3 個 AZ 稱之為 PG(Protection Group)，一個 Storage Volume 就是由多個 PG 所組成，實體上就是由 EC2 加上一群分割的 SSD 組成，最大可支援到 64TB\nSegment 是最小獨立單位 (也就是會壞掉是獨立一個 Segment 壞)，AWS 會負責持續監控與復原，目前復原一個 Segment 約 10秒鐘 (網路帶寬為 10Gbps 下)，所以出現破壞多數的場景：兩個 node 在 10秒鐘內同時壞掉且一個 AZ 掛掉又不包含同時壞掉的 node 機率就微乎其微\n小結：這邊談的是 AWS 在持久性上的優化，將最小的儲存單位定在 10GB，讓復原速度變快 / 產生 6 組備份增加容錯\nTHE LOG IS THE DATABASE 3.1 The Burden of Amplified Writes 先看第一版 Aurora 嘗試的架構 - 傳統的鏡像同步架構，一台 Primary Instance 負責寫入儲存於 EBS 並同時備份到另一份 EBS 中，有一台 Replica Instance 同步 Primary 的寫入並儲存於兩份 EBS 中，可以看到一個 MySQL 寫入最多會觸發五個 I/O binlog / redo log / frm(metadata) / double-write，要等到全部寫入結束才算是操作成功，這會拉長回應時間，更糟糕的是圖片中步驟 1,3,5 (為什麼有5?) 是同步且順序寫入\n3.2 Offloading Redo Processing to Storage 相反的 Aurora 透過 Redo Log 同步，讓 Storage level 負責 Redo Log 寫入與更新 Data file，同時分送給 Replica 更新記憶體中 page 的資料；\n不像過往 MySQL 需要在 Checkpoint / background / cache 空間不足時刷新資料到硬碟中，全部由 Storage Service 負責，大幅降低了 Network I/O\n從 Benchmark 可以看到，每筆 Transaction 所需的 I/O 從 7.4 變成 0.9，完成的 Transaction 數也提升了 35 倍\n這同時也降低了復原的時間，傳統 DB 需要從 Redo Log 上一次的 checkpoint 開始逐條執行，但 Aurora 的復原是從 Storage level 向其他備份拉 Segment，復原速度可以在一分鐘以內\n3.3 Storage Service Design Points Storage Service 核心設計要降低寫入請求的延遲，所以把大部分的儲存工作都移至背景執行，尤其是更好地利用 CPU 去換取 Disk 寫入時間，例如舊的 Page 要垃圾回收可以在背景用 CPU 執行而不要延遲前景在處理寫入請求，所以 Aurora 的背景運作不會影響前景，不同於傳統 DB 如果背景在 Checkpoint 刷新硬碟則會造成前景寫入的延遲\nStorage Service 收到請求會執行\n放入 Memory 中 寫入硬碟，寫入請求成功 排序，並確認寫入紀錄是否有遺漏 透過 gossip 跟其他節點要遺漏的紀錄 更新到 page 中 定期同步到 s3 定期清除舊的 page 定期檢查 page 的驗證碼 除了步驟 1, 2 會影響前景寫入，其餘步驟都可以非同步且於背景執行\n4. THE LOG MARCHES FORWARD 接著要確保 rumtime、replica 都保持一致性，究竟是如何不使用昂貴的 2pc 卻又能保持一致性\n4.1 Solution sketch: Asynchronous Processing 前面提過 Aurora 是透過 redo log 同步，每一筆 log 都有 持續遞增的 LCN (Log Sequence Number)，因為寫入成功只要多數的 node 同意即可，所以有些 node 可能會缺少幾個 log，可以透過 LCN 去跟其他 node 索取遺失的 log\n考量到多 transaction 的情況，每個 transaction 執行順序有所不同，在過程會陸續把 commit 送到 storage service 儲存 (到 complete) 階段，但如果發生了系統故障時，重新恢復後需要把沒有 commit 的 transaction 都 rollback，假設 DB 目前最高完成寫入的 LCN 稱為 VCL (Volume Complete LSN))，在復原中任何 LCN 高於 VCL 都會被遺棄，因為代表沒有被 complete\n更進階這些高於 VCL 的 LCN 會被標記成 CPL (Consistency Point LSNs)，接著定義出 VDL 為那些在低於 VCL 中最高的 CPL，例如 CPL 有 900,1000, 1100 但是 VCL 為 1007，則 VDL 為 1000，這代表 storage service complete 到 1007 但是持久化儲存到 1000\n這一整段沒有到非常理解，待之後慢慢思考\n4.2 Normal Operation Aurora 會同時處理大量的寫入請求，每一筆 redo log 都會產生一個大於 VDL(被持久化保存的 LCN)的 LCN，但為了不要讓 LCN 的遞增遠大於 Storage Service 所能保存的速度，LSN Allocation Limit (LAL) 預設為 10萬筆，意即 Aurora 最多並行 10 萬筆進行中的 transaction 避免寫入速度跟不上\n每一個 PG 中的每一個 Segment 只會保存部分的 redo log，但會有一個 link 指向上一份 log 所在的 PG，這用來追蹤目前所完成最大的 LCN，並且在與其他 node gossip 時可以知道缺漏的 log\nRead 如同大多數的 DB，Aurora 會在 buffer 中 cache page ，如果 cache miss 則從 disk 讀取，此時傳統 DB 會優先移除 dirty page 並寫回硬碟中；但 Aurora 並不需要刷新 dirty page (因為 storage service 已經獨立更新)，相反的是把 page LCN 大於等於 VDL 的 page 移除並讀取最新被持久化的page\n前面提到 Aurora 透過多數讀取確保一致性，但大多數時機並不需要，Aurora 會在 Page 讀取時紀錄 Page LCN 當作 read point，接著只要讀取的 storage node VDL 確定在 read point 之後，就代表該 node 有該 page 完整最新的資料，而且因為每個 segment 都有紀錄 LCN 與 link，所以能很快知道資料要到哪一個 segment 讀取\n同時每個 PG 會維護一份目前最低的 Protection Group Min Read Point LSN (PGMRPL)，這代表低於此數字的 LCN 的 page 都不會再被讀取，這樣垃圾回收就能移除過舊的 log\nReplica 一個 write node 可以搭配 16 個 read replica 並共用相同的 storage service，writer 會把 redo log 也同步給 reader，如果 log 有在 cache 中則更新，否則就直接丟棄\n因為 reader 跟 storage service 的 redo log 更新是錯開，所以 reader 在更新 log 時要確保 LCN 是小於等於 VDL / 如果 log 是 mini-transaction 的一部分則更新，確保跟所有的 database 看到相同內容\n預期 reader 的延遲會在 20ms 以內\n4.3 Recovery 傳統 DB 在災難復原時，會去讀取 redo log 中還沒被 checkpoint 執行的 log，搭配 undo log 將失敗的 transaction rollback，但這執行過程蠻花時間，而 Aurora 沒有這方面困擾\n首先會檢查每一個 PG，找出讀取多數中可以確保已經完成的寫入多數紀錄 VDL，高於 VDL 的 LCN 全部捨棄，接著一樣需要透過 undo log 去 rollback，整個過程約 10秒以內\n5. PUTTING IT ALL TOGETHER 接著看完整的架構圖 社群版的 MySQL InnoDB 引擎在寫入操作時會修改 buffer 中的 page 與寫入 WAL redo log buffer，等到 commit 時再把 redo log buffer 寫入硬碟中；而被修改的 page 要則透過 double-write buffer 避免只更新部分 page，page 寫入會發生在背景 / checkpoint / cache 移除時； 此外還有一些 B+Tree 操作與相關的 mini trasaction (MTR) 如拆分、合併 B+Tree page 需要是原子性操作\n在 Aurora 版本中，redo log record 就代表每一個 MTR 操作，最新的一筆 log 被標記成 consistency point；Aurora 提供相同的 isolation level\n其餘就是架構的說明，以及各方面的性能表現，就不贅述了\nAurora 對比 MySQL 有什麼隱憂 因為 Aurora 如果 primary 跟 read replica 在同一個 region 下，則會共用 storage service，這也就代表 undo log 會是同一份，如果今天 read replica 有一筆執行非常久的 transaction，則 undo log 也會跟變大導致 primary 效能下降\n這個在傳統 MySQL 不會發生，因為如 3.1 提到 MySQL 是透過 binlog 同步各自的 MySQL 維護各自的 redo log / undo log 所以 read replica 不會有任何影響 primary 的時候\n最後作者指出有幾個解法\n調整 read replica 預設 isolation level 為 read commited，就不會用到 undo log(需注意預設為 repeatable read) Aurora 選擇 binlog relica，就跟傳統的 mysql 一樣 拿在 S3 的備份資料，用其他大數據工具分析 Aurora 支援 clone，備份一組新的設定 結語 翻整個 Aurora 架構有很多生硬的地方沒有完全搞懂，但是看到這種解 bug 追根究底到底層架構還是覺得很過癮，之後會持續的優化文章內容，如果有哪裡有建議跟指教再麻煩留言～\n","date":"2021-07-02T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-07-02-aws-aurora-%E6%9E%B6%E6%A7%8B%E7%A0%94%E7%A9%B6%E4%BB%A5%E5%8F%8A%E8%88%87%E8%87%AA%E9%A7%95-mysql-%E7%9A%84%E5%B7%AE%E7%95%B0/","title":"AWS Aurora 架構研究以及與自駕 MySQL 的差異"},{"content":"最近功能上線遇到一些 OOM 問題，在 Staging 手動驗證流量不夠測不出來有點頭疼，所以回頭用 JMeter 進行壓力測試，因為有 Cache 所以只打單一種 request 是沒有用的，必須組合出多種參數一起執行，好在 JMeter 支援 csv 輸入參數，以下將介紹如何使用\nSample 可以參考我的 github repo jmeter-pre-post-processor\n安裝 到官網下載最新版，解壓縮完 $ bin/jmeter 即可執行，記得電腦要安裝 java\nSample1: 操作 進行基本的壓力測試，大概會有以下的步驟\n決定進行幾輪測試 要發送多少 request、同時間併發數 request 針對的 host / 參數設定 結果的顯示，可以將 response 存成檔案 / 圖表顯示 response 速度、成功率等 對應 JMeter 設定是\nThread Group Number of Threads: 多少 request Loop Count: 總共幾輪 Ramp-Up Second: 有點 ticky 的參數，指定多少時間內 Thread 會啟動，假設設定 90 sec 總共有 10 個 thread，則下一個 thread 會上一個 thread 成功後 + 9 (90/10) sec 後啟動，官方建議 預設 = threads 數量再視情況增減，如果要確保同時併發，request 執行時間要大於 thread 啟動時間 在 Thread Group 上，按下右鍵增加 Config Element 系列：可以放共用參數 Http Request Default: Http request 的預設參數，可以放 host / port 等共用設定 Http Header Manager: 放共同 Header Sampler: 採樣，也就是要測試的項目 Http Request: 指定要打的參數 Listener: 收集結果 Summary Report：統計所有 request 的速度與成功率等 Sample2: 從 csv 讀取參數 選擇 Config Element \u0026gt; CSV Data Set Config，選擇 csv 檔案後，在 Http Request 可以用 ${變數名} 的方式，就可以讀取到對應的 csv 欄位喔 例如我的 csv 長這樣\n1 2 3 4 account,password user1,test1 user2,test2 user3,test3 我希望打出 POST /login 中的 body 帶參數，變成\n1 {\u0026#34;user\u0026#34;: \u0026#34;${account}\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;${password}\u0026#34;} JMeter 會依序從上至下不斷輪迴發送喔\nSample3: Pre Processor 如果我們希望在每次 Request 前做一些預處理，例如產生亂數、字串組合等，就可以用 Pre Processor，有分成很多種，可以用 BeanShell (java-like script language) 或是 JSR223 (javascript / groovy 等更多的 script language)，這邊就用 javascript\n延伸自 Sample2，先將 csv 讀進來的 account / password 當作參數，在 account 前面加一個 \u0026ldquo;prefix_\u0026rdquo; 字串，做法上新增一個 JSR223 Preprocessor，選擇 javascript，並輸入\n1 2 3 4 var account = vars.get(\u0026#34;account\u0026#34;) account = \u0026#34;prefix_\u0026#34; + account vars.put(\u0026#34;account\u0026#34;, account) log.info(vars) 有一些環境變數可以使用，例如 vars 可以取得/設定當前的變數，其餘還有 sample 可以改變 sample 結果 / log 打出 log， console.log 是不能用的 / props 取得當前 JMeter 設定等，這些比較進階，可以參考 How to Use BeanShell: JMeter\u0026rsquo;s Favorite Built-in Component\nPost Processor 有些時候，我們會希望做一些後處理，例如 parse response 做 assertion 等，可以增加 post-processor，例如我們可以接登入取得的 token，當作下一個 request 的參數，新增一個 JSR223 PostProcessor\n1 2 3 var responseBody = sampler.sample().getResponseDataAsString(); responseBody = JSON.parse(responseBody) vars.put(\u0026#34;user_id\u0026#34;, String(responseBody.id)) 這樣下一個 request 就可以拿 ${user_id} 了\n結語 JMeter 有非常多方便且強大的組件，可以組合出各種客製化的壓測環境\n","date":"2021-06-26T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-06-26-jmeter-%E4%BD%BF%E7%94%A8%E6%95%99%E5%AD%B8-+-%E8%87%AA%E5%AE%9A%E7%BE%A9%E8%AE%8A%E6%95%B8%E4%BD%BF%E7%94%A8/","title":"壓測工具：JMeter 使用教學 + 自定義變數使用"},{"content":"當時在社群看到分享，想說聽看看也沒什麼損失，聽完才發現根本賺到，沒有想過 SOLID 可以用這種方式理解，也才真正明白自己以往在看 Uncle Bob 的書思考都太淺層，真心感謝 Fred 大大撥空跟大家分享寶貴的知識，在直播中遇到設備、網路問題還是很有條理的完成分享\n以下將整理 Fred 大大直播的分享，建議有空可以花兩小時看完直播\nSOLID Uncle Bob 在 1995 年(約 37歲)根據自己開發程式的痛點，也就是大型軟體程式可維護性，需求變更帶來程式碼維護問題，而維護問題的根本原因是程式碼的耦合，在社群提出討論，然後被戰翻了 XD\n設計壞味道 Rigidity 僵化：任何變更會導致系統中相依的組件需要變更，超出想像 Fragility 脆弱：發生變更時，其他地方容易發生問題 Immobility 難以服用：組件有太多細節的依賴 Viscosity 黏滯：變更時用太多 hack，而非以原設計的方式進行 組件相互依賴性 SOLID 本質在解決組件之間不合理的相互依賴性\nSingle Responsibility 描述與定義最模糊的一條，有幾種常見說法\n一個類別應該只有一個改變的原因 一個類別只應做一件事，就是他的職責 =\u0026gt; 那職責應該是什麼，就是這個類別該做的事 (無限 loop) 例如一個學生管理系統，Student 的類別增刪改查，要產生幾個類別？ 資料處理中的 ETL 是一個職責還是三個原則？ 違反 SRP 的設計可能會長這樣 可維護性問題方面，除了程式碼耦合，也要注意 業務耦合 一個組件有了用戶相關，又有信用卡相關，結果兩種業務就耦合了\n軟體組件發生的原因(職責)，應該來自同一個業務方，也就是同一群會對組件提出業務需求的人\n當組件規模增加，業務量增加，考慮使用 SRP 拆分組件，讓業務單純化，持續迭代，從另一個角度理解 SRP 是組件不該碰的事情別碰\n識別是否遵從 SRP 的提問「這個 xxx 是做什麼的？」/ 如果答案包含了「ooo 和 xxx」，則通常違背了 SRP\n常見錯誤\n不要用訂單 ID 格式當作分類的方法，而應該用獨立的欄位 (ex. 突然要改 id 長度就爆炸了) 透過以下問題思考 SRP 方針 不一定要拆，如果 20 個方法是緊耦合也屬於同業務 可以，但計算方式應該拆除 分開 最好不要，用戶等級就是等級，狀態就是狀態 小結 SRP 建議：任何一個組件應該只對同一個客戶/業務關聯方的需求而發生變動 不該做到的事就不要碰 識別是否違反 SRP : 這個組件/服務/類別/欄位/值是做什麼的 從被更動的組件角度解決業務的耦合 Open Close 如果每次功能修改都會造成系統一連串的組件修改，這樣就不符合開閉原則，一般會理解成不修改原始碼就可以擴展系統行為 ？！這是通靈嗎 XD 怎麼可能沒有寫 code 行為就出現\n真正的解讀是\n系統要可以妥善預測複雜度的發生點，並建立適合的 擴展點 ，而發生行為變更時，透過擴展點變更，原本的主流成與使用該擴展點本身的 client 本身不需要修改\n例如有一個外賣平台，有多種會員身份，對應不同的折數，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 calPrice(){ if(用戶是專屬會員) { if(金額 \u0026gt; 200){ 打七折 } } if(用戶是 vip 會員){ 打八折 } if(普通會員){ if(上個月還是 vip 會員){ 打八折\t} } 原價 } 如果 PM 説\n會員制度增加 調整折數 則整個計價都會受到影響 所以複雜度發生點在於 計算應付價格 ，應該在此處設計擴展點，透過 Strategy Pattern，建立 UserPayService interface，將計算邏輯放在不同的實現上\n如果發生\n增加會員等級 ⇒ 增加實作即可 原本計算的流程不需要修改 可以參考 用多型取代重複的判斷式\n小結 組件的耦合，會讓微小的變動也讓系統有大幅度修改，OCP 建議：組件的設計必須讓系統易於擴展，同時限制每次被修改的範圍，實作為將系統劃分一系列的組件，將依賴性關係依照層次結構組織 (穩定的組件不要相依於不穩定的組件)，套用到架構設計也是如此 (Clean Architecture)\nOCP 也是一個很好結合 Design Pattern 的原則\n依賴反轉原則 新提出的設計架構，會分層不同的業務 例如網頁操作會從 web ui 層 =\u0026gt; controller 業務邏輯 =\u0026gt; 儲存到 db 層再原路返回給 client，寫程式時如果按照這樣的順序，有可能發生高階組件依賴低階組件，例如客戶匯出雲端發票會儲存於 S3，那我們很容易把 s3 上傳邏輯寫死雲端發票的業務邏輯中，但如果 s3 sdk 升級怎麼辦？\n仔細想想「發票匯出」的業務需求跟「實際使用哪一間雲服務」有關係嗎？ 所以需要一個 彈性組織組件的依賴 ，將能力抽象化出業務規則，明確定義出介面方法\n小結 為了達成 OCP 效果，需要做組件的切片的方式 與 彈性調整組件依賴關係的方法 程式碼依賴關係應多使用介面，而非具體實現 里氏替換原則 70 年代軟體越來越大，開發的成本越來越高\n⇒ 加入流程控制 / blocks / subroutines 分解多個組件 (modules)，組合出結構化的系統\n⇒ 經常有多個功能類似的組件組件，讓這些組件組件之間有關聯的狀態，smalltalk 加入了 繼承\n⇒ Liskov 於 1987 年發現繼承會帶來可維護性的問題\n⇒ B 繼承 A -\u0026gt; B is A，這是非常強的耦合 所以 Liskov 才會建議 所有用 A 的地方一定要可以用 B\n在考慮代碼複用上，優先考慮組合，如果真的要多態，使用 interface 跟 abstract class 會是更好的組合\n擴展闡述 所有對某個介面 (interface / api)的實現，都可以是作為對該介面的 subtyping，無論介面背後的實現更動，行為應該保持跟當初承諾一致\napi 會依照 client 版號產生不同的行為，這會破壞當初的承諾 Interface Segregation Robert Martin 參與印表機開發，處理/印出/裝訂都會產生一個對應的 job 類別，隨著開發規模，Job 內有上百個 function 與需求，所有的業務邏輯都引用了 Job，到後面 Job 類別的 typo 修正，都會導致專案數個小時的編譯時間\n開發只改列印相關的功能，但他不知道其他人有沒有依賴這個方法，應該要具體功能類與 Job 類透過介面分離，功能類只依賴介面 API，讓實際的 Job 類工作避免互相干擾\n當一個功能豐富的 concrete class 要給多個調用方提供功能，應該透過介面或抽象類來提供給 不同調用方不同的功能夠過不同的介面隔離 ⇒ SRP 指導組件的設計，ISP 用於指導介面的設計\n不應讓客戶端知道額外的功能\n總結 SRP 是軟體架構的理念前提，一個組件不應該被多個不相關的業務而造成耦合帶來維護性上的問題 OCP 則是設計原則的指導思想，可以透過設計模式、DIP 來達成 DIP 描述組件之間如何抽象化與組織的指導方針 LSP 確保介面實現與使用方的耦合，保證介面行為的穩定 ISP 維護與使用一個組件該暴露的知識，在實作上指導介面設計 當一個介面因為業務 A 而修改，那應該也只有業務 A 被影響到 ","date":"2021-06-15T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-06-15-youtube-%E7%9B%B4%E6%92%ADfred%E8%81%8A%E8%81%8Asolid%E8%A8%AD%E8%A8%88%E5%8E%9F%E5%89%87%E6%95%B4%E7%90%86/","title":"Youtube 直播「Fred聊聊SOLID設計原則」整理"},{"content":"在與公司前輩請教時，有聊到 Mutex , RWMutex 性能對比，以及併發下用 SingleFlight 避免擊穿問題，所以就花點時間看實作與練習，發現這系列寫得太好了 6.2 同步原语与锁，拜讀過後整理以下的提問\nMutex 實作 分成兩種模式：一般模式與飢餓模式 取得 Lock 時\n沒有人佔用則直接取得 有人占用時，如果判斷是否能進入自旋模式，所謂的自旋是透過消耗 CPU cylcles 的 buzy waiting，降低 context switch 的花費 重新嘗試取得鎖，如果沒有取得鎖，等到時間超過 1ms 則進入飢餓模式，並等待信號 (runtime_SemacquireMutex) RWMutex 會產生 Write Starvation 嗎？ RWMutex 採用寫入優先，如果在取得 RWMutex 寫鎖時，發現目前有多個讀鎖\n先將 readerCount 變成負數，讓後續讀鎖都無法取得 等到信號 writerSem 喚醒 讀鎖在取得時\n檢查 readerCount 是否為負數，如果是代表有寫鎖 如果有寫鎖，則偵聽 readerSem 信號喚醒執行 在解除寫鎖 / 讀鎖，會分別去出發信號，讓等待的讀鎖 / 寫鎖開始執行\nRWMutex 跟 Mutex 效能比較 實際測試的結果，設定一個寫入比率，單純比較取鎖/釋放鎖，跑一百萬次兩者差異不大\n1 2 3 4 BenchmarkMutexTest-8 10000000 0.04064 ns/op BenchmarkRWMutexTest BenchmarkRWMutexTest-8 10000000 0.04699 ns/op BenchmarkFakeWrite 加入了讀取跟寫入，有些測試會用 time.Sleep ，但我實驗因為同時跑 goroutine 關係，如果全部 goroutine 都在 sleep 會出錯，所以改用簡單的計數從零數到一百萬當作讀取，而寫入則是五倍的讀取時間\n寫入比例抓 0.2 的話，RWMutex 用起來有優勢很多\n1 2 3 4 BenchmarkMutexTest BenchmarkMutexTest-8 10000 58176 ns/op BenchmarkRWMutexTest BenchmarkRWMutexTest-8 10000 37176 ns/op 測試的程式碼在此 go-rwmutex-benchmark\n總結\nRWMutex 測試起來效能蠻好的，在考量到讀取比較多的情況表現會比 Mutex 還要好\nSingleflight 當 Server 在處理併發時，會避免大量重複的查詢操作進入 DB 中，這尤其會發生在 Cache 失效的當下，最理想狀況是所有同樣的查詢只要進 DB 查一次，其他查詢等待返回相同的結果\n這樣的情境可以使用 Singleflight，提供阻塞其餘查詢，只讓一個查詢發生的機制，並對外開放三個方法\nDo: 使用者指定 Key 與受保護的方法，同一時間只有一個保護方法會被執行，其餘進來的呼叫會等待 DoChan: 同於 Do，但返回 channel，可以搭配 timeout 使用 Forget: 有時候會希望主動讓 Key 不再保護，例如過了數秒為了避免讀取太舊的值，可以主動刪除 Key 讓下一個保護方法可以執行 (即使前者尚未返回) 更詳細內容可參考此篇 Go: Avoid duplicate requests with sync/singleflight，自己寫了一個簡單的範例 go playground\nSingleflight 的實作也十分精練，用一個 map 保存 key 對應 struct，struct 裡面放 mutext 避免同步操作 / wg 讓其他呼叫發現有人在執行就乖乖等待 ，可參考 golang防缓存击穿利器\u0026ndash;singleflight\ntypescript 實作 後來覺得頗有趣就自己實作一個 typescript 版本：sj82516/go-singleflight，原本想要多一個儲存的 adpter 支援 redis，但卡在 Object / Error 要如何 serialize / deserialize 就先中止了\nGo Assembler 往下追 \u0026quot;sync/atomic\u0026quot; 發現沒有相關的 CompareAndSwapInt32 的程式碼，這是因為這些部分是在 runtime 產生，atomic 指令必須由 CPU 提供才能保證執行時的原子性，而指令則是各平台限定，如 x86 / x64 / arm 32 / arm 64 / powerpc 等等，這在編譯的時候可以透過 GOARCH / GOOS 指定編譯的平台\n補充資料可以參考\n高階語言如何變成機器可執行的位元檔：Compiling, assembling, and linking GopherCon 2016: Rob Pike - The Design of the Go Assembler / 文字說明 Go Tools: The Compiler — Part 1 Assembly Language and Go Rob Pike 說明一開始 Go 的原始碼有使用 C/Yaac 完成編譯的方式，但因為難管理後來在 Go 1.3 開始汰換成 Go 實作\n這邊實作有趣的地方在於 Rob Pike 表示雖然每個平台的指令/暫存器名稱都不同，但是基本的使用可以被抽象化，所以 Go Compiler 會編譯出 semi pseudo code，接著依照指定的平台轉換成對應的 assembly code，這部分實作了 obj library\n這樣的好處是對於 Compiler 來說產生 semi pseudo code 就是單純的文字轉換\n在影片中的範例\n1 2 3 4 5 6 7 8 9 ADDW AX, BX ----- \u0026amp;obj.Prod{ As: arch.Instructions[\u0026#34;ADDW\u0026#34;], From: obj.Addr{Reg: arch.Register[\u0026#34;AX\u0026#34;]}, To: obj.Addr{Reg: arch.Register[\u0026#34;BX\u0026#34;]} } 最後提到他們在開發工具，直接讀 PDF 產生各平台對應的 instruction set 在思考的過程中，開始想 assembly 夾在 source code / machine code 的地位，這一篇 SO 給出了回答 Why do we even need assembler when we have compiler?\n正如同夾在中間的地位，machine code 人類無法讀，source code 又太 high level，如果要確認 compiler 是否編譯出有效的 machine code，那查看 assembly 看實際人類可讀的指令是最好的\nGo 編譯過程可參考 Go 语言设计与实现 存參個在 SO 上被扣分的發問 Why assembly is unportable，comment 有很多解釋之後慢慢細讀再補充\n","date":"2021-06-06T01:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-06-06-golang-%E4%BD%B5%E7%99%BC%E8%99%95%E7%90%86-mutex-/-rwmutex-/-singleflight/","title":"Golang 併發處理 Mutex / RWMutex / SingleFlight"},{"content":"四年前曾經對於 WebRTC 提供 P2P 影音串流感到新奇，寫下了 WebRTC-介紹與實戰；\n後來在工作中，開始更深入使用 WebRTC，也開始自架 TURN Server，從 Spec 角度認識了\nSDP Spec 閱讀筆記 RFC 5389 - STUN 協定介紹\nCoturn Server 架設教學 - on AWS\n只是使用上或理解上，始終停留於比較表層的 SDK API 呼叫或是文件閱讀，而沒有真正從大方向理解，決定從 WebRTC For The Curious 深入理解 WebRTC 的原理與細節\n本篇文章內容會涵蓋\nWebRTC 到底是什麼 WebRTC 的連線由始而終的流程 什麼是 WebRTC 以下內容理解自 WebRTC For The Curious\nWebRTC 是 Web Real-Time Communication 縮寫，本身是 API 也是 Protocol，主要是可以讓兩個 WebRTC Agent 建立 即時、雙向、安全的串流，WebRTC API 則是遵守 Protocol 定義所提供的實作，目前多家瀏覽器都有實作 WebRTC API，也有非 Javascript 的 WebRTC API 實作\nWebRTC protocol 本身只是集合了多個已知、穩定的協定而提供的完整解決方案，主要包含以下步驟\n一. Signaling 當兩個 Agent 要互相溝通時，他們不知道彼此的位置、彼此支援的影音版本等等，此時會需要 SDP 描述雙方狀態，並透過管道溝通，SDP 是一個由 Key-Pair 組成的文本協定，只要包含以下幾個項目：\nIP/Port (Candidate) Agent 預期有幾個 Audio/Video Track Audio/Video codec 支援 連線資訊 (如 Frag 等) 安全性資訊 (如 Fingerprint 等) 需注意如何交換 SDP 不再 WebRTC 定義中，可以使用任意協定如 HTTP / Websocket / XMPP 等，只要 Agent 之間能夠交換即可\n二. Connecting WebRTC Agent 要準備建立連線時，會透過 ICE (Interactive Connectivity Establishment)，Agent 本身可能在同個 Local 區網下，也可能在世界的兩端，透過 ICE 找出兩個 Agent 能夠直接連線的方式，這背後隱藏了 NAT Traversal 與 STUN/TURN 細節，會在後續介紹\n三. Security 建立雙向連線後，WebRTC 會在增加連線的安全性，透過 DTLS (Datagram Transport Layer Security) UDP over TLS 與 SRTP (Secure Real-time Transport Protocol)\nDTLS 主要用於 Data Channel，WebRTC 透過 DTLS 交握建立安全連線，其中 TLS 驗證部分略與 HTTPS 過程不同，在 WebRTC 中沒有去向 CA 驗證憑證，而是透過先前 Signaling 交換的 fingerprint 檢查\n而 Audio/Video 串流則是透過 RTP，為了多疊加一層安全性採用 SRTP 傳輸 RTP 內容，所採用的加密金鑰則是透過 Data Channel 所交換\n目前就成功建立雙向、安全的連線後，但是影音串流還必須考量到網路狀況，如封包遺失、頻寬限制等問題\n四. Communicating 採用 SCTP (Stream Control Transmission Protocol) 做傳輸狀況的監控、流量管制，實際上 SCTP 的封包是透過 Data Channel 走 DTLS\n全部總結，WebRTC 就是由這些協定所集結而成的 WebRTC API 快速過一下 API 設計\n1. new PeerConnection 建立 WebRTC Session，也就是上述的所有 Protocol 集合\n2. addTrack 建立 RTP Stream，並綁定一個隨機產生的 SSRC，之後在 createOffer 時會帶在 SDP 當中並建立對應的 media session，每當 addTrack 一次都會建立新的 RTP Stream\n只要 ICE 後成功建立 SRTP 連線，media packets 就會馬上傳送出去\n3. createDataChannel 如果 SCTP 連線還沒建立則新建立一個，預設只有在某一方要求 data channel 才會建立\n當 DTLS 成功建立連線後，SCTP packets 就會開始傳送\n4. createOffer 建立 local SDP\n5. setLocalDescription 先前的 addTrack / createDataChannel 只是暫時性的，只有當呼叫 setLocalDescription 才確定套用修改\n通常呼叫 setLocalDescription 就會傳送 offer 讓對方 setRemoteDescription\n6. setRemoteDescription setRemoteDescription 通知 local agent 對方的 candidate 狀態，如果雙方都 setRemoteDescription就可以開始 connecting\n7. addIceCandidate 通知 WebRTC Agent 增加 remote ICE candidate\n8. ontrack 當 RTP stream 接收到封包時觸發事件，封包應該會吻合 setRemoteDescription 時指定的格式\nWebRTC 會透過 SSRC 確認對應的 Media Stream\n9. oniceconnectionstatechange 如果 ICE Agent 有異動會觸發事件，像是遇到網路狀況等\n10. onstatechange ICE Agent / DTLS Agent 狀態異動時會觸發事件\n一. Signaling WebRTC Agent 一開始建立時並不知道該與誰通訊、也不知道該用什麼格式，所以會需要在初始化時做 Signaling，讓後續的連線有機會發生\nWebRTC 並沒有硬性規定 Signaling 傳輸的機制，常見做法是透過 Websocket\n什麼是 SDP WebRTC 仰賴 SDP 交換建立連線所需的資訊，SDP 本身相當好理解，只是要去理解 WebRTC 自定義的參數與代表值\nSDP 定義在 RFC4566，每一行代表一個 key / value 組合，一份 SDP 中可以包含多個 Media Description，而一個 Media Description 通常代表一個 Media Stream\n例如說一個影片有兩個 Video Stream 外加三個 Audio Stream，那就需要五個 Media Description 描述\n如何解讀 SDP SDP 格式為 k=value，鍵通常是一個英文字母代表，等號後連接值，以下是定義的幾個鍵\nv: Version，代表版本 o: Origin，通常代表 id 用以識別 s: Session 名稱 t: Timing，通常是 0 0 m: Media Description a: Attribute 最常見的鍵 c: Connection，通常是 IN IP4 0.0.0.0 Media Description 一個 Session Description 通常包含多個 Media Description，而 Media Description 則由一連串的 RTP Payload 的格式定義，實際的 codec 會以 rtpmap 表示 如以下\n1 2 3 4 5 6 v=0 m=audio 4000 RTP/AVP 111 a=rtpmap:111 OPUS/48000/2 m=video 4000 RTP/AVP 96 a=rtpmap:96 VP8/90000 a=my-sdp-value 這裡定義兩個 Media Description\n一個是 audio 帶著 fmt 111，並指定 codec 為 OPUS 另一個是 video 帶著 fmt 96，codec 為 VP8，並帶著第二個屬性值為 my-sdp-value\nWebRTC 如何利用 SDP 接著談到 WebRTC 如何使用 SDP 的格式，WebRTC 採用 Offer / Answer 架構，一方 Agent 先主動提供 Offer，另一方再決定是否接受，並回應 Answer 確定 codec 等\nTransceivers Transceiver 指定 Media 傳輸的方向，例如說可以指定「我要用這個格式發送」「我要用這個格式接收而且我不想要收到任何資料」等，主要有四個值\nsend recv sendrecv inactive\n每產生一個 Transceiver 就會對應有一個 Media Description，並帶有一個 attribute 描述如 a=sendrecv 其餘常見屬性 group:BUNDLE:\n有些 WebRTC Agent 會在一個 connection 上傳輸多種類型的 media stream\nfingerprint:sha-256:\nDTLS 要用到的憑證 sha 256 值，用來比對憑證的正確性\nsetup:\n決定 DTLS Agent 在 ICE 連線建立後的行為\nice-ufrag: ICE Agent 的 user fragment，用來驗證身份\nice-pwd: 同驗證身份使用\nrtpmap: 指名 RTP Payload 的 codec\nfmtp: Payload 額外屬性質，例如 video encoding 方式等細節\ncandidate: ICE candidate，供後續 ICE Agent 建立連線使用\nssrc: Media Stream 的 ID\n範例 SDP 可以在這邊產生 WebRTC samples Munge SDP\n二. Connecting 上一步 Signaling 交換完連線資訊，接著就要建立雙方 P2P 連線，但現實的網路世界在有不同種的 NAT 存在，使得建立連線時會有一些阻礙，為了克服衍生出了 NAT 穿越機制 Interactive Connectivity Establishment (ICE)\nNAT 種類 NAT 是在一個區域網路下，由統一的對外路由器共用 public ip，而內部的機器則使用 private ip，如果內部機器需要與外部網路溝通，則透過路由器收發，路由器會記住哪些封包要轉送到對應的內部機器；\n不論是因應 IPv4 位置不夠用，又或是安全性考量，NAT 普遍存在於現今的網路環境下\n*圖片來自 webrtcforthecurious\n而路由器針對封包的發送會有不同的對應產生，假設 Machine A 向 public Host 1 發送請求，則會在路由器註冊一個 Mapping\n1. Endpoint-Independent Mapping 如果 Machine A 要向 public Host 2 發送請求，則可以復用同一個 Mapping\n2. Address Dependent Mapping 如果 Machine A 要向 public Host 2 發送請求，則會產生新的 Mapping\n3. Address and Port Dependent Mapping 如果 Machine A 要向 public Host 1 但是不同的 port 發送請求，則會產生新的 Mapping\n針對外部封包的接收會有不同的限制\n4. Endpoint-Independent Filtering 任何 public Host 都可以對同一個 Mapping 送封包給 Machine A\n5. Address Dependent Filtering 只有 Machine A 先送過 public Host 1，則 public Host 1 才可以透過此 Mapping 送封包給 Machine A\n6. Address and Port Dependent Filtering 只有 Machine A 先送過 public Host 1 + port 1，則 public Host 1 + port 1 才可以透過此 Mapping 送封包給 Machine A\n小結，可以看到 NAT 限制由上往下越來越嚴謹，一開始任何 public ip 都可以送，接著限制 ip address，最嚴格是要 ip address + port 都吻合才可以送\n說完了 NAT 種類，但這些種類會怎麼影響 P2P 連線呢？\nSTUN 如果今天兩個 webrtc client 都在 NAT 之後，那要建立連線就必須先知道 自己的 public IP，這也是 STUN 協定 所做的事情，讓 client 可以知道 NAT Mapping 對外的 public IP\n試想以下情境\nclient A / client B 都在各自的 NAT 之後，他們想要建立 P2P 連線 client A / client B 此時不知道自己的 public IP client A / client B 分別向 STUN Server(有固定 public IP) 發出請求，此時 NAT 上會建立 Mapping + client A / client B 取得 public IP 交換 如果 NAT 是屬於 Endpoint-Independent Mapping + Endpoint-Independent Filtering，則 client A / client B 透過 SDP 交換資訊後，就可以建立雙向連線了 反思如果 NAT 是 Address Dependent Mapping + Address Dependent Filtering，透過 STUN 之後 client A / B 可以直接連線嗎？\n答案是不行的喔，因為 client A 建立了與 STUN server 的 NAT Mapping 只能兩者用，client B 想要送封包給 client A 就會被擋下來\nTURN 遇到 STUN server 無法用，接著就要用 TURN，TURN server 會居中，所有的封包都透過 TURN server 轉發，但這樣也就不是 P2P 連線了\nTURN 的運作機制建立於 STUN 之上，擴充 STUN 封包的格式，流程大概是\nclient A 建立 Allocation，TURN server 會保留一個 port 給 client A，並回傳 Relayed Transport Address，可以想像成搬進新大樓時一樓信箱有一個格子專屬於你的，所以寄信到這個格子的資料都是要給你的 當 WebRTC 在交換 SDP 時將 Relayed Transport Address 給對方 為了避免其他人亂寄資料，client 必須在 Relay Server 幫對方增加 Permission Allocation / Permission 都有時間限制(LIFETIME參數控制)，可以後續 Refresh 另外一種對於 NAT 的分類 (錐形、對稱型) 道理也是類似，Peer A 註冊的 Mapping 要在什麼情況下 Peer B 可以直接拿來用，如果不行 (對稱型) 就要走 TURN Server，可參考 30-28之 WebRTC 連線前傳 - 為什麼 P2P 連線很麻煩 ? ( NAT ) ICE 結合以上的連線方法，就組合成 ICE protocol，每個 ICE Agent 會分成 Controlling / Controlled，有一方主導連線過程，按照以下步驟，嘗試找出雙方可以互通的連線\n1.Candidate Gathering Candidate 是指可能被使用的連線方式，共有幾種\nhost: local interface srflx: Server Reflexive，也就是從 STUN 那拿到的 public ip relay: 透過 TURN server 拿到的 ip prflx: Peer Reflexive，從 peer 那邊拿到自己的 ip，晚點補充 以上資訊收集完後，透過 signaling 方式發送給對方\n2.Candidate Selection 因為雙方都蒐集了各自的 candidate，接著就是 full mesh 組合出 Candidate Pair，並按照優先順序開始測試連線，可以成功收發訊息則標記為 Valid Candidate，主導方會挑一個 valid candidate 標記成 Nominated Pair，接著雙方在測試一次是否能成功收發訊息，完成則標記為 Selected Candidate Pair，這一個 session 就用一組連線方式\n3. Restart 如果 Selected Candidate Pair 有任何問題，則以上步驟重新來過\n最後補充一下 Peer Reflexive，為什麼會有從對方拿到自己所不知道的 ip 這種狀況，主要會發生在 Host Candidate 與 Server Reflexive Candidate 溝通\n試想 client A 透過 host candidate 多半拿到是自己的 private ip，此時透過 NAT 向 client B 發送請求，client B 會拿到 NAT 的 public address (也就是 client A 的 Mapping)，此時返回 public address 給 client A，此時就是 prflx 其實上述的過程類似於 STUN server 的功用 因為 ICE protocol 是有經過驗證的，所以 client A 可以放心的拿這組 public ip 當作自己的 ip\n三. Securing 該怎麼確保收到的 sdp 來自預期的 client / 要如何確保收到的 media 是來正確的 client，WebRTC 結合計有的加密方式 DTLS / SRTP，確保機密性/完整性/身份驗證\nDTLS DTLS 是 TLS 的近親，只是 DTLS 的傳輸層是 UDP 而 TLS 是 TCP； DTLS 是 Client/Server 架構，某一方會先發起通信，接著雙方會交換憑證，而確保憑證的正確性是比對 SDP 中的憑證 hash 值；\n完成交握後會取得共同對稱密鑰，後續就用此加密 *圖片來自 webrtcforthecurious\n流程基本與 HTTPS 很像，只是 client 也要提供憑證給 server，在 https 這一步是可選的但通常忽略\nServerHello/ClientHello 各自再產生 random secret，拿到憑證後 client 產生 pre-master 並加密傳送給 server，雙方可以得到相同但其他人不知道的 master key\n過程中突然想到，既然是 client 最後把 pre-master 加密，那為什麼還要 client/secret random number ? 原因是為了避免回放攻擊，參考 Why does the SSL/TLS handshake have a client and server random? 如果少了 Server random number，那駭客即使不知道內文也沒關係，重複回放請求，Server 沒辦法知道這些請求是過時的 反之如果有 random number，Server 就會發現 master key 不同而中斷請求\nSRTP SRTP 是針對 RTP 設計的加密版本，本身沒有定義產生 key，所以 WebRTC 就拿 DTLS 交握後產生的共同金鑰當作 SRTP 的加密金鑰\n具體內容就先略過\nReal-time Networking 網路狀況會很大幅度影響即時影音服務的品質，品質則是在 Quality \u0026amp; Latency 中做抉擇，評估網路狀況時會看\nBandwidth: 通道上最大可傳輸的量 Transmission Time and Round Trip Time：從發送到收到回應的時間 在網路狀況不好的情況下，會遇到幾種負面情況\nJitter: RTT 時長時短，導致畫面卡頓\n可以透過 Jitter Buffer 改善問題，收到封包後先 queue 一小段時間，等到可以完整解析 frame 在 render Packet Loss: 某些封包掉了，導致畫面跳格\n解決 Packet Loss 問題可以透過 收到封包後回傳 ACK 收到封包後回傳收到的封包 SACK 回傳沒有收到的封包 NACK (遇到跳號時) 今天如果網路狀況不好，則發送方應該要降低影音品質 / 減緩發送速度，避免網路狀況惡化；\n另一方面當網路狀況改善，則發送方也應該要知道，進而提升傳輸的品質與速度\n那接收方如何回報網路狀況給發送方呢？\nMedia WebRTC 本身不管底層的影音 codec，只要雙方都可以處理就好，透過 RTP 傳輸影音資料 / RTCP 處理網路狀況+控制流量\nRTCP 如何整理網路狀況 RTCP 透過幾種方式，讓發送方可以得知網路狀況\nReceiver Reports\nSender 再發送時打上當時的時間戳記 t1，Receiver 收到後加上自己處理的時間 t2，回傳 Report 給 Sender，最後 Sender 收到的時間為 t3\n則 Sender 可以算出 rtt = t3 - t1 - t2 REMB\nReciver 根據統計 packet loss，跟 Sender 回報預期的 bitrate 1 2 if (packetLoss \u0026lt; 2%) video_bitrate *= 1.08 if (packetLoss \u0026gt; 10%) video_bitrate *= (1 - 0.5*lossRate) 但這個方法理論上看起來不錯，實際應用上卻很糟糕，因為 encoder 在動態改變 bitrate 下沒有這麼有效率 3. TWCC\n追蹤每一個封包的狀況，可以讓 Sender 有更即時、清楚的網路狀況\n這一段理解不這麼全面，未來有機會補上\n總結 建立 WebRTC 連線過程，總共分成\nSignaling： 交換彼此資訊的管道，透過 SDP 描述這些資訊 Connecting：為了應付複雜的網路環境(NAT)，透過 ICE 解決連線問題 Securing：確保交換的資料是安全的 Real-time communication：即時傳送資料 ","date":"2021-05-30T07:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-05-30-%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90-webrtc/","title":"深入理解 WebRTC"},{"content":"工作了數年，可能是一開始寫動態語言的關係，鮮少注意到抽象化 / 封裝 / 物件導向的設計，導致程式碼越來越難以維護，深知這是自己技能上的弱點，開始去學習 TDD / 重構，希望可以寫出好懂 / 好維護 / 有測試保護的乾淨程式碼\n剛好最近換工作開始寫 Ruby，就順便買了這本 【Refactoring Ruby Edition】，原本是 Martin Fowler 大大用 Java 當作範例所編寫，這一本是由另外兩名作者與 Martin Fowler 掛名，採用 Ruby 當作範例並加入 Ruby 的語言特性，不影響對於重構的理解與實踐\n目前看了一半覺得很受用，整理很多 code smell，以及遇到時該如何有系統的重構成乾淨的程式碼，有一些原則可能會互斥 (抽類別或是把類別塞回去)，書中也有提及該如何判斷何者為佳\n以下內容對應的是第一章 「Refactoring, a First Example」，程式碼於此 sj82516/refactoring-ruby，以下每個步驟都會對應到一個 commit\n範例 這是一個租片服務，計算目前用戶租片的費用與回饋點數，影片有分三種類型\n重構之前 一開始先觀察這段這段程式碼， Customer 中的 statement method 明顯很長一段，負責許多事情，包含計算與整理輸出規格\n從功能面上來看，這段程式碼運作符合預期，對於直譯器來說也不存在醜不醜的程式碼，但今天如果需求變更，需要人類的介入去修改程式碼，那可讀性就很重要了\n例如說今天要增加一個輸出成 html 格式，目前的寫法只能 copy statement 並修改輸出格式，又如果之後要調整影片的計價方式，那很不幸兩個方法都要同步改動\n當你發現要增加新功能很難改動時，先重構到你覺得很好加新功能後，再加上新功能\n開始重構 以下按照書中建議，開始進行重構\ninit\n1. 寫測試 如何確保在重構過程不把功能改壞？ 寫測試！ 透過寫測試可以明確表達我們預期的程式碼行為，在與 PM 溝通時透過具體的測試案例也可以避免有認知上的歧異\ntest: add unit test\n2. 拆解與重組 - Extact Method 首先拆解過於複雜的 statement 方法，要拆解首先找到邏輯上比較緊密的區塊，並拆分出獨立的方法，例如 case 就很適合\n在拆分時要注意使用到的變數有沒有被修正，例如 rental 沒有被改動所以當作參數傳進去就沒事，但是 this_amount 有被改動，如果被改動只有一個變數，那就當做 function return 即可\n別忘記每改一小步都要跑測試\u0008，尤其是動態語言容易卡在變數名稱寫錯等小問題上\nrefactor extract amount_for method\n2-2 重新命名幫助認知 在新的 amount_for method，改動一下變數名稱有助於含義的表達，少用太廣泛的含義例如 element / i 這類\nAny fool can write code that a computer can understand. Good programmers write code that humans can understand.\n不得不推薦一下 RubyMine，一年訂閱要 $89 鎂但是有 Refactor 系列的輔助工具超方便，透過 shift 鍵 * 2 叫出 action dialog 後輸入「refactor this」，就會有一系列的重構方法讓你挑選\nrefactor: rename\n3. 把邏輯放在正確的物件上 - Move Method 仔細看一下 amount_for 方法中，運算邏輯的資料來源都從 Rental 這個物件而來，都沒有用上 Customer 物件上的資料，這時候可以合理的懷疑是不是把方法移到 Rental 會更加合適\n接著修正 Customer 的引用\nrefactor: move method to rental\n3-2. 移除不必要的區域變數 - Inline 這時候 this_amount 就有點多餘了，透過 inline 變數直接從 rental.charge 讀取\n或許會有人爭論：多次呼叫方法會降低效能，但是原則上重構是為了簡潔，效能問題等到發生時在優化即可，尤其是現在的運算資源往往很足夠，好維護上帶來的開發資源節省會比運算資源來得重要\n回過頭來看區域變數哪裡不好？\n試想我們剛剛在抽方法，需要擔心區域變數有沒有被其他人讀取或修改，而更糟糕的事會有人把區域變數重複 assign 於不同的用途，為了避免多餘的擔心與閱讀障礙，移除區域變數是會有幫助的\nrefactor: remove total_amount\n3-3. 修改 frequent_renter_points 同樣的修改可以套用到 frequent_renter_points，不過 frequent_renter_points 本身是區域變數，而且是不斷地隨著 loop 而改變，這時候當作參數傳進去在用 return reassign 有點沒必要 refactor: extract and move frequent_renter_points method\n4. 移除區域變數 Remove Temp 先前提到區域變數的缺點，接著移除 total_amount 與 frequent_renter_points，直接在使用的地方呼叫方法，同樣的，呼叫方法的效能疑慮只有在真的有問題時再考慮\nRuby 比較妙的是方法呼叫可以省略()，所以看起來跟呼叫變數沒什麼兩樣\nrefactor: remove frequent_renter_points\n5. 透過多型取代 case 如果要使用 case，那記得 case 中的判斷依據應該是物件本身的資料\n目前在 Rental 的 charge 中，會依照 Movie 的 type 分不同的收費方式，這裡如果未來 Movie 要增加 type / 調整每種 type 的收費方式，一直來改 Rental 的 charge 呼叫端不太合適，所以將 charge 的邏輯歸類到 Movie 中 refactor: move charge logic to Movie\n相同的道理也套用於 frequent_renter_points 上\nrefactor: move frequent_renter_points logic to Movie\n5-2. 將 case 轉換成多型 Movie 中的 charge 依照不同的 type 收費，這聽起來就很像繼承可以處理的事情，可建立多個不同的 Sub Movie Class\n但這點並不適用於目前的場景，因為 Movie 的 type 會隨著時間而改變，並不是初始化後就不變的，所以更適合用 State/Strategy Pattern\nState/Strategy Pattern 最大差異在於 State 表達的是狀態的改變，而 Strategy 代表的是計算時的演算法改變，兩者看起來蠻像的，主要在於命名如何表達設計者的意圖\n這邊比較適合用 State Pattern，因為 Movie 的 type 比較是一種 state，會持續一段時間而非當下計算完就結束\n這邊第一步先在 price_code= 方法中初始化狀態，先暫時用 case 頂替等等會換掉 refactor: add state pattern to extract charge logic\n5-3. 抽換 frequent_render_points frequent_render_points 也可以用類似的方式，但注意到只有 New Release 的計算方式不同，這邊可以用 Module 的方式設定預設方法，在 New Release Sub Class 中在複寫\nrefactor: update frequent_renter_points\n5-4. 移除 price_code= 最後讓呼叫者將初始化的 price class 傳入，就可以省去 price_code= 中的 case 使用\n導入 State Pattern 花了幾個步驟，主要是在未來增加新的計價模式，舊的 Code 都不會受到影響，只要新增就好，符合 Open Close 原則\nrefactor: change Movie initialize\n結語 重構有趣的地方在於一步一步調整程式碼到有彈性的方式，所以不用一開始就追求完美的設計，避免了 over design 的問題，因為只要有重構的習慣就不會讓程式碼僵硬到無法維護\n之前工作常常遇到大家會說 Legacy Code 多到無法維護而需要「重寫」，但回過頭來看如果用一樣的邏輯跟開發方式，重寫幾百次最後都面臨同樣的困境，學習重構逐步優化，並調整對於物件導向、設計模式的認知，這才是長久之計\n","date":"2021-05-01T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-05-01-refactoring-ruby-edition%E4%B8%80-%E9%AB%94%E9%A9%97%E9%87%8D%E6%A7%8B/","title":"【Refactoring Ruby Edition】(一) 體驗重構"},{"content":"還記得約在 2016 年第一次在募資平台上看到 ARRC 的募資活動，才知道交大有這麼一位熱血的教授 吳宗信教授，想要打造一隻純 MIT 的火箭，找了當時一起赴美唸書的朋友，串連多所大學的專業一起為了太空夢而努力\n台灣本土火箭 要讓太空旅行夢想成真 | 吳宗信 Jong-Shinn Wu | TEDxTaipei\n當時我還在馬祖當替代役，領著時薪 8元的薪水 XD 還是贊助了基本方案，最後收到贊助品是一本講太空與 ARRC 的發展史的書(繪本?!)\n其實我對太空一竅不通，說真的也沒有特別的嚮往，但我很認可教授所描繪的太空時代，在 2021 年回來看，像 Starlink / Facebook 都在積極發射衛星，連教授所描繪的透過衛星查看商業模式也在我喜歡的影集《Billionares》真實上演；\n另外還有太空科技是技術的火車頭，可以帶動基礎產業一起向上升級，這也是我覺得很熱血、很值得投資的事情\n所以在 2020年推出第二波的贊助時就立馬贊助，拿到台灣太空隊的衣服跟隊員證！ 最後也憑證有這次參觀的機會，以下將整理與分享參觀的過程與心得\n*補個當初教授跟呱吉的對談，超熱血的 【呱吉直播】政治電台EP12：轟隆隆隆衝衝衝拉風引擎火箭發動\n這次去的是交大 ARRC前瞻火箭研究中心 火箭的種類 火箭在高空因為外界空氣稀薄，所以需要自備燃料與助燃劑，基本種類可分成\n固態火箭：\n燃料與助燃劑先混合好，並以固態儲存，一點火就會開始劇烈燃燒，最入門的是蔗糖火箭 sugar rocket，可以看 自製硝糖火箭！大雅一號能夠順利飛向宇宙嗎？【胡思亂搞】(Feat.@黃小潔Jerry​ , ARRC前瞻火箭研究中心) 液態火箭：\n燃料與助燃劑都是液態，但因為是液體所以混合後容易爆炸，危險度較高 混合式火箭：\n燃料是固態，助燃劑是液態，可以用閥門控制助燃劑來控制火力，甚至做到懸浮的功能，這是另外兩者所不行的 混合式火箭是太空科技的未來，具有可控制 / 安全的特性，也是 ARRC 接下來 HTTP 系列火箭的發展方向\n交大「ARRC前瞻火箭研究中心」 完成全球首次混合式火箭空中懸浮飛試\n過程中有問為什麼混合式很好其他國家沒有率先發展？ 得到的答覆是美國當初二戰後綁架德國的飛彈專家，有優良的液態火箭技術 / 日本在糸川英夫的研究下發展固態火箭，因為火箭的研發成本非常高，所以既有的技術很好就會採用，又因為火箭是高機密科技，不會對外輸出，所以台灣自己只能從頭摸索，沒有過往的束縛是劣勢也是優勢\n有個故事是日本的固態火箭發展太好美國會怕，反而輸出液態火箭給日本，透過另一種方式限制日本的發展 日本的宇宙政策（下）：終結自主開發，美國向日本提供液體燃料火箭技術的兩點理由\n火箭的噴嘴與燃料 左圖為火箭的燃料，據說就像是塑膠跟樹脂的材質，燃燒會產生大量氣體，中間有一個洞會讓氣體往外排放\n右圖的上方是噴嘴，下方是機電組，噴嘴也是大有學問，在低太空會用小噴嘴，高太空則用大噴嘴，主要是為了讓燃料噴出的氣體得到最大的推進動力，氣體流動速度在小於音速時洞口越小則速度越快，但是到超音速之後反而是洞口越大速度越快，這也就是噴嘴設計是大到小在小到大；\n另外噴嘴大小也是為了配合外界大氣壓力，同樣是流體的特性，當排出氣體的壓力與外界氣壓相同時可以獲得更大的動能，所以高太空因為氣壓低所以噴嘴要更大\n以上據說是流體力學有教過的原理，我自己沒有修過如果有寫錯再麻煩糾正\n這也是多節式火箭的設計原因之一，下次再看火箭發射的過程不妨注意看看噴嘴的部分\n碳纖維 火箭要高速推進到非常遙遠的彼岸，需要很大的動力 \u0008/ 堅固同時輕量化的重量，在這種情況下，通常是用很薄的金屬打造一層外殼 (圖片右上)，在用碳纖維纏繞在外面增加強度 (左下)，一組四根 (右下)\n圖片中的是放燃料的地方，也就是火箭的引擎，也就是上面白色中空的棒棒加大版，圖片右上是金屬殼，實際長約 1.2 公尺，但他非常得輕，所以才要用外面的金屬支架保護，避免變形\n最後用碳纖纏繞技術，將金屬殼纏繞，這點在影片中有提到是一間熱心的廠商幫忙做的，在導覽過程中也提到他們這個纏繞技術很不錯，最近通過了驗證可以幫忙在太空相關的產品 【HTTP-3A 研發進度｜火箭碳纖維纏繞外殼】，再次應證了台灣的產業實力真的很棒，只是沒有一個品牌跟整合的龍頭\n但大多數的碳纖維都只能用人工去拼貼，像是碳纖維腳踏車都是純手工，複雜的部分還沒辦法用機器取代，在 ARRC 也有很多地方需要自己買碳纖維的布自己動手，這位幫忙導覽的同學就自己動手做了一台四維的碳纖維切割機(下圖)，他說外面一台可能要上百萬，自己 DIY 只花了五、六萬而已，而且省下兩、三個人力非常棒，這位同學最後說他現在才大四，從大一就開始幫忙了，滿滿的 respect! 可惜忘了問同學的大名\n圖片左上是火箭尖端的部分，因為要耐高溫所以用金屬的鼻頭，有提到火箭運用的是碳纖維與金屬的混合使用，在零件之間是透過特殊的膠水固定，螺絲基本上是不受力的\n隨手拍拍 三張小圖是還在研發的 HTTP-3A 喔！ 右下角是到時候搬運火箭的載具，因為火箭非常非常輕，所以路上搬運會有變形，需要有外層支架保護，記得在某隻影片有提到台灣在東部發射還不錯，穩度低有足夠的地球自轉速度，同時慣性會往太平洋射比較安全\n祝發射順利以及無預警出現的吳宗信教授\n結語 唱衰台灣的人很多，但肯出來改變的人很少，這次參觀的過程看到有一群踏實的人在努力做一件不可思議但逐步實現的過程，覺得滿滿的踏實與熱血，自己有這個機會可以用微薄的贊助支持覺得很開心，也期許自己的未來也可以踏實的走下去\n","date":"2021-04-23T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-04-23-%E4%B8%80%E6%97%A5%E5%8F%83%E8%A7%80-arrc-%E5%AF%A6%E9%A9%97%E5%AE%A4/","title":"一日參觀 ARRC 實驗室"},{"content":"【跨程式語言上手】系列第一篇，最近換工作學習了 Golang / Ruby，可以說是設計理念處於對立面的程式語言，在學習中不斷拿兩者比較，找到很多有趣的地方\n在這樣的過程中，慢慢找出自己上手新程式語言的 pattern，也就是逐步填補自我疑問的過程，像是「怎麼宣告變數？」「怎麼寫測試？」「api server / http request 怎麼發？」「併發或效能怎麼處理？」等等共通的疑惑點\n坊間有很多善心人士的教學，但往往都缺一點我想了解的資訊，例如 Google 搜尋「Ruby 教學」的中文素材，跑出這幾個很棒的教學\n高見龍大大的 為你自己學 Ruby on Rails Ruby on Rails 實戰聖經 Ruby 官網的 二十分鐘 Ruby 體驗 Ruby 教學會被綁在 Rails 教學中或許是 Ruby 生態特有的現象，但普遍程式語言教學也都缺少\nTesting：包含 Testing Framework / Unit Test / Mock、Stub Module：Core Module / Local Module 怎麼載入 Concurrency：底層如何處理併發/平行運算 程式語言多用於後端，所以我會在意寫 api server / http request 的感覺是怎樣 可以說這幾點都是比較進階/偏科的議題，但對我的工作很重要，也是這系列的起源，我想要重新寫一份對我自己來說完整的程式語言教學，過程中會拿我已經熟悉的程式語言如 Javascript 跟一點點的 Golang 做對比\n目標會著重於有經驗的程式設計師，已經熟練任一程式語言，想要快速上手或是品味另一門語言的人\n以下內容會包含\nRuby 設計理念與起源 基礎語法 模組 測試 Http / API 相關 其他補充 內容大量參考上附的參考資料，並融入自己的淺見，會隨著使用時間的拉長持續修改，有什麼不同的意見歡迎留言分享\n1. Ruby 設計理念與起源 Ruby 是一門 Dynamic Language，運行在 Ruby Virtual Machine 上，本身是弱型別但沒有 JS 中隱式的轉型 (ex. 1 + \u0026ldquo;23\u0026rdquo;)，在 3.0 加入 Type safety 工具 TypeProf 幫助檢查型別問題\n透過範例簡單看一下 Ruby 幾個特別的設計理念\nRuby is designed to make programmers happy 出自於 The Philosophy of Ruby A Conversation with Yukihiro Matsumoto, Part I，Ruby 給予開發者很高的自由度\n定義 symbol 在 Ruby 的框架下產生自己的 DSL，例如 sinatra 這個 web framework，看範例會以為根本不是 ruby 寫的 支援 Meta programming 可以在 Runtime 改變類別行為 可以複寫任意的方法，包含原生類別 同一種功能可以有非常多種寫法，光是迴圈可以用 while / for in / each / until / begin while 等 Seeing Everything as an Object 在 Ruby 的世界中，幾乎每一個變數都是物件，包含 1+2 也可以寫成 1.+(2)，1 本身是 Integer 類別裡頭有 + 這個方法\n這讓 Ruby 很適合 OOP，也帶來很多的彈性，像是在 operator overwrite\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Integer alias :plus :+ def + (other) puts self.to_s + \u0026#34; is adding \u0026#34; + other.to_s self.plus other end end puts 1 + 2 # 輸出結果 # 1 is adding 2 # 3 Integer 是預設類別，Ruby 遇到類別重複宣告時會合併，接著我們在 Integer 宣告 plus 是原本 + 的別名，接著覆寫 + 先打印出 is adding 字串在回傳，在 Ruby 中預設 function 最後一行即使不顯式宣告也會 return\n透過匿名函式支援 Functional Programming Style 在 Ruby 世界中，不像 Javascript / Golang 把 function 視為一等公民\n在程式語言中，所謂的一等公民條件是\n可以傳入 function 當作參數 可以被 function 當作 return 值 可以被儲存於資料結構中使用 但是 Ruby 也還是匿名函式的語法，大致如下\n1 2 3 4 5 6 7 8 9 def sum(x) total = x proc { |y| x += y } end sum_five = sum 5 puts sum_five.call(5) puts sum_five.call(5) puts sum_five.call(5) 後續會有更詳細補充，但至少 Ruby 世界中也是可以做到 functional programming 的\n綜合以上，Ruby 是一門彈性很大、很自由的語言，這是一把雙面刃，對於新手可能也不是這麼友善，畢竟有太多語法跟關鍵字要去熟悉\n如何安裝 可以從官網下載安裝 Installing Ruby，或是先安裝 Ruby 版本管理工具如 RVM\n套件管理 安裝完 ruby 後，也同時安裝了 gem，gem 是 ruby 套件管理工具，可以安裝或發佈自己的套件\ngem 我一開始理解成 npm，但 npm 層級高了一些，例如 gem 並沒有做到版本控制的功能，gem + bundle 比較是 npm 的組合\n詳細可參考 Ruby 的 Rvm VS Gem VS Bundler 的差別\n基礎語法 變數宣告 不用宣告型別 變數可以改變型別 但是沒有隱式的型別轉換 變數的 scope 只有當前的 context，但要注意匿名函式會讀取當前的 context，並不會一直往上查找，除非用全域變數 $ 開頭 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 x = 123 x = \u0026#34;123\u0026#34; x = 123 + \u0026#34;123\u0026#34; # 拋錯 TypeError (String can\u0026#39;t be coerced into Integer) sum = 5 def func puts sum # 拋錯 end 1.upto(5) { |i| sum += i } # 這樣是可以的，因為 block 是用宣告當前的 context $sum = 5 # 全域變數可以 def func puts $sum end 命名規則 變數名稱常用蛇形命名法 變數全大寫代表常數，但是常數被改會有 warning 不會有錯誤 Class/Module 名稱開頭大寫 1 2 3 naming_convetion = 123 FIVE = 5 FIVE = 4 # warning: already initialized constant X Symbols 建立唯一且不可變的物件，用 : 開頭，重複宣告都會指向同一份記憶體位置 (透過 object_id 識別)，而字串每一次宣告都會在記憶體產生新的一份 String Object，如果是要單純用來識別 Symbol 效能會比 String 好上很多喔\n1 2 3 4 5 6 7 8 9 hello = :hello world = :hello puts hello == world puts hello.object_id == world.object_id #true hello = \u0026#34;hello\u0026#34; world = \u0026#34;hello\u0026#34; puts hello == world puts hello.object_id == world.object_id #false Hash Ruby 有 Hash，可以用 =\u0026gt; 或 : 分隔 key value，但是兩者有很大的差異\n=\u0026gt; 非常的自由，key 值可以是任意的值 : 的 key 只能是 symbol，如果放字串會直接轉成 symbol\n要非常小心 string 跟 symbol 是不同的，實作上很容易踩到這個坑 1 2 3 4 5 6 7 a = { \u0026#34;123\u0026#34;: \u0026#34;123\u0026#34; } b = { \u0026#34;123\u0026#34; =\u0026gt; \u0026#34;123\u0026#34; } puts a[\u0026#34;123\u0026#34;] # nil puts a[:\u0026#34;123\u0026#34;] # \u0026#34;123\u0026#34; puts b[\u0026#34;123\u0026#34;] # \u0026#34;123\u0026#34; puts b[:\u0026#34;123\u0026#34;] # nil Ruby 中幾乎都是物件，有內建很多便利的方法 1 2 3 4 5 x = 1 puts x.methods # 列出所有 Integer 包含的 method puts x.odd? # 是不是偶數 puts x.class # 類別 puts x.to_s # 轉成字串 陣列 1 2 3 4 5 arr = [1,2,3,4,5] puts arr.include?(2) puts arr.push 0 puts arr.pop loop / control flow 條件式 基本的 if / elseif / else 與三元判斷式\n1 2 3 4 5 6 7 8 9 10 x = 1 if x.odd? puts \u0026#34;x is odd\u0026#34; elsif x.even? puts \u0026#34;x is even\u0026#34; else puts \u0026#34;never happen\u0026#34; end puts (x.odd?) ? \u0026#34;x is odd\u0026#34;:\u0026#34;x is even\u0026#34; switch case 採用 case / when / else 語法，不用加 break case 如果沒有接參數，則 when 條件可以放 statement / 如果有接參數，則 when 條件放常數 如果希望 case when 結果賦值給變數，可以用 when \u0026hellip; then 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 x = 1 case when x.odd? puts \u0026#34;x is odd\u0026#34; when x.even? puts \u0026#34;x is even\u0026#34; else puts \u0026#34;never happen\u0026#34; end case x when 1..10 puts \u0026#34;x is in 1 to 10\u0026#34; else puts \u0026#34;x is not in 1 to 10\u0026#34; end val = case x when 1..10 then \u0026#34;x is in 1 to 10\u0026#34; else \u0026#34;x is not in 1 to 10\u0026#34; end # val = \u0026#34;x is in 1 to 10\u0026#34; 迴圈 方式很多種\n先檢查條件的 for in / while / until 後檢查條件的 begin \u0026hellip; (when/until) Enumerable 物件可以用 each 但沒有常見 for(initialExpression; conditionExpression; incrementExpression) 宣告\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 arr = [1,2,3,4,5] i = 0 while i \u0026lt; arr.size do puts arr[i] i += 1 end i=0 until i \u0026gt;= arr.size puts arr[i] i += 1 end i=0 begin puts arr[i] i += 1 end while i \u0026lt; arr.size for i in arr do puts i end arr.each { |x| puts x } Error handling 透過 raise 拋出錯誤 透過 rescue 接錯誤，可以更進階指定錯誤類型 1 2 3 4 5 6 7 8 9 10 11 12 13 begin #... process, may raise an exception raise ArgumentError rescue ArgumentError puts \u0026#34;ArgumentError\u0026#34; rescue =\u0026gt; error puts error #... error handler else #... executes when no error ensure #... always executed end 進階資料請參考 How to Rescue Exceptions in Ruby\nFunction function 不用定義回傳值 呼叫可以省略括號 預設最後一行會回傳，不用在顯示宣告 return 如果呼叫的參數數量跟宣告不同會拋出錯誤 1 2 3 4 5 def add (a, b) a + b # 等同於 return a + b end puts add 1,2 Class 預設 Class 名稱開頭大寫 支援繼承 Successor \u0026lt; Predecessor 要建立 instance 透過 Class.new，會呼叫 class 中的 private method initialize 宣告 instance 變數以 @ 開頭 / 宣告 class static 變數用 @@ 需要顯式指定針對 instance 變數的 getter/setter，或是用 attr_accessor/attr_writer/attr_reader 增加 支援 public/protected/private，但跟其他語言的 private 不太同，以下節錄自高見龍大大的文章 因為在 Ruby 裡所謂的 private 方法的使用規定很簡單，就只有一條：「不能明確的指出 receiver」。用白話文講，就是「在呼叫 private 方法的時候，前面不可以有小數點」。也就是因為這樣，在 Ruby 的 private 方法其實不只類別自己內部可以存取，它的子類別也可以，並沒有像其它程式語言一樣的繼承限制\n定義 static method 可以用 def self.method，或是用 class \u0026lt;\u0026lt; self 沒有 interface / method overloading / polymorphism 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class Person attr_accessor :name def initialize(name, age) @name = name @age = age end def hello puts \u0026#34;Hello, my name is #{@name}\u0026#34; end def self.show_specy puts \u0026#34;we are Mammals\u0026#34; end =begin 等同於上者，此方法適和於大量定義 class \u0026lt;\u0026lt; self def show_specy puts \u0026#34;we are Mammals\u0026#34; end end =end protected def my_protected_method puts \u0026#34;my protected method\u0026#34; end private def my_little_secret puts \u0026#34;private methods\u0026#34; end end Person.show_specy p1 = Person.new(\u0026#34;yoyo\u0026#34;, 10) puts p1.name p1.name = \u0026#34;hello\u0026#34; # puts p1.age 對應第5點，不能直接呼叫 .age 取得 age 變數 class Teacher \u0026lt; Person def initialize(name, age, major) super(name, age) @major = major end def can_access_parent_private_method self.my_protected_method my_little_secret # 這樣可以讀取 parent private method #self.my_little_secret end end t1 = Teacher.new(\u0026#34;Mark\u0026#34;, 10, \u0026#34;English\u0026#34;) t1.hello t1.can_access_parent_private_method t1.send :my_little_secret t1.send :my_protected_method Ruby 物件比想像中複雜，尤其是支援 Meta programming，進階資料可以參考 Ruby 的繼承鍊 (1) 物件導向如何實踐\n匿名函式 block / Proc / lambda block 代表程式碼區塊，少數 Ruby 中不是物件的存在，必須依附在 function 上，透過 yeild 呼叫 block 執行，單行宣告用 {...}，多行用 do ... end Proc 是物件，不限制參數，return 時是代表當時的 context return，透過 proc.call 執行 lambda 是特殊的 Proc，會嚴格檢查參數，return 就如同一般的 function return 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 1.upto(5).map {|ele| puts ele} # 可以判斷是不是有 block 被傳入，如果有則用 yeild 呼叫執行 def hello if block_given? yield(\u0026#34;world\u0026#34;) end end hello do |x| puts \u0026#34;message from hello: #{x}\u0026#34; end # Proc / lambda 可以儲存於變數備用 my_proc = Proc.new{ |x| return puts \u0026#34;from proc #{x}\u0026#34; } my_lambda = lambda { |x| puts \u0026#34;from lambda #{x}\u0026#34; } def func(block) puts \u0026#34;before call\u0026#34; block.call(\u0026#34;func\u0026#34;) puts \u0026#34;after call\u0026#34; end func(my_lambda) # before call/n from lambda func/n after call # proc 宣告於最上層 context，所以 return 時會連帶結束整個程式 func(my_proc) # before call/n from proc func 有趣的語法 \u0026amp;:symbol 網路上有些說法是 {|x| x[:symbol]} 的縮寫，讓我們看下去\n1 2 3 4 5 6 7 8 9 10 11 12 class Person attr_reader :name def initialize(name) @name = name end end a = [ Person.new(\u0026#34;123\u0026#34;), Person.new(\u0026#34;2222\u0026#34;) ] name_list = a.map { |x| x.name } # 兩者相同 name_list = a.map(\u0026amp;:name) p name_list 參數用 \u0026amp; 開頭代表參數是以 Proc 傳入，他跟一般的參數 args是切開的 1 2 3 4 5 6 7 8 9 10 def some_method(*args, \u0026amp;block) puts \u0026#34;args: #{args.inspect}\u0026#34; puts \u0026#34;block: #{block.inspect}\u0026#34; end some_method(1,2,3, :whatever) # args: [1,2,3, :whatever] # block: nil some_method(1,2,3, \u0026amp;:whatever) # args: [1,2,3] # block: #\u0026lt;Proc:0x007fd23d010da8\u0026gt; 在 Ruby 呼叫 object method 可以用 send 的方式 1 2 3 4 5 6 7 class Person attr_reader :name def initialize(name) @name = name end end p Person.new(\u0026#34;123\u0026#34;).name == Person.new(\u0026#34;123\u0026#34;).send(:name) \u0026amp;:symbol 實際上會去呼叫 :symbol#to_proc ，而在 Symbol 中有定義 to_proc 行為，也會有人去自定義 class 中的 to_proc 方法 1 2 3 4 5 6 7 class Symbol def to_proc Proc.new do |receiver| receiver.send self end end end 綜合上述，可以拆解成\n1 2 3 4 5 6 7 a = [ Person.new(\u0026#34;123\u0026#34;), Person.new(\u0026#34;2222\u0026#34;) ] puts a.map(\u0026amp;:name) # 可以拆解成下者 puts a.map { |x| x.send(:name) } 也就是 receiver 會收到 symbol 的方法呼叫，參考自 What does map(\u0026amp;:name) mean in Ruby?\n其他優秀的資訊\nRuby 探索：Blocks 深入淺出 [Ruby] 如何理解 Ruby Block Concurrency \u0026amp; Parallelism Ruby 支援 Process ，可用於處理 CPU-Heavy issue Ruby 在 2.0 導入 Cope on Write，當 process fork 時如果 value 沒有改動則使用同一份記憶體空間，降低 fork 對於記憶體資源無謂的佔用\nRuby 支援 Thread，適用於處理 IO Event，但如果是 CPU-Heavy issue 則沒有幫助，因為 Ruby 有 GIL (Global Interpreter Lokc) 所以無法併發，一次只能執行一個 thread，這跟 Ruby VM 實作有關，如果是 JRuby 則沒有此問題 Thread releases GIL when it hits blocking I/O operations such as HTTP requests, DB queries, writing / reading from disk and even sleep\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 require \u0026#39;benchmark\u0026#39; ELE_AMOUNT = 1000 PROCESS_NUM = 2 arr = Array.new(ELE_AMOUNT) { Array.new(ELE_AMOUNT){rand(1...9)}} Benchmark.bm(10) do |bm| bm.report(\u0026#34;seq\u0026#34;) do total = arr.reduce(0) do |sum, ele| sum + ele.sum end end bm.report(\u0026#34;parallel\u0026#34;) do read, write = IO.pipe 1.upto(PROCESS_NUM).map do |i| Process.fork do p i step = (ELE_AMOUNT * 1.0 / PROCESS_NUM).ceil start_ele = step * (i-1) total = arr[start_ele, step].reduce(0) do |sum, ele| sum + ele.sum end write.puts total end end Process.wait end bm.report(\u0026#34;thread\u0026#34;) do total = 0 threads = [] 1.upto(PROCESS_NUM).map do |i| t = Thread.new do step = (ELE_AMOUNT * 1.0 / PROCESS_NUM).ceil start_ele = step * (i-1) total += arr[start_ele, step].reduce(0) do |sum, ele| sum + ele.sum end end threads \u0026lt;\u0026lt; t end threads.each(\u0026amp;:join) end end Fiber 是類似於 goroutine 的概念，更輕量的 user space thread，主要用來非同步的排程，適用於結合 Non-blocking IO，因為 Fiber 在 Context Switch 比 Thread 更為輕量\n範例來源：Introduction to Concurrency Models with Ruby. Part I 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 EventMachine.run do Fiber.new { page = http_get(\u0026#39;http://www.google.com/\u0026#39;) if page.response_header.status == 200 about = http_get(\u0026#39;https://google.ca/search?q=universe.com\u0026#39;) # ... else puts \u0026#34;Google is down\u0026#34; end }.resume end def http_get(url) current_fiber = Fiber.current http = EM::HttpRequest.new(url).get http.callback { current_fiber.resume(http) } http.errback { current_fiber.resume(http) } Fiber.yield end Ruby 3.0 導入了基於 Actor 模式的 Ractor ，真正能做到利用 Thread 達成 Parallelism，以前會需要 GIL 是為了避免 multi thread 之下的 deadlock / race condition 狀況，但是在 Ractor 中基本上 Object 都不會被共享，參考 Share Memory By Communicating 有些文章會寫 Guild，但我查 Ruby 官方文件只有看到 Ractor Do not communicate by sharing memory; instead, share memory by communicating.\n意即如果希望在多個 Thread 中共享資訊，不要透過共享記憶體來溝通，而是透過通信交換資料達到共享資料的目的 因為共享記憶體就必須處理 lock，接著就要擔心 dead lock 等問題\n如果不共享記憶體，直接將資料透過通道等方式傳遞，就不用擔心以上的問題，也可以更好的並行運算\n優良的資源參考\nRuby Concurrency and Parallelism: A Practical Tutorial Introduction to Concurrency Models with Ruby. Part I / II RubyConf Taiwan 2019 - The Journey to One Million by Samuel Williams Module Module 提供類似於 Namespace 角色，可以自定義方法與常數不用擔心與其他人衝突 Module 不能被實體化 (也就是不能被 new)，主要透過 mixin 擴充 Class 在共享實作方面，Ruby 一個 Class 只能繼承一個 Parent，但是 Module 可以 mixin 多個，在沒有直接關係的情況下想要跨多個 Class 共享某些特定的方法，Module 是不錯的選擇 使用 Mixin 要小心多個 Module 可能會有不預期的互相干擾，例如修改同一個 instance 變數，有狀態要紀錄記得取一個比較特殊的名稱 如果 Module 中有 Class 宣告，要指定該 Class 可以用雙冒號 :: 連接例如 Module Name::Class Name 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ##### calculator.rb module Greeting def sayhi puts \u0026#34;hello, #{@name}\u0026#34; end class Hi end end ###### main.rb require_relative \u0026#39;./calculator\u0026#39; class Person \u0026lt; Greeting::Hi # 透過 include 達到 mixin 效果 include Greeting def initialize(name) @name = name end end p = Person.new(\u0026#34;yoyo\u0026#34;) p.sayhi Module 除了可以被 include 外，還有 extend 跟 prepend，請參考 Ruby 的繼承鍊 (2) — Module 的 include、prepend 和 extend\n程式碼拆分檔案 把全部程式碼塞在同一個檔案十分的可怕，透過適時的拆分可以讓程式碼更好維護，在 Ruby 中如果要 include 其他檔案的宣告，可以用 require_relative '檔案本身的相對路徑'的方式\n目前看起來只有常數、Module、Class 會被自動 export，還再找到相關的文件說明\nrequire 總共有幾種\nrequire： 如果是相對路徑，則根據 $LOCAL_PAHT 設定去找對應的 library，通常是用來找外部相依或是 gem 如果是絕對路徑，則直接載入對應檔案 require_local:\n透過檔案的相對路徑找到檔案，主要是用於在自己專案中的其他檔案 Testing Ruby 並沒有內建的 Test Framework，評估後選用 RSpec，提供 TDD/BDD Style 語法，透過 describe / it 組合測試案例\n慣例是專案根目錄建立 spec 目錄，待測項目對應檔案名加上 _spec 結尾 Unit Test 支援 before / after / around(before + each)，執行頻率分成 each / all / suite suite 是在整個 test file 只會跑一次，all 則是每個 describe 都會執行一次\n如果是有變數要在每一次 test case 前執行，可以用 before(:each)，或是用 let(變數名稱){回傳值} 以下是一個簡單的運費計算\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 require(\u0026#39;rspec\u0026#39;) require_relative \u0026#39;../calculator\u0026#39; describe Calculator do let(:calculator) { Calculator.new } it \u0026#39;small package should get $100 fee\u0026#39; do expect(calculator.fee(1, 5)).to eq(100) end it \u0026#39;medium size should get $500 fee\u0026#39; do expect(calculator.fee(10, 5)).to eq(500) end before do @calculator = Calculator.new end around do |t| p \u0026#34;before each test\u0026#34; t.run p \u0026#34;after each test\u0026#34; end it \u0026#39;large size should get $700 fee\u0026#39; do expect(@calculator.fee(100, 10)).to eq(750) end end Stub / Mock / Spy RSpec 提供 mock 方法叫做 double (出自於 stunt double 演員替身)\ndouble 可以憑空產生物件，可以自定義 function 與回傳值 double 可以只覆寫特定方法的回傳值 allow(instance).to receive(method).and_return(value) spy 不改方法的實作，只確認有沒有被呼叫，以及確認傳入的參數以及呼叫順序是否如預期 每一個 test case 後都會被自動 restore 因為是動態語言，所以要 \u0026ldquo;double(string)\u0026rdquo; 等方法都是可以的 運費計算加上一個 VIP 檢查\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class VIP_Service def is_vip? raise \u0026#34;do some query\u0026#34; end end describe let(:calculator) do vip_service = VIP_Service.new # 這裡透過 mock 動態改變 allow(vip_service).to receive(:is_vip?).and_return(false) Calculator.new(vip_service) end end spy 的案例請看官方文件範例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 require \u0026#39;rspec\u0026#39; class Invitation def deliver(email) p email end end describe \u0026#34;Invitation\u0026#34; do let(:invitation) { spy(\u0026#34;invitation\u0026#34;) } before do invitation.deliver(\u0026#34;foo@example.com\u0026#34;) invitation.deliver(\u0026#34;bar@example.com\u0026#34;) end it \u0026#34;passes when a count constraint is satisfied\u0026#34; do # 透過 have_recieved 看方法有沒有被呼叫 expect(invitation).to have_received(:deliver).twice end it \u0026#34;passes when an order constraint is satisifed\u0026#34; do # 加上 with 檢查方法呼叫時傳入的參數 # 加上 ordered 代表要按照此順序呼叫 expect(invitation).to have_received(:deliver).with(\u0026#34;foo@example.com\u0026#34;).ordered expect(invitation).to have_received(:deliver).with(\u0026#34;bar@example.com\u0026#34;).ordered end end HTTP Request \u0026amp; API Server Ruby 並沒有提供 http server 的封裝，只有 tcp server，所以我們選用 Rack 這一套 library，他被 Ruby 生態圈廣泛採用的底層框架，如 RoR 也是\nRack provides a minimal, modular, and adaptable interface for developing web applications in Ruby.\nAPI Server 新增 server.ru，透過 $rackup server.ru 啟動\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 require \u0026#39;rack\u0026#39; require \u0026#39;json\u0026#39; rack_proc = lambda { |env| req = Rack::Request.new(env) case req.request_method + \u0026#34;:\u0026#34; + req.path_info when \u0026#34;GET:/hello\u0026#34; return [200, { \u0026#34;content-type\u0026#34;=\u0026gt; \u0026#34;application/json\u0026#34; }, [ { \u0026#34;hello\u0026#34; =\u0026gt; req.params[\u0026#34;name\u0026#34;] }.to_json ]] when \u0026#34;POST:/world\u0026#34; body = JSON.parse req.body.gets return [200, { \u0026#34;content-type\u0026#34;=\u0026gt; \u0026#34;application/json\u0026#34; }, [ { \u0026#34;hello\u0026#34; =\u0026gt; body[\u0026#34;name\u0026#34;]}.to_json ]] else return [406, { \u0026#34;content-type\u0026#34;=\u0026gt; \u0026#34;html/txt\u0026#34; }, [\u0026#39;not_implemented_yet\u0026#39;]] end } run rack_proc HTTP Request 參考 5 ways to make HTTP requests in Ruby\n1 2 3 4 5 6 7 require \u0026#34;http\u0026#34; response = HTTP.get(\u0026#34;http://localhost:9292/hello\u0026#34;, :params =\u0026gt; {:name =\u0026gt; \u0026#34;world\u0026#34;}) p response.parse response = HTTP.post(\u0026#34;http://localhost:9292/world\u0026#34;, :body =\u0026gt; {:name =\u0026gt; \u0026#34;jojo\u0026#34; }.to_json) p response.parse 結語 寫過最長的文章，整理了這一個禮拜上手 Ruby 的過程，Ruby 實際上還有非常多的 黑魔法，跟新同事們一起上手的過程不斷發現驚喜(恐?!)\n必須坦白說上手到現在沒有很愛 Ruby，因為太自由了，又有太多關鍵字跟寫法，可能要在一段時間熟悉，之後再來慢慢補充\n","date":"2021-04-02T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-04-02-%E8%B7%A8%E7%A8%8B%E5%BC%8F%E8%AA%9E%E8%A8%80%E4%B8%8A%E6%89%8Bruby-%E5%9F%BA%E7%A4%8E%E6%95%99%E5%AD%B8/","title":"【跨程式語言上手】Ruby 基礎教學"},{"content":"上過 91 老師的 TDD 後，開始注重程式語言支援的測試框架，編寫測試代碼與寫出容易寫測試的代碼是很重要的一件事，好測試的代碼通常好維護，因為通常代表有更低的耦合性、物件依賴關係明確等，說是「通常」也代表不是這麼絕對；但反之 不容易寫測試的代碼往往都是有奇怪 smell 的\n關於測試案例的種類請參考 91 老師的 Unit Test - Stub, Mock, Fake 簡介\n以下將分享如何在 Golang 中編寫\n單元測試 如何 Stub/Mock 外部相依 如何針對 http handler 做 http request 假請求檢查 自己開始真正寫 Golang 也是這幾個禮拜，有一些命名、寫法不正確，煩請指教，但針對測試的本身應該是沒什麼問題的\n目前採用 Ginkgo + gomock + httptest 組合的測試工具\n以下我們將寫一個簡單的匯率兌換表，用戶輸入既有的幣別 / 欲兌換的幣別 / 數量，Server 回傳兌換後的數量，程式碼於此 golang-exchange-currency\n以下是程式碼結構\nmain.go: 啟動 http server src/exchange_currency_model.go: 模擬去資料庫讀取匯率兌換表 src/currency_exchange_handler.go: http handler，處理 request 與 response 單元測試 首先要決定測試框架，這部分評估過 原生的testing、Testify，最後選擇了 Ginkgo，最大原因是熟悉原本 Nodejs的 Decribe / It 組織 test case 的方式，以及有方便的 BeforeEach 可以抽出重複測試行為的部分，例如在每個測試案例之前都先 new 好 object\n這些在 testing / Testify 都要額外的功夫處理\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import ( . \u0026#34;github.com/onsi/ginkgo\u0026#34; . \u0026#34;github.com/onsi/gomega\u0026#34; ) var _ = Describe(\u0026#34;currency exchange\u0026#34;, func() { c := \u0026amp;CurrencyExchangeHandler{} .... BeforeEach(func() { c = NewCurrencyExchangeHandler(e) }) It(\u0026#34;should get 0 if amount is 0\u0026#34;, func(done Done) { .... Expect(c.Exchange(\u0026#34;US\u0026#34;, \u0026#34;TW\u0026#34;, 0)).To(Equal(0)) close(done) }) .... } Stub/Mock Stub 專注於測試物件本身，只是把外部相依的方法塞一個設定值回傳；\nMock 則延伸 Stub，除了塞回傳值外，而外檢查被呼叫物件的傳入值 / 呼叫次數 / 狀態改變等非測試物件本身的狀態\n在 Golang 中，使用 gomock 真的是超級方便，可以直接針對檔案產出對應的 mock 檔 exchange_price_model_mock.go，這邊要注意 mock 是針對 interface 產生，所以如果你的檔案中沒有 interface，mock 檔出來就會是空的\n所以我在 exchange_price_model.go 中有定義\n1 2 3 type IExchangePriceModel interface { GetExchangeRate(string, string, chan\u0026lt;- ExchangeRateResult) } 接著執行 mockgen -source={要 mock 的檔案} -destination={輸出位置} -package={package 名稱}，例如 $ mockgen -source=exchange_price_model.go -destination=exchange_price_model_mock.go -package=src，mockgen 是 gomock 用來產生 mock 檔案的 binary 執行工具\n之後測試案例採用 NewMockIExchangePriceModel 這個由 mockgen 產生的 struct 即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 var _ = Describe(\u0026#34;currency exchange\u0026#34;, func() { c := \u0026amp;CurrencyExchangeHandler{} var ( mockCtrl *gomock.Controller e *MockIExchangePriceModel ) BeforeEach(func() { mockCtrl = gomock.NewController(GinkgoT()) e = NewMockIExchangePriceModel(mockCtrl) c = NewCurrencyExchangeHandler(e) }) It(\u0026#34;should get 0 if amount is 0\u0026#34;, func(done Done) { e.EXPECT().GetExchangeRate(\u0026#34;US\u0026#34;, \u0026#34;TW\u0026#34;, gomock.Any()).Do(func(from string, to string, ch chan\u0026lt;- ExchangeRateResult) { ch \u0026lt;- ExchangeRateResult{ IsExists: true, ExchangeRate: 40, } }) Expect(c.Exchange(\u0026#34;US\u0026#34;, \u0026#34;TW\u0026#34;, 0)).To(Equal(0)) close(done) .... }) } 編寫 mock 的方式如下\n1 2 3 e.EXPECT().Method(\u0026#34;預期 method 要收到的參數\u0026#34;).Do(func(\u0026#34;實際執行時收到的參數\u0026#34;) { 做任何造假 }) 等於是寫一次連預期輸入、造假輸出都一並做完，如果要方便可以 .Return() 直接寫回傳內容，但因為涉及 channel 要傳遞資料，所以我選擇 .Do() 並塞入造假的資料回傳 channel\n如果不在意預期輸入，可以都用 gomock.Any() 跳過檢查\n如何造假 Time.Now 等系統相依的函式 搜尋了一下這類問題，建議是把有外部相依都抽到另一個 Object 去，然後透過依賴注入的方式傳進去，才能夠造假\n例如\n1 2 3 4 5 6 7 8 9 type ObjectA {} func (a *ObjectA) MethodA(){ a.MethodB() } func (a *ObjectA) MethodB(){ return Time.Now() } 這樣是無法測試的，要拆解成\n1 2 3 4 5 6 7 8 9 10 11 interface IObjectB { MethodB func() } type ObjectA { ObjB IObjectB } func (a *ObjectA) MethodA(){ a.ObjB.MethodB() } 在使用 Interface 替換過程，要注意 *Type 跟 Type 的差異，如果發現以下錯誤訊息請參考 X does not implement Y (… method has a pointer receiver)\n從問答中回去文件看，可以注意到以下內容 Method sets ¶\n1 The method set of any other type T consists of all methods declared with receiver type T. The method set of the corresponding pointer type *T is the set of all methods declared with receiver *T or T (that is, it also contains the method set of T) 這一段也就是說\n如果 method 宣告的 reciever 是 non pointer type func (t T) method，則 T / *T 都有包含此 method 但如果 method 宣告的 reciever 是 pointer type，則只有 *T 包含此 method 延伸至 embedded struct\n1 2 3 - If S contains an embedded field T, the method sets of S and *S both include promoted methods with receiver T. The method set of *S also includes promoted methods with receiver *T. - If S contains an embedded field *T, the method sets of S and *S both include promoted methods with receiver T or *T. 如果是\nS 是 non pointer，且 T 也是 non pointer，則包含了 T non pointer type methods S 是 non pointer + T 是 pointer / 只要 S 是 pointer type，則包含了 T non pointer / pointer type methods 詳見程式碼，我把 struct 有的 method 都列出來，可以清楚看到以上的規則 Go playground\n另外抽出依賴再注入，如果忘記初始化會有記憶體存取失敗的錯誤 http: panic serving runtime error: invalid memory address or nil pointer dereference，看到錯誤記得去檢查\n針對 HTTP Handler 做檢查 透過單元測試與 Stub/Mock，可以檢查完商業邏輯的部份，但如果想更確定 server 是否有正確處理 http request，包含是否回傳預期的錯誤結果，可以再進一步針對 http handler 做測試\n這邊採用 core library 包含 net/http/httptest 測試，完整教學可以參考 Testing Your (HTTP) Handlers in Go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 It(\u0026#34;test ServeHttp integration\u0026#34;, func(done Done) { e.EXPECT().GetExchangeRate(gomock.Any(), gomock.Any(), gomock.Any()).Do(func(from string, to string, ch chan\u0026lt;- ExchangeRateResult) { ch \u0026lt;- ExchangeRateResult{ IsExists: false, ExchangeRate: 0, } close(ch) }) req, _ := http.NewRequest(\u0026#34;GET\u0026#34;, \u0026#34;/exchange-currency\u0026#34;, nil) query := req.URL.Query() query.Add(\u0026#34;from\u0026#34;, \u0026#34;US\u0026#34;) query.Add(\u0026#34;to\u0026#34;, \u0026#34;TW\u0026#34;) query.Add(\u0026#34;amount\u0026#34;, \u0026#34;10\u0026#34;) req.URL.RawQuery = query.Encode() rr := httptest.NewRecorder() e := \u0026amp;ExchangePriceModel{} c := \u0026amp;CurrencyExchangeHandler{ E: e, } handler := http.HandlerFunc(c.ServeHTTP) handler.ServeHTTP(rr, req) Expect(rr.Code).To(Equal(200)) var body struct { Amount int } _ = json.Unmarshal(rr.Body.Bytes(), \u0026amp;body) Expect(body.Amount).To(Equal(300)) close(done) }) 以上基本就是造假 / 初始化 handler / 初始化 http request / 透過 handler.ServeHTTP(rr, req) 模擬 http handler 處理過程 / 檢查 response\n基本上 Context / Cookie 等都可以處理，處理起來相當方便\n結語 從動態語言過來，最不習慣的就是要一直去想物件之間的相依，包含要處理 mock 時要拆出 interface 與外部物件，而不能針對某一個 object 的某一個 method 造假\n但整體上，Golang 的測試算方便且好上手，~找不到偷懶不寫測試的理由了~\n","date":"2021-03-18T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-03-18-golang-test/","title":"Golang Test - 單元測試、Mock與http handler 測試"},{"content":" 我覺得這是一本寫給熱愛產品而非技術本位的工程師\n工作了一段時間，歷經公司管理制度與文化的轉變，開始在思考怎麼樣的環境跟團隊才是適合自己，那完美的團隊又應該是怎樣的型態？ 期間看過了 Netflix 體系的《給力》/《一千零一個點子之後》/《零規則》，強調人才密度 / 開放 / 下放權力等方式，但這些都比較是以人資 / 管理層角度出發 而《Clean Coder》是由專業工程師所撰寫，如何當一位堂堂正正的程式設計師，裡面糾正了自己很多壞習慣，但比較像教條/守則一般，少了點趣味與團隊的描述\n而這本《創意競擇》，由曾經是 Apple 首席軟體工程師，參與過 Safari / iPhone 第一代鍵盤設計 / iPad 鍵盤設計，描述每一個專案經歷的過程與挫折，並收斂出作者認為為什麼 Apple 可以設計出頂尖產品的心法，並描繪出專業的產品團隊在決策/執行的過程，每一步的反思都發人深省，以下是整理過後的自我反思\n演示(Demo) -\u0026gt; 蒐集反饋 -\u0026gt; 下一次演示 2005 年 Apple 啟動了 Purple 計畫，也就是 iPhone 的設計，在 2005 年 9月，整個工程團隊並召集要解決最棘手的問題 鍵盤，不同於黑莓機的實體鍵盤，iPhone 打算用多點觸控頻幕當做軟體鍵盤，但是因為第一代 iPhone 螢幕太小，所以鍵盤設計就至關重要，這是 iPhone 計畫遇到的一大難題\n主管在測試第一代鍵盤時，發現連自己的姓名都打不出來，於是召集所有人，要大家停下手邊任務，全力攻克鍵盤問題，大家就各自發想，提出自己的解決方案\n作者想到，如果把多個英文字母併入一個輸入格，透過字典自動校正猜出用戶要輸入的單字，就可以解決面積小 + 提升速度的問題，後來在展示過程中也成功擊敗其他提案，他也就成為 iPhone 主要負責鍵盤的程式設計師\n但是這樣的鍵盤問題很多，例如只透過字典校正，那如果用戶輸入「Arrrr!」就打不出來了，所以就不斷的調整 -\u0026gt; 演示 -\u0026gt; 回饋，每個成員手上都帶著原型機，形影不離不斷的測試並給予反饋\n這聽起來是 common sense，但是演示的細節才是關鍵\n給予直接的回饋 演示的時候是不是能抓到重點，把力氣花在正確的位置上，更重要的是「有沒有人可以給出果斷的反饋」\n作者提到賈伯斯可以的話會盡可能參與演示，並給予非常直接的回饋，例如說第一代 iPad 鍵盤其實有兩個鍵盤模式可以切換，但賈伯斯玩一玩就問作者「你覺得是不是只要一個就好？你喜歡哪一個？」\n之後就拍板決定，只留一個簡化模式的鍵盤\n工作久了覺得有一個果斷的決策至關重要，很多時候大家在會議上都不敢給予直接的反饋或決定，可以聽出大家對於背負責任的恐懼與排斥，或是猜不透掌權者的心思，這來回之間留下很多朦朧的空間，十分的可惜\n演示很重要 另一點是工程師對於演示的心態與技巧，年前剛好與 CEO One on One，他提及工程師普遍不重視演示，不懂得如何用正確的方式讓大家 (包含 CEO 本人)了解技術的重要性，例如敝司有持續在研究低光源錄影的優化演算法，但是 Demo 時選擇在明亮的會議室，CEO 表示他很納悶為什麼不找一個合適的場景更凸顯技術上的突破呢？\n站在科技與人文的交界點 在書中有提及品味，例如鍵盤就應該要讓用戶順暢的打出他所想要的字，所以作者不斷的優化自動校正功能，同時保留用戶輸入冷僻字的可能\n品味聽起來虛無縹緲，作者的定義是\n培養靈巧的判斷力，找到令人愉悅又完善的平衡點\n這仰賴團隊中設計師與工程師不斷的溝通，以及管理者很明確的表達目標，例如開發 Safari 的時空背景是要取代原本 Mac OS 內建的 IE，那時候賈伯斯給予非常明確的指示「速度」，要用戶放棄 IE 跳槽到 Safari 速度就是最重要的用戶體驗指標\n另外作者對於 A/B Test 提出質疑，最有名的就是 Google 測試 41 種藍色，從用戶給予的數據回饋作為依硅是「最顧客導向的做法」，但是作者表示這不會是 Apple 做事的態度，或許他們會採用數技分析，但他們有專業的設計師在為所有的設計把關，包含像設計師提出觸控螢幕應該要有長按拖動的設計，就像是在手按著紙在桌面拖動的感覺，這樣才有更沈浸的體驗；\n滑動捲軸時持續滑動會加速，觸底會有小小的反彈回饋，都是 Apple 設計師所設計的\n敝司近年也在導入數據驅動設計，但自己也在反思需求設計是不是真的完美？從用戶的數據不斷去打磨就能得到最棒的產品？\n或許還是要回歸需求設計的本身，這樣的提案是不是有「品味」，才有後續的 AB Test 的價值\n看到這一段也頗有感觸，在鍵盤專案中有專職的設計師，而作者身為軟體工程師卻不單單把自己擺在實作者的角色，而是把自己當作用戶不斷去優化鍵盤設計，在書中描述了五六代鍵盤的優化過程，像是從原本的多字一鍵改成一字一鍵，然後再改成 QWERTY 鍵盤，自動校正也從單純的字典比對變成輸入的位移向量差，可以看出作者想要把最棒的用戶體驗帶給所有的人，包含他自己\n最困難的問題 因為 Safari 專案的成功，作者的主管希望作者把瀏覽器包成套件 Webkit 嵌入 Email 當中做文字編輯器，作者花了很多時間在解決滑鼠游標位置的問題，因為這涉及到文字排版 / 換行跳字等邏輯，遠比想像中複雜得多 作者在書中有描述他遇到的難題，以及他曾經嘗試過的解法，花了好幾週的時間都無法突破，最後向主管尋求幫助，從其他部門找來有經驗的工程師，最後就一起解決了問題\n作者統整了幾個點\n問問題之前必須自己先努力過 身為程式設計師必須拋開自己的尊嚴，有問題就要有誠意地去尋求協助 聽起來很一般，但仔細回想工作的經歷，可謂文人相輕，很多時候其實同事之間很少有切磋，也不知道是大家在藏招還是害羞，覺得實在很可惜，尤其是事後回想彼此都踩過一樣的坑，如果早點分享或是互相請教，就能避免這無謂的浪費\n結語 自己在職涯發展的過程，也不斷思索自己的定位，一方面希望自己有足夠的技術實力，能夠挑公司而不是讓公司挑我；另一方面也希望自己能夠找到熱愛的產品，把自己的技術能力變成影響力\n以前會覺得兩者有點衝突，畢竟產品開發有時候需要的不見得是技術能力，而是反覆的溝通以及更深入的理解用戶需求，投入產品越多會不會讓技術發展上的時間縮短 ?!\n羅胖說過要做到 U盤化生存「自带信息，不装系统，随时插拔，自由协作。」\n但慢慢地開始體會到，兩者有時候是相輔相成的，熱情這種東西是無形且容易被忽視的價值，如果上班對於產品有熱情，這可以延燒到對於技術專精的執著，讓解決問題可以從更根本的角度而不是嫌麻煩的負面心態去面對\n講得有點籠統，自己可能也還沒想得很清楚，但是從這本書中看到另一條我覺得值得效法的職涯\n","date":"2021-03-07T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-03-07-%E5%89%B5%E6%84%8F%E7%AB%B6%E6%93%87%E5%BF%83%E5%BE%97%E5%88%86%E4%BA%AB/","title":"《創意競擇》心得分享"},{"content":"在一兩年前 Golang 很火紅時有學了一下，但當時沒有深入的理解 Golang 相對於其他語言的特色與魅力，只停留在表層的語法學習，時隔多年因為換工作的需求，重學 Golang 發現有蠻多有趣的地方，拿來與 Nodejs 相比有許多類似但不同的實作差異，尤其在非同步這一塊，特此筆記並分享如何從 Nodejs 跳槽到 Golang\n本篇將著重於介紹 Golang 上手的教學資源，以及對比 Nodejs (Javascript)，Golang 的特色在什麼地方\nGolang 與 Nodejs 異同之處 - 以非同步為例 Golang 與 Nodejs 在非同步設計上有些雷同之處，相較於傳統的每一個 IO 事件就開一個 thread 讓 OS 去排程，Nodejs 與 Golang 都盡可能減少 kernel thread 的產生，而是透過 Non blocking system call 或是 user thread 與 scheduler 方式，降低 OS Context Switch 就能更有效率使用 kernel thread\nNodejs 內部非同步處理 透過 runtime 底層 libuv 呼叫 non-blocking system call，向 system 註冊有興趣的事件並加入 event loop ，event loop 中又細分成多個 phase 檢查不同的任務如 timer / io / check 等，等到 main thread 執行完後檢查 event loop 上 IO 事件是否完成，如果完成則觸發 callback 到 main thread 執行\n但需要注意 Nodejs core library 有些是沒有 non blocking system call，例如 fs / crypto / dns.lookup 查詢，這些是從 Worker Pool 另外開 thread 執行，會受限於環境變數 UV_THREADPOOL_SIZE 控制整體 kernel thread 數量，所以再次提醒 Nodejs runtime 是 multi thread，只是 js 跟 event loop callback 都跑在同一個 main thread 上，具體可以參考\nThe Node.js Event Loop: Not So Single Threaded Node\u0026rsquo;s Event Loop From the Inside Out by Sam Roberts, IBM Basics of libuv Instead, the application can request the operating system to watch the socket and put an event notification in the queue. The application can inspect the events at its convenience\n如果是 core library 沒有支援的 non blocking 任務，就必須自己透過 worker_threads / child_process / cluster 等方式才不會 block main thread，這也是新手搞混的問題 是不是用 Promise 包成非同步就不會 blocking (X)\nGolang 的 goroutine 與 scheduler 內容摘錄自這幾篇優良的內容：\nGopherCon 2018: Kavya Joshi - The Scheduler Saga / Go: Goroutine, OS Thread and CPU Management / Scheduling In Go : Part II - Go Scheduler\n前情提要，processor 只能執行 kernel thread 任務，每個 kernel thread 在記憶體佔用與 context switcth 花費都不小 (8KB / 1 ms)\n而 Golang 本身建立 user thread (goroutine)，開銷相對只要 (2KB/10 ns) 透過 Schedule 安排 user thread -\u0026gt; kernel thread -\u0026gt; processor 實際運行程式，盡可能讓 kernel thread 持續保持 running 減少 context switch\nGolang 本身有 Scheduler 負責排程，透過 go func()啟動 goroutine (user thread)，此 user thread 將由 Scheduler 排程\n(來自參考文件)\nScheduler 預設為啟動一個 Processor P1，這裡會對應到硬體上可獨立執行 thread 的運算單元 不一定等於 CPU core 數量，因為像 intel Hyper-Threading 功能，一個 physical core 可運行兩個 thread\nProcessor P1 上運行一個 Kernel Thread M1，並將 goroutine G1 排程到 M1 中執行 如果有新的 goroutine G2 誕生，且目前的 Processor P1還在跑，則建立新的 kernel thread M2與 Processor P2，並執行 goroutine G2 如果 kernel thread 啟動數量達到上限 GOMAXPROCS，則會放到 FIFO queue 當中，簡稱 runq，每個 Processor 都有對應自己的 local runq Processor 如果把 local run queue 都處理完，可以去偷其他 processor 的 runq (work stealing)，達到工作 balance 除了 local runq，還有一個 global runq 放被中斷的長時間佔用 goroutine，kernel thread 會用比較低的頻率去執行，還有像垃圾回收等任務 1 2 3 4 5 6 7 8 runtime.schedule() { // only 1/61 of the time, check the global runnable queue for a G. // if not found, check the local queue. // if not found, // try to steal from other Ps. // if not, check the global runnable queue. // if not found, poll network. } 遇到 system call，會有兩種反應 如果是 blocking system call，則該 kernel thread 會暫停 (parking) 並移出 processor，把 process 讓給其他人 (handoff)，此時 thread 不會佔用總體上限 如果是 non blocking system call 例如 network 相關，則不需要移出 thread，而是把 goroutine 放到 network poll，processor 會在有空的時候去 network poll 找出完成 system call且 runnable 的 goroutine 這樣每個 process 都盡可能綁定一個 kernel thread，且此 kernel thread 就持續在 running 狀態而沒有切換，透過 Go Scheduler 替換 user thread 排程工作\nScheduler 本身會在以下情況處理排程\ngo keyword 垃圾回收：垃圾回收有獨立的 goroutine system call sync 相關呼叫: atomic / mutex / channel 相關操作，會導致 goroutine 堵塞 goroutine 可以針對 function level 啟動併發，也有豐富的同步語法，例如 sync.Mutex 保護 critical section / sync.WaitGroup 等待 goroutine 完成 / channel 在 goroutine 當中傳遞資料與控制執行\n範例一：透過 http request 讀取 users list 並再次透過 http request 取得 10 位 user 的詳細資料，算最後的性別加總 範例只是要稍微品嚐一下兩個語法的差異，錯誤處理等就先不要太在意\n這部分寫起來用 Nodejs 就蠻方便的\n範例二：計算 1000 * 1000 數字矩陣加總，併發四個 thread 執行最後加總 Nodejs 在 child_process 或是 worker thread 我自己都覺得有點不太方便，不能針對某一個 function 起新的 thread，傳遞資料上也不是太方便，不如 Golang 直接 go func() 搭配 channel 來的簡便\n接著回過頭來看，分享從 Nodejs 跳槽到 Golang 的學習方式\nGolang 教學資源推薦 影片付費資源：Go Core Language ，自己本身蠻喜歡影片式教學，可以快速過一遍，Pluralsight 的課程品質還不錯，而且還有 Skill 可以測試自己的能力，把上面 Go 核心課程看完大概就花個 5個小時左右，覺得入門來說頗划算\n為什麼要用 Golang 除了本身是靜態強型別的編譯式語言，Golang 相比於 Nodejs 語言本身有幾大特色讓我十分喜歡\n1. 非常工程導向/簡潔： Golang 從一開始推出就是為了解決 Google 所遇到的大型軟體系統設計難題，所以從一開始設計就非常工程、團隊合作導向，例如\n只有 for loop 沒有 while / do while 等，讓寫法有統一的方式，不會每個人都有各自的實作 package 中大寫代表 public / 小寫代表 private test function 必須是以 Test 開頭\nGolang 只有 25 個 keywords，且在許多地方都有明確的限制，而不是給予空泛的自由，這讓團隊有明確的 coding style 可以遵守 2. 語言核心包含常用的功能，例如 CLI / Testing 在 Nodejs 中，我們常需要各種 npm package 完成任務，小至 http request 都要安裝 node-fetch / axios / request 等，因為核心 library 提供的 api 不好用； 但是 Golang 中沒有這樣的問題，如果是要寫 CLI 工具，處理參數 / 產生 -help 文件等核心 library 都處理妥當；http request 用原生的 net/http 就很方便，甚至 api server 也都可以不用社群的 framework 就能夠快速實作\n3. 跨平台編譯出單一可執行的 Binary 檔 雖然有 Docker 提供跨平台部署的一致性保證，但是 Golang 可以直接編譯出對應平台可執行的 Binary 檔還是很方便，在寫 Dockerfile 也不用擔心太多環境設定是否正確 / 安裝過多套件是否有安全漏洞等等\n4. 官方文件齊全且詳盡 Golang 官網中的 Frequently Asked Questions (FAQ) 與 The Go Blog就有解答我許多疑惑，包含\n為什麼 Golang 要把型別宣告放在後面 Why are declarations backwards? 因為用口語念程式碼更直覺表達出意圖\n為什麼要用 Go Two recent Go articles\n因為目前熱門的語言都是在網路/多核心時代前的產物，如C/C++/Java，像是thread 功能都是在語言誕生很久之後才設計的；另外現今系統的規模 / 協作人員等數量都完全不同，所以需要有新型態的語言來支援；\nGo 具備快速編譯/跨平台支援/垃圾回收機制/goroutine 併發設計，讓設計現代軟體更加簡便\n官方就有豐富的資源可以讓開發者更深度的理解 Golang 設計精髓與奧妙\n結語 關於底層的 runtime 種種還有許多未解之謎，就等未來慢慢填坑，看著不同語言的發展與設計理念，覺得實在是有趣\n回歸工作，如果是一些簡單的任務，寫 Nodejs 還是蠻順手的；只是在大型軟體開發上，Golang 的設計理念 / 語言特性都讓他成為熱門的選擇，不愧是 Docker / K8s 等工具選擇的開發語言\n","date":"2021-03-07T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-03-07-%E5%BE%9E-nodejs-%E5%88%B0-golang-concurrency-%E5%AF%A6%E4%BD%9C%E6%AF%94%E8%BC%83/","title":"從 Nodejs 到 Golang: Concurrency 實作比較"},{"content":"朋友於 2018 年就去上了 91 老師的課，不斷的大推極速開發與 TDD，當時想說一堂課一兩萬也太驚人，後來朋友直接現場 Demo，就此打破我的世界觀，沒有想到寫程式可以流暢成這種地步，報名幾次都沒有搶到票，最後報到 2021/01 的課程，如果想要報名，記得要再開放第一天就搶票喔！\n絕對物超所值\n經過四週的練習，目前 Tennis Kata 使用 WebStorm + ES6 開發，可以在 15 分鐘完成，跟那些壓在10分鐘以內的神人同學相比還差很遠，但想說還是可以用自己的角度分享練習的過程\n前情提要 91 部落格【極速開發+】 中明確提到\n我認同「時間不夠」是個問題，然而卻很少人去改善或解決這個問題。這門課，將讓各位學到，如何建立自我刻意練習的模型，將所有工具的整合起來發揮最大綜效，透過正確的開發方式與順序，讓你寫代碼時能行雲流水，並且兼顧設計、品質與生產力。\n一開始聽朋友分享，會覺得熟悉工具使用好像很 low，身為工程師應該更專注於演算法、架構層面，但殊不知自己一天的工作時間都浪費在無謂的鍵盤按鍵與滑鼠移動\n上課的第一個課程是，兩兩一組，一人寫 Tennis Kata，另一個人紀錄寫程式的人浪費多少無謂的方向鍵，或是在怎樣的場景使用滑鼠，這無關乎熟不熟悉 Tennis Kata，而是對於寫程式這項技藝是否簡潔到沒有多餘的動作\n在寫測試案例，我們是否每次都要重新打\n1 2 3 it(\u0026#39;....\u0026#39;, ()=\u0026gt;{ expect(...).toBe(...) }) 我們眼睛盯著程式碼第 100 行的 name 這個詞，是否能夠很精準又快速跳到單字上，並重新命名為 firstName 然後套用到所有的引用上？\n上完課最震撼的是，自己從沒有抽絲剝繭「寫程式」這件事，而明明有好的工具 / 好的方法，為什麼不用呢？！\n如何練習 上完課 91 笑著說，每天只練 30 分鐘就好，因為多練也可能是重複錯誤的動作，遵守老師的吩咐，我固定每天早上練習 30 分鐘，雖然有時候會超過，但盡量不要讓自己練習到心煩，避免容易中斷習慣\n第一週練習 第一週算是最痛苦，但好在也是剛上完課能量最強的時候，focus 在 tune IDE 上，從原本的 VSCode 轉移至 Webstorm 的陣痛期，主要調整\nNodejs 環境設定：因為想要 ESM 所以要打開設定參數 (Nodejs v14 以前) 排版：幫忙補上；等 / 去除多餘的斷行 / 換字體等等 調整 .ideavimrm\n這部分很花時間，但很建議自己掃過整個檔案，把多餘的指令移除，並確保自己大概知道每個指令的含義，尤其是 zr 系列 1 2 3 4 5 zri: inline zrp: 抽參數 zrm: 抽方法 zrf: 抽成 field zrn: 我自己加的，refactor element 像是用 Mac 開發，要使用 F1~F12 都很痛苦，我就去查 :actionlist 找出 idealvim 支援的 action，或是查這一分 IdeaVim actionlist\n雖然設定 keymap 也可以，但想說能夠用一套就全部用一套，所以切換檔案我也重新綁過\n第一週只做到 tune ide，並練習到 \u0026rsquo;thirty love\u0026rsquo;，並熟悉 zr 系列的快捷鍵\n第二週 接下來挑戰寫完整個 tennis，速度大概在 22~25 分鐘，每天固定先看一次老師的影片，注意自己很卡的地方老師怎麼寫，例如補 \u0026rsquo;this.\u0026rsquo; / 補 ; 補 , 等等，慢慢修掉一些壞習慣，盡可能每次練習只專注在三個點上面，也慢慢更熟悉使用 j / k / w / b 移動，時時提醒自己想不出來也不可以自暴自棄用方向鍵跟滑鼠，停下來看老師的影片\n第三週 自己練習速度卡在 22 分鐘上下，發現是後半段太不熟練，決定專攻後半段，用 git 保存進度，每天 reset 反覆練習，一樣是先看老師的影片配早餐，接著用便條紙寫下要改進的三個點，如果遇到自己很卡的地方，直接修改 .ideavimrc\n這一週速度的進步來自\n更多採用 Acejump\n一開始下意識都是用 j/k/w/b 去移動，開始有意識練習用 Acejump，這邊也很感謝同梯同學在群組中不斷討論最佳寫法 練習 vim surround 優化 Math.abs 編寫\n可以參考這份 gist vim-surround使用指南.MD，如果要針對已選取的部分要用 S 加上對稱符號 取消 Webstorm 抽參數時補上預設\n假設以下要把 \u0026ldquo;joey\u0026rdquo; 抽成參數，我的預設會是帶入預設值，這樣會讓原本在初始化 Tennis 時不會自動補上 new Tennis(\u0026ldquo;joey\u0026rdquo;) 1 2 3 4 5 6 7 constructor(){ this.firstPlayerName = \u0026#34;joey\u0026#34; } constructor(firstPlayerName = \u0026#34;joey\u0026#34;){ let firstPlayerName = firstPlayerName } 在抽參數時會有小提示，記得要點掉\n另一個練習是檔案目錄操作，例如說多建一個 folder / 把檔案移動過去 / 刪除檔案這些，原本 Tennis 沒有練到這部分，但實務上這些動作很需要，就有花額外的時間練習\n第四週練習 從原本 22 分鐘進步到 19 分，開始用段落切割，一邊放老師的影片一邊放自己的影片，找出為什麼自己會落後的地方\n老師的時間大概是\n(1:54) fifteen love (4:16) thirty love (8:00) love thirty (9:24) deuce (13:24) done 觀察後有幾個自己容易忘記的快捷鍵\n選取抽方法，可以用 z( / z{ 快速抽取，對比 vi( 快上一點 刪除含單引號的變數 工作上用得到嗎？ 練習看起來很理想，但實務上有用嗎？\n這也是在練習過程不斷思考的，雖然果斷也把工作機轉成 Webstrom + idealvim，新專案編寫自然沒太大問題，但是舊專案使用上就卡卡的，主要是以前的 js 寫法比較亂，ide 很難自動辨別，加上一個檔案程式碼上千行，花最多的時間不是在於修改而是查詢，通常是使用 ,m 去看 file structure，接下來就是用滑鼠滾滾滾看 Code\n老師也有在上課提到，滑鼠也不是不好，在查詢時滑鼠滾動還是很方便，不用太拘泥\n總之目前工作使用上還沒有到很順，會少量用一些快捷鍵如抽方法 / 抽參數，但是還沒有辦法流暢的使用組合技\n","date":"2021-02-16T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-02-16-%E6%A5%B5%E9%80%9F%E9%96%8B%E7%99%BC-%E4%B8%8A%E8%AA%B2%E8%88%87%E7%B7%B4%E7%BF%92%E5%BF%83%E5%BE%97/","title":"【極速開發+】上課與練習心得"},{"content":"工作也四年了，來到職涯最為迷惘的一年，過去也不算混東學西學好像拼湊出一些東西，但也開始懷疑自己該選什麼技術發展？又應該選擇怎樣的工作與職涯？未來又何去何從？\n在這低潮期，剛好上完 91 老師的極速開發課程，在技術培訓上帶給我很棒的視野與強大的學習動能，發現自己的基本功應該再好好雕琢；\n又剛好在商業思維學院看到 91 老師分享職涯的種種選擇與規劃，覺得十分的敬佩，自己以一個工程師職涯發展的角度，摘錄 91 大大在商業思維學院的分享\n先分享最後 Q＆A，也是大家常見的問題，接著在摘錄後續 91 的分享\n高薪工程師與低薪工程師的差異 回歸市場需求，看這個市場需要怎麼樣的能力？是不是有很多人可以提供這樣的能力？我可不可以又快又好又便宜的解決問題？培養出自我的稀缺能力，才有議價的自由\n工程師要小心技術本位，需要著眼於商業價值的貢獻，91 老師舉例自己的課程沒有串金流，因為這不是商業上發展的瓶頸，但很多工程師會覺得很酷很方便就去做，但這不會帶來太大的效益\n如何避免自己成為一年經驗用十年的工程師 如果自己是環境中最強的人，建議換個環境，找新的學習動能 觀察別人如何解決問題，所以 91 推崇 Pair / Mod Programming，從別人的作法可以找到不同的學習方式 持續學習 / 持續嘗試 觀察自己有沒有很常加班，或是一直做重複性很高的工作，請用時間去賺時間，找到正向增強迴路 該選擇什麼技術發展 同樣是市場需求，當初 91 不喜歡動態語言，但前陣子 python 很火熱，他透過顧問向客戶學習，最後發現他也喜歡上 python 了\n91 很慶幸自己是顧問與教學兼併，顧問可以從客戶中得到實務上的經驗，轉變成培訓時的教學方向，如果只做教學就一直吃老本無法成長\nJunior 工程師改往廣還是往深 先找到立足之地，熟悉一個技術到可以解決大多數的商業問題，例如說 GraphQL 用動態語言 python / js 比較好實作，但 c# 一樣可以解決但可能比較不適合(這好像怪怪的？)，慢慢發現某些地方可能解法不夠漂亮，再開始跳去學新的技術，就有足夠的動機去學習\nJunior 工程師該去新創還是大公司 都是選擇而已，新創公司需要快速、低成本的迭代，可以學習如何搶灘、快速得到市場的回饋；大公司則是分工很細，開發只做開發，不懂整個產品面向，91 不太認同這樣的分工方式，但是當初選擇 Yahoo 是因為可以接觸到更多的視野 / 實力得到更多的培養，而且能做全世界的生意\n最後回歸重點，時間是最貴的，用時間去賺時間\n91 大大主要分享工作上的選擇 / 工作以外如何創造連結 / 如何透過技術變現 / 最後回顧職涯的成長\n大事記(工作經歷) 第一份工作：叡揚(國防役) 4 年 國防役就是四年跑不掉的好用勞工，所以 91 也常被派去處理棘手的任務，在這第一份工作中，91 磨練了扎實基本功 / 如何帶 team / 敏捷開發，因為是使用微軟的 solution，91 也拿到了微軟 MVP 的頭銜\n練好基本功，找到自我的立足之地\n第二份工作：Yahoo 5 年 離開國防役時有拿到 7 個 offer，最後選擇薪資最低的 Yahoo 原因是\n本身對於交易型系統有興趣 職涯才剛開始，選擇職涯出路最廣的全球性 / 純軟公司 很深的體悟是 如果已經是團隊最強的人，那你的進步會很慢，選擇 Yahoo 有很多很強的技術人可以當作 role model，同時開放的文化也讓 91 有更多嘗試的機會，累積更多的技術聲量\n第三份工作：東森信息 - 1 年 在 Yahoo 五年也做得差不多了，第三份工作有點跌破大家眼鏡，去了一個百廢待興的東森信息，擔任架構師 / 負責與其他團隊溝通，把之前所有累積的經驗都變成薪資籌碼 (350+)，但這也是人生最慘敗的一年，發現 上班時的不快樂不能從下班中補足，同時科普一下年薪過 300 後稅收也增加，多一些錢但其實不會比較快樂\n第四份工作：Odd-e 以前就找到自己熱愛的天職：顧問+教學，基於上一份教訓，決定從自己熱愛的事物變現(商業思維)，雖然要面臨更高的不確定性，但決定給自己一年的時間嘗試，最後就一路做到現在 (薪資還翻好幾倍!)\n創造連結 要在社群中找到自己的立足之地，91 = 敏捷 + 開發，所有的嘗試都圍繞著這兩塊走\n社群貢獻 在社群中找到很多貴人，也透過社群奠定自己的江湖地位\n持續曝光 寫部落格 / 參加 IT 鐵人賽 / 講師交流 / FB 粉絲專頁 / Youtuber\n異業結盟 天瓏書局 / 出版業 / 企業招募 / 商業思維學院\n專業形象 出版書籍 / 微軟 MVP / 研討會講師\n技術變現 從審校開始 第一次出版是審校簡體版的 .net 書籍，此時以開發者的角度出發，踏出第一步將技術變現\n接著第二次出版變成共同翻譯敏捷原文書，也共同出版了跟 .net 相關的書籍\n接到第一場企業內訓 持續出版跟 .net 相關的書籍累積聲量，同時接到第一場企業內訓，當時有底氣接企業內訓也是因為在 Yahoo 累積很多場內部員工培訓的經驗，也開始開辦公開課程\n在社群的耕耘以及部落格的累積，開始收到專欄邀約，有了另一份收入\n持續嘗試 91 在演講不斷重複「在不餓死的情況，我還有XXX 想要嘗試」，像 2021 年初翻譯了 Kent Beck 的書籍，算是人生一大突破，也很符合「敏捷+開發」的人物設定； 在今年 91 因應疫情，決定嘗試 Youtube / 線上課程，並嘗試與不同講師建立連結、彼此導流\n小節 循序漸進的突破： 從審校 \u0026gt; 共同翻譯 \u0026gt; 共同出版 / 公司內訓 \u0026gt; 外部企業培訓 \u0026gt; 公開課程 \u0026gt; 顧問 讓所有的嘗試有連結： 翻譯《單元測試的藝術》，支撐企業內訓與培訓課的內容 / 公開課對於事業的幫助，學員會去公司推動，帶來企業內訓 + 顧問案的機會 找到防線： 確保自己有底線，有第一間顧問案鈦坦，之後才好談底價，自己也不會慌 永遠留時間保持嘗試： 找到第二間顧問永豐金 / 學習新語言 Python，藉由客戶拓展視野，回饋到公開課程 / 社群的內容中 為市場帶來價值：\n觀察市場的動向，以解決企業問題為目標 職涯成長 1. Job 時間是最貴的：如何衡量自己的價值？ 91 表示節省老闆的時間 / 節省團隊的時間就是你的價值 用時間賺時間：透過工具 / 提升技能等賺更多的時間，才能持續提升自己的價值 跨出去持續學習：挑公司時選擇開放透明，把新的方法帶回公司，用時間賺時間 (呼應上一點) 向上溝通：持續與上司即時同步，取得共識 2. Work 找愛：這是長期投資，原本 91 很討厭動態語言，但他覺得應該要嘗試過才能批評，最後也喜歡上 Python、Ruby 等，要試過才知道 綜效：找愛後開始想辦法關聯，例如市場上開發與敏捷培訓的需求，有敏捷教練，但他們可能不懂開發 / 反之亦然，91 就把兩個他所愛的興趣結合起來，在市場上就只能找他 3. Career 天花板 \u0026gt; 薪資：選擇 Yahoo 薪資很低，但是帶來很大的成長，把這些成長灌在下一份工作上 用成果描述自我，而不是顯擺的 Title：持續在 Job / Work 輸出，用成果說服人，後續 91 工作都是邀約，而不用自己投 4. Values 初心：擇其所愛 人脈：你可以幫助到多少人？ 是否有能力幫助其他人？ 是否有辦法知道其他人現在有需要幫忙？ 為什麼他人要選擇你幫忙？\n找到自己價值的基底 自由：選擇的自由 / 不需要擔心別人競爭與壓迫 / 健康 總結 我自己把講座拆成兩部分，一個是技術者的職涯發展 / 一個是技術者的技能變現，自己目前專注於前者，總結自己的心得\n時間最貴，不斷的想如何節省自己的時間 / 團隊的時間 / 老闆的時間 從市場需求與商業價值的角度去思考自我職涯發展 嘗試把自己所熱愛的事物變現，想想看如何發揮綜效 ","date":"2021-02-06T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-02-06-%E5%B7%A5%E7%A8%8B%E5%B8%AB%E7%9C%8B%E5%95%86%E6%A5%AD%E6%8A%80%E8%A1%93%E5%B7%A5%E4%BD%9C%E8%80%85%E7%9A%84%E5%95%86%E6%A5%AD%E6%80%9D%E7%B6%AD%E8%AC%9B%E5%BA%A7%E5%88%86%E4%BA%AB/","title":"【工程師看商業】技術工作者的商業思維講座分享"},{"content":"\nClean Code 相信很多人都看過，提供軟體界一個自我審思程式碼品質的標竿，也被許多工程師奉為圭臬，這一本 Clean Coder 更多著重於身為一名專業人士所需具備的內涵，包括但不限於程式碼\nHighlight 兩點我覺得被重重提醒的地方，並提出一些自己的想法\n說「不」與說「是」 身為工程師，都遇過 PM 臨時來的需求，並強硬的要壓一個不太合理的日期，在這種壓力下，不論是出於善意或是懶得爭辯，是否都曾說過「好的我試試」\n對於答應接下工作的工程師而言，「試試」就是代表有可能做到，但是 PM 會解讀成「好的沒問題」\n等到上線日來臨開天窗，身為 PM 覺得被工程師耍了，而工程師也很無辜的表示「就說試試而已」，開始踢起皮球，這對雙方都是很不專業的表現\n當我們要給予承諾時，請非常小心我們的措辭，如果因為壓力就改口說「我試試看」，那是否代表著過去的你都在偷懶，在壓力相逼的情況下反而增加工作產能？\n如果還是照既定的方式做，那所謂的「嘗試」又是在嘗試什麼呢？\n這聽起來很理想化，或許會嗤之以鼻，但及早說不，並堅守底線，不要因為壓力就放棄測試，不要因為壓力就想寫出有臭味的程式碼，身為專業人士應該有對自己工作品質的保證，而不是一昧的降低標準\n或許更好的方式是將任務細分，在時限內給出合理的成果，並規劃未來逐步完成的時辰，遇到時辰困難應儘早反應，而不是抱著僥倖的心態祈禱或是冷眼看著任務失敗\n當確定要接下任務時，請確保再給予「是」的承諾時包含三個步驟\n口頭上說自己會去做 心裡認真對待自己所做出的承諾 真的付諸行動 如果發現有這些徵兆，那多半承諾都會落空\n需要 / 應當 - 我們需要有人完成這項事情 希望 / 但願 - 但願我有時間幫你處理 讓我們 (而不是「讓我」) - 讓我們一起做這項任務 真正的承諾，應當是明確指出自己將負責這件事情，並給予明確的時辰與交付內容\n這點真的很有感，尤其是做後端，時常要協助其他團隊調查 API 呼叫紀錄 / 伺服器運作狀況 / DB 資料查詢等瑣事，常常會說「好的，我等等幫你查」，一方面正事一直被打斷效率差，另一方面也容易忘記答應過的事情，導致整天工作很忙卻也沒有什麼產出\n後來我做了一些調整\n上班一開始就用筆記本寫下今天要完成的任務 開發途中有人敲 slack 需要協助，除非事態緊急，否則等任務做到一個階段再回覆 每天只接固定的瑣事數量，其餘的給予明確時間回覆(如隔天) 盡可能確保自己的任務準時且有品質交付，之餘也協助其他團隊成員的調查，讓自己給予的承諾是正面的\n預估 預估工時是一個蠻微妙的藝術，PM 會拿一個不是完全明確的需求來詢問一個預估的工時，預估不是承諾，帶有著一點模糊的空間，「可能三天？如果有緊急事件可能要五天？」未來有著太多的不確定性，但是預估與資源安排息息相關，不能因為預估不準而讓 PM 難以安排進度，而往往工程師有過度樂觀的狀況 (笑\nPERT (Program Evaluation and Review Technique) 美軍在 1957 年發展潛艇極地航行計畫時所設計的預估工時方法，採用三個數值\nO 異常樂觀：所有事情都很順利的情況下的工時，機率應小於 1% N 常規預估：機率發生最高的預估工時 P 異常悲觀：考量各種災害如地震等，給予最悲觀的工時預估，機率應小於 1% 得出上面三個數值後，可以用機率分析表示\n預估完成工時: u = (O+4N+P) / 6 標準差，用來衡量不確定性：(P-O) / 6 德爾菲法 剛才的 PERT 是一個人預估的工時，但是換一個人可能會給出不同的預估結果，仰賴團隊成員給出不同的預估，最後達成共識也是一種方法\n主持人請每個成員在他人不知道的狀況下決定預估工時，時間到一起亮牌(數手指之類的)，看大家是否有共識，如果有人有很大的歧異，則開始討論分歧的依據，接著下一輪，直到大家給予的評估都差不多才結束\n凝聚共識也是很重要的一環，尤其是團隊合作上，從每一個人評估工時的不同，就能去看出對於細節與實作想法上的差異，彌平這類的差異可以讓團隊合作更順暢\n大數定律 將大任務拆解成小任務後在估時會比較精準，且經過拆解可以更理解任務的各種面向\n結語 書中還有談其他面向如測試 / 練習 / 協作等，算蠻有趣的一本書，明確指出專業人士的區別，其中舉的案例也是歷歷在目，值得在職涯中時時品味\n","date":"2021-01-26T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-01-26-clean-coder-%E7%84%A1%E7%91%95%E7%9A%84%E7%A8%8B%E5%BC%8F%E7%A2%BC%E5%BF%83%E5%BE%97/","title":"《Clean Coder 無瑕的程式碼》心得"},{"content":"要打造一個「可以動的 Docker Image」很簡單，參考 Node.js 官方文件 Dockerizing a Node.js web app 就可以產出一個將近 1GB 的 Docker Image\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 FROM node:14 # Create app directory WORKDIR /usr/src/app # Install app dependencies # A wildcard is used to ensure both package.json AND package-lock.json are copied # where available (npm@5+) COPY package*.json ./ RUN npm install # If you are building your code for production # RUN npm ci --only=production # Bundle app source COPY . . EXPOSE 8080 CMD [ \u0026#34;node\u0026#34;, \u0026#34;server.js\u0026#34; ] 就開始想\n這樣安全嗎？ 可以打造更輕量、更好 ship 的 Image 嗎？ 後來找到這一篇文章10 best practices to containerize Node.js web applications with Docker覺得十分實用，也解決安全性上的疑慮，以下摘要重點\n1. 選擇正確的 Base Image 並透過 Build Stage 精簡產出 tldr;\n採用 alpine 或 -slim 版本的 base image 用 sha256 指定 base image 版本避免異動 支援多階段，可以前期 build 用比較大的 base image，最後產出在使用精簡的 base image 1 2 3 4 5 6 7 FROM node:latest AS build .... # --------------\u0026gt; The production image FROM node:lts-alpine@sha256:b2da3316acdc2bec442190a1fe10dc094e7ba4121d029cb32075ff59bb27390a .... COPY --from=build /usr/src/app/node_modules /usr/src/app/node_modules .... 進入 docker hub node.js 官方 image，可以看到玲琅滿目的版本，除了對應不同的 node.js 版本，底層的 os 可以分成幾種\nstretch: 基於 debian 9 bulter: 基於 debian 10 alpine: 基於 alpine linux 坦白說目前還不太理解 debian 9 / 10 真正的差異，而 alpine linux 目標是打造最輕量的 container os，最大差異在採用 musl libc 取代 glibc，如果使用的 js 套件有利用到 libc 可能會有問題\n而帶有 -slim 結尾則是代表該 image 是最輕量可運行 Node.js 的 container os，也是官方建議在生產環境採用\n例如說 node:14.15.4-stretch 尺寸 942MB vs node:14.15.4-stretch-slim 尺寸 167MB，前者連 build 工具都有包含如 python / node-gyp 等，這導致尺寸差異非常巨大\n安裝越多工具的 Image，導致的潛藏性安全漏洞就越多，所以正式環境運行盡量採用 slim 版本\n所以最棒的是在第一階段採用完整 Image 方便 build node_modules，最後階段產出用精簡 Image 確保運行時沒有多餘的工具\n2. 確保只安裝 production 需要的 node modules 並指定 NODE_ENV 為 production 透過 RUN npm ci --only=production 只安裝 dependencies 而沒有 dev_dependencies 開發用的套件\nnpm ci 與 npm install 看似都在安裝套件但有很大的差異；\nnpm ci 只讀取 lock file，並用 package.json 做比對，如果 lock file 與 package.json 版本不合會噴出錯誤，最適合用在要穩定且強一致的套件版本要求\nnpm install 則是讀取 package.json，並透過 lock file 做安裝的版本指定，如果有套件沒有出現在 lock file 中，則 npm 直接安裝\n運行環境，根據 Node.js 的不成文規定，請指定NODE_ENV=production，各個套件都會針對 production 進行優化，另如文中舉例 express.js 會在生產環境加入頁面 cache 機制\n3. 不要用 root 運行 container!! 記得加入 USER node 切換使用者，並記得 COPY 時要給予使用者相對權限 COPY --chown=node:node . /usr/src/app\n這很重要，也很容易忘記，給予用戶不多不少的權限一直是安全性的基本準則，預設 docker container 內是以 root 運行 process，但如果不小心應用程式有漏洞，甚至有可能讓駭客跳脫 container context 獲得 host root 權限\n4. 正確接收程序中斷事件與優雅地退出 採用 dump-init 直接啟動 Node.js process\n1 2 RUN apk add dumb-init CMD [\u0026#34;dumb-init\u0026#34;, \u0026#34;node\u0026#34;, \u0026#34;server.js\u0026#34;] 並記得在 Node.js 中偵測事件\n1 2 process.on(\u0026#39;SIGINT\u0026#39;, closeGracefully) process.on(\u0026#39;SIGTERM\u0026#39;, closeGracefully) 伺服器長時間運行時，總會遇到版本更新的時候，最理想的做法是讓中斷流量並讓程序完成剩餘工作後優雅退出，但如果採用的方式錯誤 Docker Container 會無法收到系統中斷的事件 SIGKILL 、SIGTERM 等\n以下是幾種常見的 Container 執行指令，但分別有一些問題\n1. CMD \u0026ldquo;npm\u0026rdquo; \u0026ldquo;start\u0026rdquo; 這會遇到兩個問題\n透過 npm 啟動 Node.js process，但是 npm 不會正確 pass 所有的系統中斷到 Node.js process 上 CMD \u0026quot;cmd\u0026quot; \u0026quot;params\u0026quot; .. 與 CMD [\u0026quot;cmd\u0026quot;, \u0026quot;params\u0026quot;] 是不同的，前者是先啟動 shell 再去執行後面的 cmd；而後者則是直接執行 cmd，最直接的差異就是 PID 1 是 shell 還是 cmd，透過 shell 同樣有可能不會收到全部的系統中斷 2. CMD [\u0026ldquo;node\u0026rdquo;, \u0026ldquo;index.js\u0026rdquo;] 優化上面的指令，改由直接啟動 node.js 少了 npm 與 shell，這會遇到另一個問題 linux 對於 PID 1 的程序有特別的處理，因為 PID 1 代表此程序要負責系統的初始化，所以 Kernel 會有額外的處理機制，實際的影響有\n一般的程序收到 SIGTERM 後 Kernel 會有預設的結束處理，但是 PID 1 沒有預設終止處理，也就是收到後沒有主動退出就不會退出 如果有 orphan process 會被主動掛載到 PID 1 之下，但一般的 process 不會去處理 orphan process，會留下很多 zombie process 根據 Node.js Docker 小組建議，不要讓 Node.js 運行在 PID 1 上\nNode.js was not designed to run as PID 1 which leads to unexpected behaviour when running inside of Docker. For example, a Node.js process running as PID 1 will not respond to SIGINT (CTRL-C) and similar signals\n最終解法：透過 init process 再去啟動 Node.js 最後用 dumb-init，這套由 yelp 釋出的啟動 process，會正確將所有的系統中斷都 pass 給所有的 child process，並且在退出時清除所有的 orphan process\nIntroducing dumb-init, an init system for Docker containers\n市面上還有不同的選擇，可以參考此篇 Choosing an init process for multi-process containers\n5. 正確處理 Build 階段使用的機敏資料 善用 multi stage，讓機敏資料不要外洩在 Image 當中，並使用 mount secret 同步機敏資料\n1 RUN --mount=type=secret,id=npmrc,target=/usr/src/app/.npmrc npm ci --only=production 如果有一些機敏資料是在 Building 階段所需要，例如要去 private github 拉 repo 的 .npmrc 等資料，需要被妥善處理，常見的錯誤示範有\nhard code 寫死 採用環境變數，在 docker build 時指定，但如果用 docker history 還是會被發現 奇怪的是我並沒有在 Docker 文件 Use bind mounts 看到 \u0026ndash;mount type=secret 的說明，但確實有在官方教學看到 Build images with BuildKit\n結語 最終的產出會長這樣\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # --------------\u0026gt; The build image FROM node:latest AS build WORKDIR /usr/src/app COPY package-*.json /usr/src/app/ RUN --mount=type=secret,id=npmrc,target=/usr/src/app/.npmrc npm ci --only=production # --------------\u0026gt; The production image FROM node:lts-alpine RUN apk add dumb-init ENV NODE_ENV production USER node WORKDIR /usr/src/app COPY --chown=node:node --from=build /usr/src/app/node_modules /usr/src/app/node_modules COPY --chown=node:node . /usr/src/app CMD [\u0026#34;dumb-init\u0026#34;, \u0026#34;node\u0026#34;, \u0026#34;server.js\u0026#34;] ","date":"2021-01-21T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-01-21-%E5%A6%82%E4%BD%95%E6%89%93%E9%80%A0%E5%AE%89%E5%85%A8%E7%9A%84-production-ready-node.js-docker-image/","title":"如何打造安全的 production ready Node.js Docker Image"},{"content":" 此篇為參加商業思維學院 2021 講座《量化行銷》感想與反思，講者是來自 KKday 營銷長 Yuki，分享在公司經營流量與商業開發的心得\n量化行銷的迷思 在傳統觀念中，會直覺認為流量會帶來營收，甚至把流量制定為先行指標，卻忽略了流量的品質、後續購買轉換、不同的客戶終生價值等分析，導致看似擁有大流量，營收卻也不見起色，以下 Yuki 老師整理了幾種迷思\n創造流量就有更多的生意？ 不分行銷通路來源 常見的電商網站會與折扣網合作，帶來更多的曝光，但是這樣的曝光跟流量值得嗎？試想一般消費者行為，到網站找到商品後，會再去折扣網搜尋折扣碼，這樣等同於多了一次流量，但一樣只成交一筆，還賠上了毛利\n所以在獲取流量的同時，應該去關注流量本身的品質\n不分產品與活動 Yuki 老師舉自家網站的範例，之前五月天演唱會門票秒殺，話題正在風口上，他們也規劃了「五月天演唱會接駁服務」的商品，成功獲得巨大的流量，但是沒有什麼成交量，因為用戶多半是想要買演唱會門票，即使被騙進來也很快就走了\n所以請小編寫案蹭流量很簡單，但其實都沒有實際幫助，還不如開發有用的商品\n舉辦行銷活動讓舊客回購，就能擁有免費流量 電商網站經營常會透過折扣等舉辦行銷活動，希望喚醒沈睡的舊客回購，但如果沒有仔細去考量成本，其實喚醒舊客的成本往往比獲取新客成本來得高\n獲客成本與獲流量成本沒有會員價值分析 Yuki 老師在分析數據時發現到，用戶的第一筆訂單如果是高單價，那他後續的訂單往往也是高單價，終生會員價值會是一般用戶的 2 ~ 4 倍以上！ 這樣的用戶很明顯就值得更高的成本去拉攏\n如果沒有仔細分析會員價值，而是把所有用戶都混為一談，甚至訂製錯誤的 KPI，會導致流量做得很辛苦也不見起色\nTakeaway 以產品線延伸創造舊客流量與分群回購，而不是靠折扣！ KKday 之前曾做過搭虎航泡溫泉的活動，累積了一批航空迷用戶，後來推出星宇微旅行，這些老用戶就很買單 / 另外有一批親子用戶，過往固定會在寒暑假出國度假，但因為疫情的關係根本無法出國，給予再多的折扣沒用，KKday 改為舉辦夏令營/冬令營，就受到這群親子用戶的喜愛 要做出用戶喜愛的產品才能夠雙贏\n要追求的不是流量，而是能創造價值的流量！ 如同先前的分享，高價值的用戶獲取成本通常會比較高，而且轉換率也會比較低，但換算終生價值後還是遠高於一般用戶，千萬不要用錯誤的 KPI 導致錯誤的結果\n真正的機會在行銷漏斗之外 傳統的行銷漏斗，孜孜矻矻在優化每一層的轉換當中，但 Yuki 老師分享到往往最大的機會都在這漏斗以外，以下是幾個 KKday 嘗試的方向\nIP 合作 KKday 曾爭取到阿妹台東跨年演唱會的合作方案，搭配包機 / 機場接駁等服務，組合成曙光跨年行程，既搭上熱門話題也迎合客戶需求，所以一推出就秒殺，後續客戶也給予好評，後續也敲開與其他明星的演唱會合作機會\n相比於五月天接駁車的擦邊球，提出有話題又有真正需求的商品才能長長久久\n在與 IP 合作過程中，Yuki 老師強調 不要妄自菲薄想說「為什麼XXX 這麼有名怎麼會想跟我們合作」，她在過去連續與多家 IP 如福斯汽車/ tomica小卡車合作等等，都推出多贏的商業合作案，最棒的是對方提供限定商品，同時也會幫忙宣傳，用戶買單也會很開心\n科技工具的應用 KKday 要結合自家商品產出大量的文章推廣，但資料的整理/圖文排版等十分耗費人力，後來就研發 KKday 智能小幫手，透過 AI 爬文 / 自動產生文案最後再由資深編輯審核發布，加速旅遊文章的發布速度\n過程也提到這技術門檻不高，但就是提供很棒的商業價值，解決了真實的問題\n轉換率迷思 只要發生流量大、轉換率低，通常直覺就是要去 tune 介面提升轉換率，但這往往陷入了思維的誤區，Yuki 老師分享到 KKday 一個案例，發現過年期間的日本某一個活動的流量很好但轉單率超低，仔細研究才發現是台灣去日本玩的遊客點擊，但是在選擇日期時就離開了，追查後看到是因為活動的前置日期是五天，如果是遊客不太可能在旅遊中買五天後的行程，最後的調整是與供應商洽談，把前置日期壓縮到一天，轉換率也就跟著提升\n這回應到老師另一個重點\n最直覺的改善方案，往往只在初期有效，非直覺的機會更顯得珍貴\n這點非常有感，因為我們公司也是在一個時期瘋狂的在優化 onboarding 流程 / 購買轉換，不斷在看每一個 step 間的數據，確實也在過程中得到一定的提升，但是後續就遇上貧頸，最終還是要持續的優化產品/調整購買方案，才能讓用戶願意買單\n好做的通常都做過了(沒做過當然還是要做)，但要記得產品本身的內涵才是真正的長遠之道\n如何帶領團隊 👍 別只問數字 如果主管只過問數字，不在乎過程，那最後產出不見得是最理想的，像是洗出低品質的流量 👍 別鼓勵做報告 數字應該可以自己從報表檢視，應該讓下面的人有更多思考與試錯的機會\n👍 做策略要清空腦袋 不要用開會塞滿大家的行程\n👍 用詢問代替建議 因為管理階層的權威感，可能突發奇想的建議就會被下屬理解成執行方向(超有感)，應該以詢問代替建議，很多事情第一線的員工反而更清楚\n總回顧 新客流量：\n能被優化的大多都做了，要做一些意想不到的突破 舊客回購：\n透過產品線的延伸，迎合用戶的需求而非透過折扣 增加助攻： KKday 數據研究發現，用戶在旅遊前 14 天會開始逛網站買行程，購買途中會有 7 個關鍵點，如何在購買旅程中打動用戶，就要不斷地去思考跟嘗試，例如增加圖片/影片的品質可以提升轉換等 健康的流量模型：\n分產品/分流量/分用戶價值，才能制定正確的流量策略 Martech:\n利用現有的工具優化行銷的方式 職場心態 在過程中，Yuki 老師持續強調 老闆思維，不要只著眼於自身的 KPI 達成，而是要更近一步發揮自身影響力，試著站在老闆的角度思考，看看公司產品還有什麼地方可以改善，而不是受限於自身的職位\n但同時老師也提醒，當老闆還是用舊有的觀念時，盡可能先迎合老闆的需求，例如流量翻倍，在過程中有可以施力的地方在使力，慢慢的導正觀念，而不是直接反駁老闆，這樣才有轉變的機會\n反思 聽完演講，反過來思考幾件事情\n我們公司有完整的用戶價值分析嗎？\n如果要有幾個詞描述我們公司最有價值的用戶群會怎麼描述呢？\n我們有完整的流量模型分析嗎？\n我們有做什麼流量推廣嗎？\n回顧自家公司是做 SaaS，專注於單一的產品開發，所以不像 KKday 是以平台的形式，有大量的商品與合作案可以操作，但是對於用戶輪廓與分群這部分也是有在著手，例如說我們會在 onboarding 流程埋入用戶的用途選填，藉此分群用戶\n但有趣的是我們公司用戶遍佈世界各地，用戶的購買行為/轉換率等受到國情影響比用戶用途影響還要大，所以在做大部分購買實驗時都是以國家為區分\n至於流量追蹤後續用戶轉換的部分，則還在著手開發中，但入口相對也單純，從網站導流或是 app store / play store 搜尋、下廣告進來的，持續觀察會不會有什麼變化出現\n","date":"2021-01-19T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2021/2021-01-19-%E5%B7%A5%E7%A8%8B%E5%B8%AB%E7%9C%8B%E5%95%86%E6%A5%AD%E9%87%8F%E5%8C%96%E8%A1%8C%E9%8A%B7%E7%9A%84%E6%B5%81%E9%87%8F%E6%80%9D%E7%B6%AD%E8%AC%9B%E5%BA%A7%E5%BF%83%E5%BE%97/","title":"【工程師看商業】量化行銷的流量思維講座心得"},{"content":"寫了一個簡單的購物流程 SQL，在一個 transaction 中執行\n讀取 product 資訊：select * from product where id = 1 寫入新訂單 order 狀態: insert into orders (product_id) values (1) 更新 product 販售狀態：update product set sold=1 where id = 1\n其中 order table 的 product_id 是引用 product table 的 id 當做 foriegn key，並在併發的情況下執行，開始偶然遇到 Deadlock 1 Error: update `products` set `sold` = 34 where `id` = \u0026#39;919\u0026#39; - Deadlock found when trying to get lock; try restarting transaction 當下覺得奇怪，第一印象中 Deadlock 只發生在兩個 Transaction 互相所需的欄位而無法釋放，如\nT1: lock(r1) , wait(r2) T2: lock(r2), wait(r1)\n但明明我就一種 SQL 併發執行，怎麼也會有 Deadlock 以下開始排查原因\nMySQL Deadlock 說明 讓我們來看一下官方文件 15.7.5 Deadlocks in InnoDB\nDeadlock 主要是多個 Transaction 手上握有對方需要的資源，在等待資源釋放的同時卻也不會釋放手上的資源，常發生在使用 update 卻順序剛好相反\n如果 Deadlock 數量很少不太需要擔心，應用程式記得 retry 就好，但如果發生很頻繁就要檢查 SQL 的狀況\n為了盡量減少 Deadlock 發生，可以檢查以下方式\n盡可能減少 Update / Delete 在單一 Transaction 中的數量 Lock 時請依照同樣的順序 (例如 select \u0026hellip; for update) 降低 Lock 的層級，避免 lock tables 的操作 警慎選擇 index，因為過多的 index 可能會造成 deadlock，後續會再展開描述 InnoDB uses automatic row-level locking. You can get deadlocks even in the case of transactions that just insert or delete a single row. That is because these operations are not really “atomic”; they automatically set locks on the (possibly several) index records of the row inserted or deleted.\n考慮降低 isolation level，高層級的 isolation level 會去改變 read 的操作，例如 MySQL 中 serializable 其實就是隱式把所有 select 都加上 lock for share，引自於官方文件 15.7.2.1 Transaction Isolation Levels This level is like REPEATABLE READ, but InnoDB implicitly converts all plain SELECT statements to SELECT \u0026hellip; FOR SHARE if autocommit is disabled.\nDeadlock detection 預設是開啟，但如果在高流量下會有效能的影響，如果預期 Deadlock 狀況不多可以改透過 innodb_deadlock_detect 選項關閉，用 innodb_lock_wait_timeout 一直等不到 lock 發生 timeout 而觸發 rollback 取代\nMySQL 可以透過 SQL 指令 \u0026gt; SHOW ENGINE INNODB STATUS 看最後一筆發生 Deadlock 的原因，或是開啟 innodb_print_all_deadlocks 把每一次 Deadlock 原因都輸出到 error log 中\n開啟設定的 SQL 為 \u0026gt; SET GLOBAL innodb_print_all_deadlocks=ON;\n具體的 Deadlock log 當在應用程式發現 deadlock 錯誤後，回到 MySQL 使用指令查看，得到以下原始 log\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 ===================================== 2020-12-26 00:10:16 0x7f9668490700 INNODB MONITOR OUTPUT ===================================== Per second averages calculated from the last 29 seconds ----------------- BACKGROUND THREAD ----------------- .... ---------- SEMAPHORES ---------- .... ------------------------ LATEST DETECTED DEADLOCK ------------------------ 2020-12-26 00:05:14 0x7f9657cf9700 *** (1) TRANSACTION: TRANSACTION 14048, ACTIVE 1 sec starting index read mysql tables in use 1, locked 1 LOCK WAIT 11 lock struct(s), heap size 1136, 6 row lock(s), undo log entries 2 MySQL thread id 54, OS thread handle 140283242518272, query id 45840 172.22.0.1 api-server updating update `products` set `sold` = 32 where `id` = \u0026#39;919\u0026#39; *** (1) HOLDS THE LOCK(S): RECORD LOCKS space id 3 page no 8 n bits 336 index PRIMARY of table `online-transaction`.`products` trx id 14048 lock mode S locks rec but not gap Record lock, heap no 259 PHYSICAL RECORD: n_fields 7; compact format; info bits 0 0: len 4; hex 00000397; asc ;; 1: len 6; hex 0000000036d7; asc 6 ;; 2: len 7; hex 010000013f1e26; asc ? \u0026amp;;; 3: len 21; hex 50726163746963616c204672657368204d6f757365; asc Practical Fresh Mouse;; 4: len 4; hex 800000b1; asc ;; 5: len 4; hex 800000fe; asc ;; 6: len 4; hex 80000020; asc ;; *** (1) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 3 page no 8 n bits 336 index PRIMARY of table `online-transaction`.`products` trx id 14048 lock_mode X locks rec but not gap waiting Record lock, heap no 259 PHYSICAL RECORD: n_fields 7; compact format; info bits 0 0: len 4; hex 00000397; asc ;; 1: len 6; hex 0000000036d7; asc 6 ;; 2: len 7; hex 010000013f1e26; asc ? \u0026amp;;; 3: len 21; hex 50726163746963616c204672657368204d6f757365; asc Practical Fresh Mouse;; 4: len 4; hex 800000b1; asc ;; 5: len 4; hex 800000fe; asc ;; 6: len 4; hex 80000020; asc ;; *** (2) TRANSACTION: TRANSACTION 14052, ACTIVE 1 sec starting index read mysql tables in use 1, locked 1 LOCK WAIT 11 lock struct(s), heap size 1136, 6 row lock(s), undo log entries 2 MySQL thread id 57, OS thread handle 140283970258688, query id 45841 172.22.0.1 api-server updating update `products` set `sold` = 34 where `id` = \u0026#39;919\u0026#39; *** (2) HOLDS THE LOCK(S): RECORD LOCKS space id 3 page no 8 n bits 336 index PRIMARY of table `online-transaction`.`products` trx id 14052 lock mode S locks rec but not gap Record lock, heap no 259 PHYSICAL RECORD: n_fields 7; compact format; info bits 0 0: len 4; hex 00000397; asc ;; 1: len 6; hex 0000000036d7; asc 6 ;; 2: len 7; hex 010000013f1e26; asc ? \u0026amp;;; 3: len 21; hex 50726163746963616c204672657368204d6f757365; asc Practical Fresh Mouse;; 4: len 4; hex 800000b1; asc ;; 5: len 4; hex 800000fe; asc ;; 6: len 4; hex 80000020; asc ;; *** (2) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 3 page no 8 n bits 336 index PRIMARY of table `online-transaction`.`products` trx id 14052 lock_mode X locks rec but not gap waiting Record lock, heap no 259 PHYSICAL RECORD: n_fields 7; compact format; info bits 0 0: len 4; hex 00000397; asc ;; 1: len 6; hex 0000000036d7; asc 6 ;; 2: len 7; hex 010000013f1e26; asc ? \u0026amp;;; 3: len 21; hex 50726163746963616c204672657368204d6f757365; asc Practical Fresh Mouse;; 4: len 4; hex 800000b1; asc ;; 5: len 4; hex 800000fe; asc ;; 6: len 4; hex 80000020; asc ;; *** WE ROLL BACK TRANSACTION (2) ------------ TRANSACTIONS ------------ ..... -------- FILE I/O -------- ..... ------------------------------------- INSERT BUFFER AND ADAPTIVE HASH INDEX ------------------------------------- ..... --- LOG --- ..... ---------------------- BUFFER POOL AND MEMORY ---------------------- ..... -------------- ROW OPERATIONS -------------- ..... ---------------------------- END OF INNODB MONITOR OUTPUT ============================ 挑出重點來看\nLATEST DETECTED DEADLOCK 以下顯示最後一次發生的 Deadlock，有表述互相死鎖的兩筆 Transaction *** (1) TRANSACTION: 與 *** (2) TRANSACTION: 接著看到兩筆 Transaction 手上握有的 Lock，看得出來他們都有同一行 product row 的 lock mode S locks 1 RECORD LOCKS space id 3 page no 8 n bits 336 index PRIMARY of table `online-transaction`.`products` trx id 14052 lock mode S locks rec but not gap 接著他們在等待的鎖為 1 `online-transaction`.`products` trx id 14052 lock_mode X locks rec but not gap waiting 答案就此揭曉，因為兩個 Transaction 手上都握有 share lock，如果要取得 exclusive lock 則對方必須先釋放 share lock，因此造成 Deadlock，最終看到 Transaction 2 被 rollback 了\n1 *** WE ROLL BACK TRANSACTION (2) Debug 資訊乍看很多，但仔細看還蠻好理解的\n找出問題根源 知道是因為 shared lock 導致後面的 exclusive lock 死鎖的原因，回頭爬指令看哪裡有問題\n首先定位到 select from，根據官方文件，除非 isolation 是 serializable，否則一般的 select from 是沒有 lock 的，出處 15.7.2.3 Consistent Nonlocking Reads\nConsistent read is the default mode in which InnoDB processes SELECT statements in READ COMMITTED and REPEATABLE READ isolation levels. A consistent read does not set any locks on the tables it accesses, and therefore other sessions are free to modify those tables at the same time a consistent read is being performed on the table.\n既然不是 select 造成，嫌疑犯就變成 insert order 了，orders table 中的 product_id 是引用 product table 中的 id 當作 foreign key，果然找到相關的描述 14.7.3 Locks Set by Different SQL Statements in InnoDB\nIf a FOREIGN KEY constraint is defined on a table, any insert, update, or delete that requires the constraint condition to be checked sets shared record-level locks on the records that it looks at to check the constraint. InnoDB also sets these locks in the case where the constraint fails.\n真相大白\n如何解決 在 SQL 最一開始，因為篤定會改變 product 欄位，直接使用 select for update 用 exclusive lock 鎖住，就解決問題了\n需注意設定成 serializable 還是有機會發生 Deadlock 喔，因為在 MySQL 中只是追加 select from 的鎖，而不是真的像 redis 一次只順序執行一道指令喔\n補充資料 - 關於 MySQL Lock 解决死锁之路（终结篇） - 再见死锁 強力推薦這篇文章，後來與同事在工作上排查死鎖，發現 update 單筆資料竟然也有死鎖的狀況，才知道 lock 需要鎖定對應的 Index 並且在沒有命中時會使用區間鎖 (如果 isolation level 在 Repeatable Read 以上)，還是有機會造成死鎖\n透過上述的參考資料，並重新翻閱 MySQL 文件，整理了另一篇 【MySQL】Lock 與 Index 關係和 Deadlock 分析\n","date":"2020-12-26T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-12-26_mysql-deadlock-%E5%95%8F%E9%A1%8C%E6%8E%92%E6%9F%A5%E8%88%87%E8%99%95%E7%90%86/","title":"MySQL Deadlock 問題排查與處理"},{"content":"UUID 是一個被大量使用的演算法，分散式地產生大量不重複且固定為 128 bit的 ID，分散式是指說多台機器每秒同時產生多筆 UUID，有極大概率這些 UUID 都不會發生重複，不需要有一台機器居中負責 ID 的管控與發放，反例像是 MySQL 資料庫中的 auto increment id\n先提重點，如何選擇 UUID v1 ~ v5，參考 Which UUID version to use?\nv4: 完全隨機，沒有特殊需求選這個，根據 uuid.js 統計有 77% 用戶選擇這個 v1: 組成包含 timestamp 與機器識別碼(MAC Address)，如果需要識別由哪一台機器在什麼時間點產生可以選這個，根據 uuid.js 統計有 21% 用戶選擇這個 v5 \u0026amp; v3: 可以指定 Namespace 與 Name，相同的 Namespace 與 Name 會產生相同的 UUID，v3 雷同 v5，差別在於 v5 會採用 SHA1 當作 Hash function 而 v3 採用 MD5，除了相容性等考量，否則請優先採用 v5，根據 uuid.js 統計有 1% 用戶選擇這個 v2: 不採用，連 RFC 文件也只有帶到定義，沒有實作規範 需要特別注意，如果是使用 uuid.js 的 v1，uuid 實作是沒有採用 MAC Address，所以如果有識別同一台機器產生的 uuid 的需求，需要自己另外實作，後面有更詳盡的補充\n以下將摘要 RFC 4122 - A Universally Unique IDentifier (UUID) URN Namespace ，並比對每週有將近四百萬下載的 uuid.js 實作，有詢問作者一些細節，他也非常熱心補充很多有棒的資料，會一並整理分享\nRFC 4122 UUID 共有 128 bits，以下是 v1 實作規範\n4.1.2. Layout and Byte Order 以下將介紹每個欄位的組成\n4.1.3. Version UUID 版本夾在 time_hi_and_version 最高效位元(most significant)中 4~7 bit，所以從 UUID string 就能看出版號\n4.1.4. Timestamp Timestamp 總共是 60 bits，對於 UUID v1 來說是 UTC 時間且自 1582/10/15 00:00:00 (the date of Gregorian reform to the Christian calendar) 開始計算，如果機器沒有 UTC 時間，可以採用 local time，但是要確保 local time 是穩定的\n4.1.5. Clock Sequence 如果系統時間發生倒轉，或是 Node ID 發生改變，則會增加碰撞的可能性，所以透過 Clock Sequence 來紀錄，如果發現產生過的 UUID 採用的 Timestamp 比當下的時間還要晚時 (意即 Clock 時間被倒轉)，則 clock sequence 遞增，初始化則隨機產生\n另外如果 Node ID 發生改變，那最好也將 clock sequence 隨機重置，降低碰撞的風險\n需注意 clock sequence 的設定最好是在系統啟動後設定一次就不要再改變，降低跨系統產生碰撞的風險\n4.1.6. Node Node ID 採用 IEEE 802 MAC address，如果有多個 MAC Address 任一挑一個有用的即可，如果沒有則隨機產生\n以上是 v1 的欄位意義，v3、v5 則是把 Name 加上 Namespace 取雜湊，接著分配至上述的欄位中，v4 則全部隨機產生\n4.2.1. Basic Algorithm 接著看最基本的演算法實作流程\n取得系統級別的全域鎖 讀取系統設定檔，包含 clock_seq / node id / timestamp 計算出 timestamp 取得 node id 如果保存 node id 與讀取的 node id 不同，重新設定 clock_seq 當前 timestamp 小於保存的 timestamp，clock_seq + 1 將目前計算的值保存回去 釋放鎖 將目前的 timestamp / clock_seq / node id 組合出 uuid 如果要高頻率製造 uuid，會遇到以下幾個效能貧頸與對應解法\n每次存系統讀取資料很沒效率:\n僅需要再系統啟動時讀取一次進 memory 即可，假設系統沒有穩定儲存空間，則每次都要隨機產生 clock_seq，這會導致碰撞機率增加，應該要盡量避免；如果確定 node id 都不會變，也可以不用保存直接返回即可 system clock 粒度不見得有到 100-nanoseconds:\nSystem Clock Resolution：如果產生頻次不高，則直接將系統時間放大到 100-nano 的粒度即可，但如果系統單一時間產生過多 uuid，實作必須返回錯誤，或是暫停產生，直到系統時間正常，如果要提高粒度，也可以是在同一個系統時間內累計產生的 uuid 個數， 每次要回寫系統資料很沒效率\n只需要定時更新儲存資料即可，將 timestamp 設定在比至今產生的 UUID 使用的 timestamp 大一點，但又不要大到超過 reboot 時的所需要的啟動時間，目的在於降低 clock sequence 重置的機會，在下方的建議實作中是每 10 秒寫入一次 1 2 3 4 5 6 7 if (timestamp \u0026gt;= next_save) { fp = fopen(\u0026#34;state\u0026#34;, \u0026#34;wb\u0026#34;); fwrite(\u0026amp;st, sizeof st, 1, fp); fclose(fp); /* schedule next save for 10 seconds from now */ next_save = timestamp + (10 * 10 * 1000 * 1000); } 跨進程分享狀態很沒效率\n如果跨進程共享狀態很耗資源，可以每個進程切割一塊時間區段個別產生 uuid ，直到時間區段用完才去要新的 4.3. Algorithm for Creating a Name-Based UUID v3 跟 v5 主要是在某一特定的 Namespace 下針對 Name 產生對應的 UUID，有以下特性\n相同的 namespace 相同的 name，不同系統時間一樣有相同的 uuid 相同 namespace 下不同 name，uuid 不同 相同 name 不同 namespace，uuid 不同 如果兩個 uuid 相同，則代表 namespace / name 相同 UUID 欄位則是透過 Name + Namespace 雜湊後的值去派發\n4.5. Node IDs that Do Not Identify the Host 如果 MAC Address 不能使用，有幾種做法能保證 Node ID 的獨一性\n去跟 IEEE 聲請獨立區段的位址，在文件編寫時期價格是 US$550 使用密碼學強度的隨機碼取最低位 47 bit，最高 bit 設定為 1，主要是避開 IEEE 中 MAC Address 的區段 常見做法是在 buffer 中隨機累積一段資料，接著用 SHA1 或 MD5 取 48 bits，然後把最高 bit 設定為 1\n6. Security Considerations UUID 並不保證隨機性，所以不會很難猜，所以不能拿來做跟安全性有關的業務\nDo not assume that UUIDs are hard to guess\n以上大概挑個重點帶過\nuuid.js 實作拆解 以下將閱讀uuid.js github repo的原始碼，在開始看 v1~v5 的實作前，先看一個用於產生隨機數的重要函式 rng.js\n1 2 3 4 5 6 7 8 9 10 import crypto from \u0026#39;crypto\u0026#39;; const rnds8Pool = new Uint8Array(256); // # of random values to pre-allocate let poolPtr = rnds8Pool.length; export default function rng() { if (poolPtr \u0026gt; rnds8Pool.length - 16) { crypto.randomFillSync(rnds8Pool); poolPtr = 0; } return rnds8Pool.slice(poolPtr, (poolPtr += 16)); } 程式碼很短，主要就是產生一個 rnds8Pool 陣列，隨機塞入數值，最後每次回傳 16 bit，如果這一段 rnds8Pool 都回傳了，就在一次產生新的隨機亂數\n這可以保證產生 Generates cryptographically strong pseudo-random data.，來自 Nodejs 官方文件的保證，也是 uuid 不會有高碰撞機率的保證，切記 Math.random 不足夠隨機，拿來使用問題會很多\nUUID v1 實作 以下挑重點說，不得不說作者的程式碼以及註解寫得很乾淨，直接標明實作對應的 RFC 段落\n1 const seedBytes = options.random || (options.rng || rng)(); 先產生隨機數備用，在前面文件介紹中，有用到隨機產生的都會從 seedBytes 中提取\n1 2 3 4 5 6 7 8 9 10 11 if (node == null) { // Per 4.5, create and 48-bit node id, (47 random bits + multicast bit = 1) node = _nodeId = [ seedBytes[0] | 0x01, seedBytes[1], seedBytes[2], seedBytes[3], seedBytes[4], seedBytes[5], ]; } 這裡可以看到，實作中的 node_id是每次啟動時隨機產生，這符合文件 4.1.6，沒有採用 MAC Address 自己亂數產生也可以；\n同時這一段我有特別留一個 Issue 詢問作者，為什麼不照文件規範去拿機器的 MAC Address，他回答到基於隱私問題，而且如果 Node ID 跟 Clock Seq 每次都隨機產生也是符合文件規範的\nI believe that this comes close to the idea of the spec while avoiding the privacy problems that come with trying to derive a stable node ID from hardware.\n1 2 3 4 if (clockseq == null) { // Per 4.2.2, randomize (14 bit) clockseq clockseq = _clockseq = ((seedBytes[6] \u0026lt;\u0026lt; 8) | seedBytes[7]) \u0026amp; 0x3fff; } 沒有 clockseq 就亂數產生\n1 2 3 4 // Per 4.2.1.2 Throw error if too many uuids are requested if (nsecs \u0026gt;= 10000) { throw new Error(\u0026#34;uuid.v1(): Can\u0026#39;t create more than 10M uuids/sec\u0026#34;); } 如果實作者發現短時間內有太大量的 uuid 產生，需要拋出錯誤或是暫停 uuid 生成避免碰撞\n1 2 3 4 5 6 7 8 9 10 11 12 13 // `time_low` const tl = ((msecs \u0026amp; 0xfffffff) * 10000 + nsecs) % 0x100000000; b[i++] = (tl \u0026gt;\u0026gt;\u0026gt; 24) \u0026amp; 0xff; b[i++] = (tl \u0026gt;\u0026gt;\u0026gt; 16) \u0026amp; 0xff; b[i++] = (tl \u0026gt;\u0026gt;\u0026gt; 8) \u0026amp; 0xff; b[i++] = tl \u0026amp; 0xff; // `time_mid` const tmh = ((msecs / 0x100000000) * 10000) \u0026amp; 0xfffffff; b[i++] = (tmh \u0026gt;\u0026gt;\u0026gt; 8) \u0026amp; 0xff; b[i++] = tmh \u0026amp; 0xff; // `time_high_and_version` b[i++] = ((tmh \u0026gt;\u0026gt;\u0026gt; 24) \u0026amp; 0xf) | 0x10; // include version b[i++] = (tmh \u0026gt;\u0026gt;\u0026gt; 16) \u0026amp; 0xff; 產生 time 相關欄位\nv4 實作相當簡單，就是保留 version，其餘塞隨機數； v3,v5 大同小異，所以作者寫了一個 v35.js ，重點大概就這麼幾行\n1 2 3 4 5 6 7 // Compute hash of namespace and value, Per 4.3 // Future: Use spread syntax when supported on all platforms, e.g. `bytes = // hashfunc([...namespace, ... value])` let bytes = new Uint8Array(16 + value.length); bytes.set(namespace); bytes.set(value, namespace.length); bytes = hashfunc(bytes); 把 namespace 跟 value 合起來然後 hash 過，接著就按照文件塞到對應的位置\n後記：作者提交 proposal 給 tc39 作者在 PR 中有說到他提了一個 proposal 給 tc39 proposal-uuid，目前還在 stage 0，希望把 uuid 產生變成 js 的規範，主要是有太多錯誤且粗心的實作，例如使用 Math.random 等，這邊引用一篇非常棒的文章指出為什麼 Math.random 不好 TIFU by using Math.random()，以下將摘錄重點\nTIFU by using Math.random() 文章重點摘要 TIFU =\u0026gt; Today I Fucked Up\n作者公司採用 microservice，但他們希望可以追蹤每個 request 在 service 中交互結果，所以需要有一個全域的 request id，需要一個隨機生成演算法產生足夠隨機的 id\n所謂的足夠隨機包含兩點\n足夠大的 identifier space：有足夠多的組合與可能性 足夠隨機的 identifier generation：有了足夠多的 identifier space，還需要足夠隨機的生成機制 作者決定用長度 22 的 base 64，也就是 space 有 64^22 這麼大，generation 則是用 decent pseudo-random number generator (PRNG) 常見的演算法，V8 即是採用這一套，如果足夠隨機，那這樣的空間足以預計每秒產生一百萬次也要三百年才會碰撞，多麼的美好\n最後作者兜出來的程式碼如下\n1 2 3 4 5 6 7 8 9 var ALPHABET = \u0026#39;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-_\u0026#39;; random_base64 = function random_base64(length) { var str = \u0026#34;\u0026#34;; for (var i=0; i \u0026lt; length; ++i) { var rand = Math.floor(Math.random() * ALPHABET.length); str += ALPHABET.substring(rand, rand+1); } return str; } 看起來一點問題都沒有，也是大家常見的隨機生成做法\n但是在不久後同事發現 ID 碰撞了 💥\n“Anyone who considers arithmetical methods of producing random digits is, of course, in a state of sin.” From John von Neumann 意即要透過數學方式產生真正隨機根本是不可能\nPRNG 實作 簡單看一下偽隨機數的生成方式之一 PRNG (pseudo random number generator) 來自原文的圖片\n簡單來說就是會有一個初始的 Seed，接著按照數學公式算出一個對應的位置，所以只有經過幾次輪轉，所以要用 finite state 產生隨機數是不可能的，只要 PRNG 持續產生，那最終輸出會重複出現 Periodic，作者比遇到， PRNG 就像一本壓縮的密碼本本包含著一串數字，Seed 像是你挑某一頁開始看，接著一路往下翻，到書尾再從書首開始看起，終將輪迴\n不過只要 Cycle 的長度長到在有限時間內不會發生即可，這也決定 PRNG 演算法品質，稱之為 full-cycle generator\nIf a PRNG’s state has a k-bit representation, the cycle length is less than or equal to 2ᵏ. A PRNG that actually achieves this maximum cycle length is called a full-cycle generator.\n良好的 PRNG 會盡可能達到 2ᵏ 上限\n後面有一段再說明 Chrome 當時的 Math.random 演算法錯誤，所以實際上 590 million 就會發生循環，更糟糕的是基於生日悖論，產生僅僅 3 萬次就會有 50% 的碰撞機會 (50% chance of collision after generating just 30,000 identifiers.)\n最後的結論是如果要採用偽隨機生成數請用 CSPRNG(cryptographically secure PRNG)，或是採用系統核心基於外部噪音、網路封包等產生的真隨機數 urandom\n其他 今天路過看到一篇關於 UUID 的好文 閒談軟體架構：UUID，主要更深入探討如果把 UUID 當作資料庫的 Key 對於效能的影響，主目的是希望能達到 分散式產生遞增的 Key，UUID v1 算是有符合這個要求，但因為有做過 timestamp 的拆分，導致 Java 實作在比對時會有點問題\n可以自己客製化 UUID 的格式，或乾脆自創，參考其他實作如 Twitter Snowflake(已 deprecated) 或是 Firebase Push ID，大抵上都脫離不了 timestamp 加上亂數或是加上機器識別碼的做法\n結論 ID 是常用的屬性，用來抽象化指向某個物件/事件，選擇正確的方式產生 ID，才不會對於系統產生效能貧頸，透過學習 UUID 的實作過程，看到分散式產生 ID 的方式，尤其是在大數據時代，快速產生獨一(且遞增)的 ID 尤為基礎且重要，而在之中隨機性在整個過程扮演著很關鍵的角色\n","date":"2020-12-01T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-12-01-uuid-%E5%8E%9F%E7%90%86%E8%88%87%E5%AF%A6%E4%BD%9C%E5%88%86%E6%9E%90-%E8%A9%B2%E5%A6%82%E4%BD%95%E6%8C%91%E9%81%B8%E9%81%A9%E5%90%88%E7%9A%84-uuid-%E7%89%88%E6%9C%AC/","title":"UUID 原理與實作分析 - 該如何挑選適合的 UUID 版本"},{"content":"自己是一個喜歡透過運動賽事增添生活趣味的人，也享受著認識不同運動的魅力，從八月底購入自行車後，就開始找相關的自行車賽事，最後挑選了時間還蠻剛好的環花東 365 挑戰賽，有大概三個月能準備，有點趕但值得拼拼看，網路上的分享大多是車友的比賽騎乘心得，比較少見到備賽相關的經驗，以及能不能在沒有車隊或補給車的情況下完成比賽\n答案是可以的，以下將分享我從八月底到十一月底如何準備，以及比賽這兩天的心路歷程，最後給一些整體的準備清單，也必須說我自己開始騎自行車也不到半年，分享比較適用於跟我一樣的小白，如果專業的車友有不同的建議或指教，也非常歡迎留言\n事前準備 除了自行車之外，有幾項東西覺得要練車應該要準備的\n1. 車錶＋踏頻＋心率感測： 一開始我是用手機開 Strava 紀錄，但真心建議要認真練還是準備車錶，像我自己用 Bryton 320 覺得續航達 30 個小時真很方便，也不用心疼手機一路震動怕壞掉；\n踏頻的推薦是因為一開始騎車抓不到感覺，看網路教學建議維持在 100 以上的踏頻，避免傷膝蓋；\n心率帶是因為耐力運動最好是維持在最大心率的 70% 上下，最大心率的公式約為 206.9 - (0.67 x 年齡)，像我自己是抓 150，平常練習就盡量維持，最大不要超過 165 避免太早爆掉反而撐不久，我覺得練習了一陣子心率帶比踏頻更重要 買一套 Bryon 3000元上下就很夠用了\n2.車衣、車褲 一開始有穿過便宜的車褲，騎沒多久屁股就會麻麻的，買了 Monton 車褲覺得舒適很多，如果騎了不舒適會影響騎乘的意願，就更別說要比賽了，一件約 2000 元還算蠻值得的 車衣的部分，我平常也會穿一般的排汗衫，但專門的車衣最大特點在後方有口袋，要補給就非常方便，比賽的時候很好用，也蠻推薦\n3.防雨外套 這我個人沒有買，但真心覺得應該要買，因為比賽兩天都下雨，有防風防雨的外套會讓騎乘更加舒適安全，而且早上六點出發還很冷，沿途的車友五成以上都有買\n4. 能量膠 一開始是因為平日練習到一半會肚子餓才嘗試購買，但發現真的很好用，甚至可以說是必備的用品，我是買 SIS 能量膠，口味沒這麼甜主打可以直接吃不用配水 / GU 的我也有買，除了啤酒口味超雷其他的都不錯，他比 SIS 更濃，重量只有一半但提供相同的熱量，需要配一口水 基本上我平常練習帶一包，騎乘約 40 ~ 50 分鐘吃一包，比賽時身上帶 5~6 包備用\n備賽 八月底開始，每週至少騎三次，兩次挑平日早上，從三重沿河濱來回南港約 60 公里，約 2小時出頭完成 假日挑一天騎長距離，有騎過三重翻過風櫃嘴來回金山(約85公里但蠻多山路)、三重延河濱往淡水到石門再折返(平路總計約100公里)，連續騎乘時間都在五個小時左右\n自己騎乘的實力大概是從楓林橋上風櫃嘴33分，看 Strava 算蠻一般的成績，就提供給大家參考，不過以上的練習不太夠，推估可能一週練 250 公里，持續四個月備賽會讓比賽好過一些\n交通與住宿 因為沒有隊車，所以要自己想辦法到花蓮，我後來選擇搭火車到花蓮，到了當地在租汽機車代步，在網路上買一個 500 元的攜車袋，拆除前後輪就可以上路，過程比想像中簡單，包好的腳踏車就算是一般的行李，沒有其他的班次限制，除非是沒有拆的才要特別找專門列車喔\n住宿部分花蓮站的起點在亞士都飯店，離花蓮火車站不遠，建議住市區就好，餐飲解決也比較方便；\n台東的起點在娜路彎飯店，不得不說我一開始以為也在台東火車站附近，結果相差超遠 XD 而且台東火車站附近沒什麼東西，離市區還有一小段距離，就不太建議住在台東火車站附近，因為第一天比賽比完要回民宿，騎太遠腳超級酸，還要再出來覓食真的累 OTZ\n另外台東住宿建議問一下能不能沖洗自行車，第一天比完車子應該都蠻髒，我有準備清潔用品，洗乾淨重新上油迎接第二天的挑戰\n比賽兩日 抱著忐忑的心，因為自己也沒有真的騎乘超過 150 公里的經驗，在沒有車隊、補給車，甚至還不會補胎(只有帶工具並隨時準備看 Youtube 教學XD)的狀況下，自己真的能完賽嗎？\n環花東 365 有分自行車賽跟挑戰賽，要記得報名挑戰賽喔，當初一直看想說怎麼路線不一樣，才發現自行車賽是專業在比的，不要搞混囉\n比賽前一天會去亞士都飯店報到，下午五點會有說明會，簡單致詞與說明交通路況\n11/28 第一天: 花蓮到台東 六點從花蓮亞士都飯店，沿著海岸走台 11 線一路殺到台東，途中會翻過海拔兩百的牛山，接著就上上下下的丘陵地，據說以往的環花東都在四月舉辦，今年因為疫情延到十一月，順著東北季風一路往南殺，一開賽看著大家平路時速都飆 40 公里，害我也跟著衝，看到心率還維持在 150 左右，才發現強勁的季風真的很給力\n一路上路況都不錯，汽車也不算多，蠻多路段都有機慢車道所以騎乘蠻順暢，在 60 公里處開始下雨，無奈自己沒有雨衣，就頂著雨繼續衝，到了 80 公里處吃午餐，此時才早上九點，遠比預期的快\n吃飽後繼續淋雨，在順風的情況下，大家都三兩成群的騎，因為第一天體能充沛，又有淋雨怕身體冷掉所以每一站都有進補給，但吃一吃上個廁所就趕快繼續騎，建議可以在補給站帶一塊香蕉在身上吃，我自己的補給策略是 每個補給站都進，路上騎乘約 50分鐘吃一包能量膠，路上有車就跟，也不用不好意思就蹭在後面，保持安全車距；\n跟車時速度快可以換大盤，踩個幾下後休息，不要用輕齒比狂踩，保持車距不拉超過三個車身即可，要抓緊休息的節奏，這是好心的大哥路過教我的，這樣輕鬆了許多\n前面腳都沒什麼問題，大概一路到 130 公里處天氣放晴，但大腿也開始有疲勞性痠痛，一路上獨推時就放慢速度，等到有差不多時速的集團出現就跟上，跟到不行在放掉休息，一路這樣到終點，完成第一天的 170 公里，總完賽時間約六個半小時\n不得不說一路上大家都很拼，下大雨下坡照樣飆 4,50 沒在怕的 XD\n最後成績是 209 / 1395，比預期的好 :P\n11/29 第二天: 台東到花蓮 第一天的比賽過稱堪稱爽快，有順風可以飆，後段也有克服痠痛的意志過程，但第二天的比賽就真的是進入地獄了，在一開始熱身段身體還帶著昨天的疲勞，膝蓋與小腿還有點緊繃的痠疼感，這才不到 1/10 的路段啊，咬著牙告訴自己多撐一段在放棄也不遲\n風神只眷顧了昨天，今天回程轉變成大逆風，不同於昨天三兩成群，這一天大家乖乖排成一直線，由勇者破風帶領大家前行，處於隊伍中央真的省力非常多，路上車友也都很客氣，要插入隊伍比個手勢大家都很樂意互助\n跟到不行就慢慢獨推，等下一波集團再繼續跟，但這一天的狀況比第一天糟，溫度下降了兩度，同樣飄雨，只是加上第一天的疲憊，後來決定除了補給站，遇到 7-11 也進去坐一下吃個巧克力補一補，大概每 10 公里就會肚子餓了，不像第一天急著想要完賽，第二天就只有能夠完賽就是英雄的念頭XD\n回去的路線大多走台9線，騎在花東縱谷上看著兩岸的山脈也是頗有趣，前期要先上鹿野高台再往下溜，接著到瑞穗爬一個陡坡到北回歸線景點吃補給，接著就繼續上上下下的丘陵路段回花蓮市區\n小小抱怨一下這一天午餐安排在 125 公里處，在加油站旁邊臨時搭棚子，但因為下雨，排隊等著領炒麵時還要淋雨，感覺有點不好\n堅持了八個小時多才完成比賽，可惜車錶到後面當機沒有完整記錄到 QQ 最終成績是 439 / 1206，第二天沒力果然掉很多 XD\n後來女友搭上火車，看到有一些車友上火車坐個幾站再繼續騎，會覺得好像怪怪的，但我覺得如果身體撐不住，這樣倒也是個做法，無關乎毅力或是鬥志等等，顧及到自己的生命，盡可能地做到最好就好，挑戰賽本來就意在超越自己，致敬所有完賽的車友們，也慶幸自己一路平安的完賽\n結語 如果能有車隊真的很不錯，看到沿路大車隊都有滿滿的補給車，真的好棒啊，因為大會安排的補給點有時候距離不等有點頭疼；\n沿路還有認識的車友互相加油打氣是一件很棒的事，但這也不代表一個人就無法參加，大會名冊上約有 1600 人報名，其中約有兩三百人也都是個人身份，自己也在這樣的狀況下完賽，希望分享能夠給其他想參加比賽的車友一些參考\n","date":"2020-11-30T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-11-30-%E7%92%B0%E8%8A%B1%E6%9D%B1365%E6%8C%91%E6%88%B0%E8%B3%BD%E5%88%86%E4%BA%AB-%E5%A6%82%E4%BD%95%E4%B8%80%E4%BA%BA%E5%82%99%E8%B3%BD%E8%88%87%E6%AF%94%E8%B3%BD%E9%81%8E%E7%A8%8B/","title":"環花東365挑戰賽分享-如何一人備賽與比賽過程"},{"content":"在系統設計中，我們常常需要檢視某一個值是否出現過，例如遊戲中用戶 ID 是否已經註冊過等等，如果把每個 ID 都存成一張表每次去檢視，會需要很大量的記憶體 ( ID 大小 * ID 數量)，如果是像 Google 這類有上億的用戶，光是帳號重複檢查可能就需要 1,20 GB 的記憶體空間；\n以下將介紹，Bloom Filter 為什麼可以犧牲一點準確性就能節省大量的空間\n原理 Bloom Filter 原理其實很簡單，產生一個陣列，用 bit 代表該元素是否出現過，透過 Hash function 將輸入專換成陣列位置，藉此標記與查詢是否元素出現過\n因為 Hash 會有碰撞問題，所以會有 False Positive 但不會有 False Negative 意即 Bloom Filter 回答元素已存在但實際上沒有存在， Bloom Filter 回答不存在則一定不存在\n原理很好懂，但複雜的是陣列要多大? 要選幾種 Hash 才能平衡記憶體用量以及避免 False Positive 的錯，這一個部落格用數學證明 Bloom Filters - the math\n假設我們決定用一個長度為 m 的陣列，陣列的元素是一個 bit 表示該鍵值已出現過 當今天增加鍵值時，經過 Hash 會隨機分配陣列中的一個位置給該鍵值，換句話說陣列的某個位置被插入的可能性是 1/m 今天假設插入了一個鍵值，那第二個鍵值與第一個鍵值碰撞的機率是 1/m (好死不死分配到同一個陣列)，也就代表不會碰撞的機率是 1 - 1/m 今天假設插入了兩個鍵值，那第三個鍵值不會碰撞的機率是 (1 - 1/m)^2，如果插入了 n 個鍵值，那第 n + 1 個不會碰撞的機率是 (1 - 1/m)^n 以上是 Hash 數量為 1 的情況，接下來考量獨立 Hash 為 k 的情況\n因為 Hash 數量為 k ，所以每一輪陣列會有至多 k 個位置被標記成 1，需注意 Hash 之間也可能標記到同一個位置，所以在機率上是獨立事件，所以是某位置在插入後不會被選中的機率是 (1-1/m)^k，也就是每次 Hash 後的值都沒有選到他 所以插入 n 個鍵值後，第 n + 1 個不會產生碰撞的機率是 (1-1/m)^(k*n) 換算一下，會產生 False Positive 的機率是 (1 - (1-1/m)^(k*n))^k，也就是第 n + 1 個鍵值與任一一次 Hash 後產生碰撞的機率，簡化後可以變成 (1 - e ^ (-k*n/m))^k 從這個公式可以看出\nm 越大，False Positive 的機率就越小，這也蠻直觀的，因為 Hash 後產生的碰撞機率自然就變小；\n但是 k 的值就比較沒這麼直觀，不是越大越好，也不是越小越好，而是在 m,n 在對應的比例下，會有一個最剛好的值\n我們可以用 Bloom Filter Calculator 手動調整參數，去評估預期的 False Positive 機率與所需要耗費的記憶體空間 (m 的大小)\n基本上，如果 m 是 n 的 10 倍，在選擇 4 個 Hash function 下 False Positive 機率約為 1.2 %，如果選擇 5 個 Hash function 則機率降為 0.9 %\n一開始在思考時，會想說明明增加 Hash function 的數量應該會增加碰撞機率才對，後來才想到前提是 m \u0026gt;\u0026gt; n 的時候，有足夠的多餘空間讓多個 Hash function 的整體碰撞機率更小\n變形 - 增刪鍵值 如果今天要刪除鍵值時順便更新 Bloom Filter，就不能用 boolean 儲存，而是要用 unsigned integer，增加時 + 1 移除時 - 1，那 integer 需要 4 bit / 8 bit 還是多少個 bit 才足夠呢 ?!\n同一篇部落格文中，同樣有數學推導，但因為不太了解就先略過，最後的結論如果 Hash function 足夠隨機的話 4 bit 應該已經足夠，但需要小心如果非常非常不幸 4 個 bit 不夠儲存，會產生 False Negative，也就是 Bloom Filter 回答不再但實際上還是存在的狀況\n實作 實作部分，可以拆成兩個步驟\n如何產生 k 個獨立的 Hash Function 實作插入與查詢的 bitwise 操作 如何產生 k 個獨立的 Hash Function 如何產生一個運算快速、足夠隨機且 Universal 的 Hash function 是非常關鍵的一步，影響後續的錯誤率，選擇用 non-cryptographic hash function 即可，強度不高但是性能夠好，差別在於 hash 後被逆推的可能性(沒有強抗碰撞與弱抗碰撞的保證)，可能會遭遇 HashDos，被發現碰撞後就不斷嘗試導致性能變差\n回歸正題，爬了一些 Bloom Filter 的實作，有發現使用 xxHash、MurMurHash 等已知的 hash function library，這一個實作蠻有趣 bloomfilter.js，Hash function 是實作 Fowler–Noll–Vo(FNV) hash function，程式碼如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 function fnv_1a(v, seed) { var a = 2166136261 ^ (seed || 0); for (var i = 0, n = v.length; i \u0026lt; n; ++i) { var c = v.charCodeAt(i), d = c \u0026amp; 0xff00; if (d) a = fnv_multiply(a ^ d \u0026gt;\u0026gt; 8); a = fnv_multiply(a ^ c \u0026amp; 0xff); } return fnv_mix(a); } // a * 16777619 mod 2**32 function fnv_multiply(a) { return a + (a \u0026lt;\u0026lt; 1) + (a \u0026lt;\u0026lt; 4) + (a \u0026lt;\u0026lt; 7) + (a \u0026lt;\u0026lt; 8) + (a \u0026lt;\u0026lt; 24); } // See https://web.archive.org/web/20131019013225/http://home.comcast.net/~bretm/hash/6.html function fnv_mix(a) { a += a \u0026lt;\u0026lt; 13; a ^= a \u0026gt;\u0026gt;\u0026gt; 7; a += a \u0026lt;\u0026lt; 3; a ^= a \u0026gt;\u0026gt;\u0026gt; 17; a += a \u0026lt;\u0026lt; 5; return a \u0026amp; 0xffffffff; } 在 Stack Exchange 看到有趣的問答 Which hashing algorithm is best for uniqueness and speed?，有強者比較多種 Hash Function 的效能以及碰撞機率，FNV-1a 表現還不錯，但作者比較推薦 Murmur2\n接著，有另一篇論文 Building a Better Bloom Filter 證明如果要產生多個 Hash function 應用在 Bloom filter 上，只需要產生兩個 Hash function 再加上係數的組合出其他的 Hash function 即可\n1 gi(x) = h1(x)+ ih2(x) mod p // p 是質數 總結 如果原本的鍵值很長，再容忍一定的 False Positive 下使用 Bloom Filter 可以節省非常大量的儲存空間，但需注意 Hash 運算會多一些 CPU 資源\n","date":"2020-11-17T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-11-17-sketch-data-structure-bloom-filter-%E4%BB%8B%E7%B4%B9%E8%88%87%E5%AF%A6%E4%BD%9C/","title":"Sketch Data Structure - Bloom Filter 介紹與實作"},{"content":"共識演算法主要應用於分散式系統中，一個集群中有多個節點組成，讓每個節點都維護相同的狀態，例如說在多種系統中都需要集群有單一個 Leader 存在，所有節點都必須承認這一個 Leader，否則多個 Leader 可能會導致 Split brain 等資料不一致等問題；\n但如何讓節點狀態一致是一件不簡單的事情，要考慮到節點可能失敗 / 網路封包延遲等等\nRaft 演算法是由史丹佛大學的教授所提出，他在影片中提到過往的共識演算法 Paxos 過於複雜，世界上真正了解的人沒有幾個，市面上的實作也分歧出非常多的實作版本，理論太過艱深導致實務上有很大的落差\n所以他在設計 Raft 的一個核心理念是好懂，他在 Conference 上也僅用約 10 分鐘就大致介紹完 Raft 的原理，整份論文也才 16 頁\n以下將整理影片介紹與論文摘要，探討 Raft 如何在分散式系統中讓多節點在容忍錯誤下達到強一致性\n影片介紹 先從在 Conf 上的介紹影片快速理解，Raft 由三個部分組成\nLeader Election:\n整個集群中票選出一位 Leader Log Replication:\nLeader 負責接收 Client 的指令，並把狀態同步到多數節點上 Safety:\n當 Leader 要重新票選時，確保擁有最新資料的節點才能當上 Leader，避免確認過(commited)的資料被取消 以下將摘要論文 《In Search of an Understandable Consensus Algorithm (Extended Version)》部分內容\n論文摘要 Raft 是一種用於管理副本紀錄的共識演算法，效果類似於 Paxos，但結構上完全不同，這也使得 Raft 相較於 Paxos 更容易了解\n為了增加可讀性，Raft 解構出幾個共識演算法中關鍵的元素，像是 Leader Election / Log replication / Safety，並透過減少狀態達到更強的凝聚性 (意指節點可以變化的狀態少，就更容易達到一致)\n1. Introduction 共識演算法 (Consensus) 提供由機器組成的集合可以運作類似同一體並能夠容忍部分成員出錯，也這是這些特性，共識演算法在建構大型軟體系統是很關鍵的角色\nPaxos 主宰這一塊近十年，但不幸的是 Paxos 很難懂，同時在實務上需要有很負責的設計才能夠使用，所以不論是學生還是系統工程師都十分困擾\n所以作者開始想要設計一門足夠好理解、同時對於系統工程師也容易實踐的共識演算法\nour primary goal was understandability: could we define a consensus algorithm for practical systems and describe it in a way that is significantly easier to learn than Paxos\n透過解構出獨件以及減少機器在不一致狀態的可能性，讓整個演算法更容易理解\nRaft 有以下幾個特點\nStrong Leader:\nLog 的寫入必須經由 Leader 再到其他節點上，這樣可以簡化很多不必要的複雜性 Leader Election:\n在選舉時，Raft 利用隨機延遲(randomized timers)方式快速且有效解決可能的衝突 (避免大家在同一個時刻想要變成 Leader) 身份改變\n在改變節點的設定檔時，可以將前後兩種設定檔以聯集方式合併(joint consensus)，在更改設定同時還能維持服務 作者認為 Raft 是個優異的共識演算法，就讓我們繼續往下看\n2. Replicated state machines Replicated state machines 是指一群機器的集合，可以運算出相同的狀態，並在少數節點失敗時還能正常運行，主要用來解決分散式系統中各種錯誤容忍性的問題，例如 GFS / HDFS / RAMCloud，在這些系統中有獨立的 Replicated state machine 掌管 Leader Election / 保存 Config，在 Leader crash 情況下還能繼續運作； Replicated state machine 的實際案例有 Chubby / ZooKeeper\nReplicated state machine 通常是由複製 Log 所實踐，如下圖，每個 Server 都有一份 log，依照相同順序紀錄著相同的指令，運算整份 log 就能得到相同的狀態機 (state machine)\n從 client 收到指令後，如何保持順序複製相同的指令到每台機器上，就是共識演算法的任務，共識演算法具體提供下列保證\nSafety:\n在非拜占庭情況下，系統即使遭遇網路延遲、網路分隔 (partition)、封包遺失、指令重複發送、發送順序改變等問題，都不影響結果 只要多數節點存活就能夠正常運行，例如 5 個節點 3 個還活著就可以，並且失敗的節點可以在後面重新加回集群中 不依賴時間當作判斷，在分散式系統中時間是不可信的(除非學 Google 用原子鐘自幹出 TrueTime )，每個系統都存在著時鐘沒對齊的可能 多數節點有回應收到副本就算成功，少數節點晚回覆不會造成性能影響 3. What’s wrong with Paxos? 這一章節主要在描繪 Paxos 的背景與有多難理解的原因，但因為之前沒有學過 Paxos，就先略過這章\n4. Designing for understandability 這一章節主要是作者不斷強調 understandability 是他們的核心理念，當遇到設計有多種方案選擇時，他們會選擇最好解釋給別人聽的那一個，對於他們來說可讀性是核心理念\n具體上，透過 decomposition 拆分獨立模組以及減少不確定性與狀態可能性 達成目的，但在某一些部分還是有用上隨機性，因為這讓演算法更好理解\n5. The Raft consensus algorithm Raft 在實作上會先選出一名 Leader ，由 Leader 管理 log 的複製到其他節點的狀態機上，透過 Leader 的好處是不需要其他節點同意 Leader 能獨自決定新的 log 要儲放的位置；\n如果 Leader 失敗了可以再選新的 Leader 出來\n5.1 Raft basics 通常 Raft 集群會由五個節點組成，可以容忍兩個節點失敗，而每個節點有三種狀態 leader / follower / candidate，通常情況下是一個 leader 其他人都是 follower\nfollower: 被動的處理來自 leader 或 candidate 的請求 leader: 負責所有 client 的 request，並複製指令到 follower 中 candidate: follower 發現沒有 leader，設一個隨機 timeout 切換成 candidate 模式，準備要選新 leader 下圖為狀態機示意圖 Raft 將時間切割成 回合 (term)，每一個回合代表著一次的選舉，也就是 candidate 去競選 leader 的過程，如果成功推選出 leader 後，則每個節點紀錄這一個回合數；如果選舉失敗，則開啟新的回合直到有人成為 leader\n以下為概念圖 回合本身是一個遞增數值，用來表示邏輯上的時間概念，有可能節點觀察到的回合跟其他節點不同，如果回合數小則代表自身的資料過時，他必須更新自己的回合數；\n例如說原本的 leader 可能斷線，其他節點推選出新的 leader 則會進到下一個回合數，原本的 leader 回歸後發現自己的回合數比較小，則會主動變成 follower；\n如果節點收到請求時，發現回合數比自己小，則代表請求過期，直接拋棄該請求\n5.2 Leader election Raft 透過 heartbeat 觸發 leader 選舉，當 leader 選上時，會在固定時間內發內容為空的 AppendEntries RPC 當作心跳包，如果 follower 超過 election timeout 沒有收到心跳包，則進入選舉階段\nfolower 會將現今的回合數加一便轉為 candidate，並請求其他 follower 投票給他 RequestVote RPC，遇到以下情況 candidate 才會改變狀態\n贏得選舉 其他節點成為 leader 超過一定時間都沒選出 leader 贏得選舉 如果 candidate 拿下過半的票數，則成為新的 leader，每一個 follower 在同一個回合數下只會投票給請求先到的 candidate，這避免無效選舉的發生\n如果 candidate 成為 leader，則開始發送心跳包\n發現有其他 leader 如果在 candidate 階段收到心跳包(AppendEntries)，則代表有其他 leader 產生，candidate 會去比對回合數至少要大於等於他自身的回合數，如果是則轉成 follower；\n反之則繼續維持 candidate\n超過一定時間都沒選出 leader 超過一定時間還是沒有選出來的話，timeout 後開始下一輪新的選舉\nRaft 透過在一定時間內隨機 timeout (150-300ms)，避免所有的 follower 同時進入選舉階段，這樣能加速 leader 的推選，後續會有更近一步的說明\n在設計過程，作者曾考慮加入 rank ，rank 較高者更有機會成為 leader，但發現這會導致演算法設計更加複雜，且有可用性的問題，例如高順位 candidate 發生狀況，則低順位 candidate 要多一次 timeout 才能當上 leader 等問題\n5.3 Log replication Leader 會透過 AppendEntries RPC 將指令同步到 follower 並回傳執行結果給 client，如果 follower 此時 crash 等，leader 會持續送直到 follower 狀態同步\nLog 是以 回合數 + 指令 的方式依序儲存，主要是檢視是否有不一致的狀況產生\n每個節點都會保存現今最後一個 commited log 的索引 (commitIndex)，Leader 在同步 log 時，會紀錄 log 要寫入的位置 (commitIndex + 1)，並同時發送給 follower (leaderCommit)，等到多數的 follower 都寫入 log 後才標記成 commited，Raft 保證 commited log 是已經持久化且最終每個 follower 都會達到一致性\n因此 Raft 保證 Logs 有以下特性\n如果任意兩個節點的 log 有著相同的 index 與回合數，則他們必定儲存相同的指令 如果任意兩個節點的 log 有著相同的 index 與回合數，則該指令先前的 log 紀錄都必定相同 第一點保證節點儲存 log 後就不會再被改變，第二點則確保當 follower 發現自己的 log 跟 leader 不同時，可以依此重新跟 leader 同步紀錄\nLeader 會針對每一個 follower 維護指針 nextIndex，用來記錄 follower 目前需要同步的 log 索引，leader 在發送 AppendEntries 指令時，會夾帶最新 committed log 的回合數與 index，讓 follower 可以比對 log 是否同步，如果 follower 在自己的 logs 中沒有找到對應的紀錄，則代表兩者非同步，回傳失敗，接著 Leader 不斷遞減 nextIndex 直到 follower 找到兩者最近一次同步的紀錄，接著開始一步步同步紀錄\nleader 與 follower 找到最近一次同步紀錄的方式，可以優化成 follower 回傳這一個回合數下他所以紀錄的索引，leader 直接倒回這個回合開始同步；\n但作者認為不一致狀態應該很少發生，優化的效益不大\n透過這樣的 log 同步設計，Leader 在剛啟動時不用擔心太多同步的問題，利用 AppendEntries 的成功與失敗去調配 follower 儲存的紀錄，leader 也不用去刪除或更新自身的 log，讓整個同步的過程更加的簡單\n5.4 Safety 先前介紹了 leader election 和 relicate log，但僅有這樣的機制是不夠的，試想如果 leader 發生錯誤，今天有一個 follower 僅包含部分的 commited log，選上 leader 後便會複寫原本其他已經 commited 的 log，這樣就不能保證一致性與持久化\n這一章介紹 Raft 在 leader election 加上限制後，如何達到確保每一台狀態機都以相同順序保存相同的指令\nRaft 會確保任何時候集群都符合上述條件\nElection Safety: 每一輪選舉至多只選出一位 leader Leader Append-Only: leader 從不刪除或改寫自己的 log，只會一直增加 Log Matching: 任兩份 logs 在同一個 term 同一個 index 上，則 log 內容必定相同，且先前的 log 也都相同 Leader Completeness: 某個 log 在任一一個 term 中認定 commited，則後續 term 中的 leader 都必須有該份 log State Machine Safety: 如果某個 server 在某 index 上的 log 套用至狀態機，則不會有其他的 server 在同一個 index 上套用不同的 log 5.4.1 Election restriction 在 leader-based 的共識演算法中，leader 最終會儲存所有的 commited log，在某些演算法中，leader 在沒有保存所有 commited log 情況下也能夠檔選，並透過額外的機制去回補這些未同步的 log，這增加了蠻多的複雜性\nRaft 確保 leader 必須是由 擁有大多數節點同意的最新 committed log 的 candidate 才能當選，確保 leader 一定是擁有最新 log 的節點，只負責增加新的 log，讓資料流只有一個方向，省去其他的麻煩\n在 RequestVote RPC 中增加了限制，RPC 中夾帶 candidate 最後一個 log 中的回合數與索引數，如果 follower 發現 candidate 的紀錄比自己舊，則回傳失敗\n5.4.2 Committing entries from previous terms 先前提到，Leader 會等多數的節點儲存 log 才標記成 commited， 但如果 leader 將 log 寫到多個 follower 後，還來不及收到 commited 就 crash 了\n此時新的 leader 會持續同步 log，但無法確認先前的 log 是不是被 commit 了，因為只有先前的 leader 才能確認 log 已經被多數的節點所保存\n如以下圖示 S1 一開始是 leader，同步 term2 到 S2 就 crash S5 接著當 leader (S3,S4 可以投給他) 開始了 term 3，此時發生 crash S1 又回來當 leader，將 term2 的 log 同步到 S3 後，此時又 crash 接著拆兩種情況\nd. S1 來不及同步 term4 資料，則 S5 有機會當入 leader，並用 term3 複寫掉其他資料\ne. S1 同步 term4 資料到大多數節點上，則 S5 無法當上 leader\n在 leader 還沒收到 commited 情況下，即使多數的節點已經同步 log，但新的 leader 有機會複寫\n為了減少這樣的情況，Raft 不去計算先前 log 所同步的副本數去判定是否 commited，只有 leader 當下的回合數是透過副本的計算來決定 log 是否被 commited，如果 commit 後，則先前的所有 log 都被視為 commited，降低計算上的複雜性，並因為之前的 log 特性保證，先前的 log 一定也都被複製到其他副本上\n5.4.3 Safety argument 更進一步解釋 Leader Completeness，透過反證法，找出集群無法不遵守 Leader Completeness\n假設 leaderT 代表在 term T 的 leader，leader U 則是 term U，且 U 代表是 T 的下一位，且 leaderT 所認定的 commited log (logT) 在 leader U 不存在 (違反 Leader Completeness)\nleaderU 在當 follower 時並保存沒有 logT leaderT 將 logT 同步到多數節點上，而 leaderU 在選舉時獲得多數同意，也就是至少有一位投票節點 (voter) 收到 logT 同時又投票給 leaderU voter 必須先保存 logT，才又投票給 leaderU，反過來就不會有衝突 (term U \u0026gt; term T 所以 logT 後到會被丟棄) voter 依然保存 logT，因為 leader 從不改變既定 log，而 follower 也只有與 leader 衝突時才會複寫 製造出場景後，讓我們來看為什麼 Raft 不可能達到這樣的狀況\n5. 依據先前規定， follower 只會投給 log 保留比自己更多的 candidate\n6. 如果 voter 與 leaderU 最新一個 term 都是 T 的話，則 leaderU 的 log 數應該與 voter 相同，也就是 leaderU 至少擁有 voter 所擁有的 log\n7. 反之，如果 leaderU 的最新 log term 大於 voter 的話，為了要符合早於 leaderU 的 leader 所有 commited log 都應該被保留到 leaderU 中，因此根據 Log Matching 則 leaderU 應該要儲存這些 log\n在無法達成的情況，證明了 Raft 滿足 Leader Completeness 條件，確保 leader 如果最新的 log term \u0026gt; T，則 leader 必定擁有 term T 所 commited 的一切 log\nThus, the leaders of all terms greater than T must contain all entries from term T that are committed in term T\n有了 Leader Completeness 屬性，就能進一步證明 State Machine Safety，如果某一機器在某一 index 套用指令到狀態機中，則其他機器在相同 index 下並然套用相同的指令，因為能夠被套用的 log 一定是 leader commited 後的 log，且因為 Log Completeness，所有更新的 leader 也都保留被 commited 後的 log，因此能確保所有的節點終將以相同的順序套用相同的指令到狀態機上\n5.5 Follower and candidate crashes 先前都是討論 leader crashed 後的處置，至於 follower 與 candidate 則單純很多，因為 Raft 的 RPC 指令都是 idempotent，leader 可以持續的送指令直到成功，follower 跟 candidate 失敗後重啟就重新接收指令就好\n5.6 Timing and availability Raft Safety 建立在不依賴 timing，系統不會因為訊息發送的快慢而得到不預期的結果；\n但整體系統的可用性卻還是跟時間有一些關聯，例如說 server 不能再 election timeout 期間內票選出 leader，而 Raft 在沒有 leader 的情況下就不可用\n所以整體上要符合以下不等式 Raft 才能運作正常\nbroadcastTime ≪ electionTimeout ≪ MTBF\nbroadcastTime 代表 RPC 完整 request / response 所耗費的時間，須小於 electionTimeout 一個量級，通常在 0.5ms ~ 20ms； electionTimeout 則是 follower 等待多久後會觸發選舉，這是可以主動去設定的，通常在 100ms ~ 500ms； MTBF 則代表 server 進入錯誤狀態的區間，通常是數天至數個月 試想如果 broadcastTime \u0026gt; electionTimeout，則每一次選舉在還沒收到投票結果又開始下一輪選舉，則永遠選不完；\n如果 electionTimeout \u0026gt; MTBF，則選舉還沒結束 server 又 crash，那也一樣會有 leader 無法產生的問題\n總結 後續還有 Cluster membership changes / Log compaction / Client interaction 進階探討，就先暫時略過，僅理解前半部演算法的核心設計\n分散式系統的迷人之處在於複雜，需要面對網路延遲 / 時間不一致(time skewed) / 機器失敗等不穩定的因子，「再不穩定的條件下架構出可容錯/高可用/一致性的穩定系統」，看似荒謬卻可以透過演算法的設計達成目的(或是說更貼近)，實在令人讚嘆這些設計演算法的專家們\nRaft 透過由 Leader 主導 Log 的同步，並加入選舉時的條件限制，確保 commited log 不會被改寫達到 強一致性；如果 Leade 失敗，也不用擔心 log 發生問題，等下一位 Leader 被推選出來又能夠繼續維持系統運作\n整份論文讀起來還算蠻好理解的，確實符合作者不斷強調可讀性的重要\n","date":"2020-11-03T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-11-03-raft-%E6%BC%94%E7%AE%97%E6%B3%95%E4%BB%8B%E7%B4%B9%E8%88%87in-search-of-an-understandable-consensus-algorithm%E6%91%98%E8%A6%81/","title":"Raft 演算法介紹與《In Search of an Understandable Consensus Algorithm》摘要"},{"content":"Gossip Protocol Gossip Protocol 是一種通訊機制，應用於同一網路內機器與機器間交換訊息使用，原理類似於辦公室傳謠言一樣，一個傳一個，最終每一個機器都擁有相同的資訊，又稱 Epidemic Protocol\n上一篇分享到 Cassandra 內部如何使用 Gossip Protocol，影片中有推薦 Efficient Reconciliation and Flow Control for Anti-Entropy Protocols，以下摘要此篇論文所探討的內容\n建議可以先讀上篇，有個概略認識後在看理論會比較好懂些\n《Efficient Reconciliation and Flow Control for Anti-Entropy Protocols》 摘要 anti-entropy，又或稱作 gossip，用於不需要強一致性的狀態同步，在一些限制下，時間複雜度是 log(N) (N 為群體數量) 且在 host 遭遇錯誤或是訊息丟失都不影響\ngossip 希望盡量在可控制的回合內完成同步，但如同其他同步操作，這會仰賴 CPU 資源與 Network 流量，在高負載下 CPU 可能來不及運算要更新的狀態或是網路流量不夠快導致延遲封包\n這份論文主要提供兩個價值，\n在限定 CPU / Network 下優化 gossip protocol 傳輸效率 分析 gossip protocol 的流量管制 gossip protocol 主要有兩種類型\nanti-entropy: 持續傳送 gossip information 直到全部資料都更新完成 rumormongering: 選定一個足夠有效的時間持續送 gossip information，大概率節點都會拿到最新資訊 假設目前的集群 {p,q \u0026hellip;}，每個參與者都需要維護一份列表，這個列表是由 key -\u0026gt; (value + version) 組成，也就是 Cassandra 內的 ApplicationState\nσ ∈ S = K → (V × N ) // σ 代表取狀態 σ(k) = (v, n) ，表示 key 此時對應的 value v 跟 version n\n列表包含 key -\u0026gt; value -\u0026gt; version，如果是這個 key 的最新資料，則他的 version 會大於舊的 version\nσ1(k) = (v1, n1), σ2(k) = (v2, n2) // σ1(k) 代表這一個節點取他的 key，返回 (v1, n1) 代表 value 為 v1 且 version n1； σ(k) 表示取 σ1 與 σ2 取 XOR，並遇到相同 key 時取 verson 較大者，也就是如果 n1 \u0026gt; n2 則 σ(k) = (v1, n1)\n操作流程大致是\n從集群中隨機挑一個 host 傳送訊息，訊息內容是自己所維護的列表 收到訊息後運算，此動作稱為 merge 或稱為 reconciliation ，也就是收到訊息時會去運算列表的差異，並保留差異中 version 較高的 key -\u0026gt; value ∀r : µq (r) = µq (r) ⊕ µp(r) // q 真正要更新的是 p 傳來的訊息與 q 自身現在的訊息取 XOR 找出差異處\n傳送訊息有三種格式\npush: 節點 p 傳送整份列表，節點 q 收到則計算 merge 後合併入自己的列表 pull: 節點 p 傳送 key -\u0026gt; version 而沒有 value，節點 q 回傳節點 p 須要更新的鍵值，變免多餘的值傳送 push-pull: 就像 push，節點 p 傳送完整列表，節點 q 會回傳 p 過期須要更新的鍵值 ( pull 後半段，push-pull 是最有效率的做法 如果某個 key 不再更新，那在一定的時間內很高的機率大家都會同步相同的 value，如果集群隨機挑選節點的演算法 (Fp ⊆ P − {p}) 夠隨機的話，即使遇到 message loss 或是 host 短暫 failed，也僅僅是稍微延遲同步的時間\n假設 update key 的時間是固定的，那隨著集群數量線性增長 N，則達成同步所需要的時間會成 log(N) 增長。\n但實務上必須考量到 CPU / Network 以及更新的頻率，如果更新的頻率太高，因為資源受限則同步的延遲可能會無限的增長，實際上應用程式在意的不是多常更新，而是資料是不是抵達同步\n接著要探討如果我們限定 gossip message 不能超過 MTU(Maximum Transmission Unit)，那我們該怎麼決定要更新哪些 key 才能最有效讓所有節點狀態一致\nRECONCILIATION 先前提到 p 跟 q 來回通信都只送兩者狀態的 delta，如果超過 MTU 則必須有一個優先序決定哪些鍵值要先更新( \u0026lt;π 代表此排序演算法)，作者介紹了兩種，一種是 precise reconciliation 最為基準線，對比另一個作者提出的 Scuttlebutt 更新機制\nprecise reconciliation 根據更新時間序決定哪些 key 要先送出去，在實務上 precise reconciliation 比較麻煩些，如先前說必須要先送 state 給對方做比較才能算出 delta，這會消耗頻寬以及 CPU cycle\n依據時間序又可以細分成 precise-oldest: 在 MTU 限制下先送那些很久沒有更新的 key / precise-newest: 先送最近才被更新的 key，後者要留意會有 starvation 問題，在實作上節點必須同步時間，才能作為判斷的依據\nNote that, if implemented, both these orderings would require a synchronized clock among the members and that all updates be timestamped with this clock.\nScuttlebutt Reconciliation 作者提及另一種解法，也是 Cassandra 採納的做法，初始化時 version 固定是最小的數字，每次更新鍵值時，要把 version 設定大於成目前任意鍵值對應的最高版號\n{(r, max(µp(r))) | r ∈ P} // 此公式表示 r 是屬於節點 P 的屬性，找出 r 以及當下 P 中 r 最大的版號；\n例如說目前 Participant p 的列表是\n1 2 3 4 key | value | version a | a1 | 1 b | b1 | 2 c | c1 | 3 如果此時 a 要更新，則版號至少要拉到 4，而且不像 precise-conciliation 會一次送多組間值更新，Scuttlebutt 允許一次可以送一個鍵值，但必須按照 version 大小逐一送 整個架構必須符合以下狀態 也就是說在整個集群下，任一鍵值 k 在節點 p / 節點 q 必須滿足以下任一條件\n在節點 p 跟節點 q 中同一個 key version 是一樣，代表資料已經同步 如果節點 p 的 key version 跟節點 q 不同，則此 version 必須比是節點q 中任意最大的版號還要大 第二點非常重要，這是保持每次更新不需要整包送，先從版本判斷就能判斷哪些欄位真的需要更新的依據\n來看一個實際案例，目前有三個節點 r,p,q，共有 3 個 key a,b,c，可以看到 t1 時 r 的三個 key 都被更新過，版號分別是 21/22/23；\n此時 r 要向 p,q 發送 gossip message，他必須先從 a 開始，因為這是 a,b,c 三者中版號最小，且大於 µq(p) / µp(p) ，這意味著節點 p 和 節點q 都要更新，所以 r 會同時送訊息給 p , q，在 t2 時只有 key a 先被更新\n可以回去看 Gossip 介紹(上)的 GossipDigestSynMessage 部分\n雖然說一次只更新一個鍵值效率好像很低，但優點是 r 不需要送已經更新過的值，減少重複，在頻寬有限情況下，Scuttlebutt 也必須決定 gossip message 傳送的優先序，這裡有兩種做法\nscuttle-breadth: 在同一個 participant 中，將 delta 用 version 從小到大排序，如果兩個不同 delta 的 version 相同，則隨機抽 participant 發送 scuttle-depth: 在 participant 中，只有鍵值有落差就算一個 delta，從 delta 最多的 participant 開始送，所以有可能都送給同一個 participant 實驗結果 總共 128 participants 與 64 組 key/ value，每秒每個 participant gossip 一次； 前 15 sec 暖機，並開始限縮頻寬 / 25 秒開始加倍更新頻率 / 75 秒更新頻率回歸正常 / 120 sec 停止更新，中間 25~75 加大流量主要是想要看演算法在高負載下的表現，以及高峰過去後的恢復速度\n第一張圖表代表這一個時間上，該鍵值自從上次被更新後隔了多久才收到最新資訊，越低者越好 staleness of such a mapping µq (p)(k) is the amount of time that has lapsed since µq(p)(k) was last updated\n第二張圖表代表這一個時間有多少個鍵值是過期的 ，越低者越好 reports the number of stale mappings as a function of time\n交叉比對有以下結論\nScuttle-depth 表現優異 Precise-newest 可以看出有 starvation 狀況，也就是有鍵值很久沒有被更新 (圖一他最高)，但是真正影響到的鍵值其實是少數 (同一時間點其實過期的鍵值數不多)，但是高峰過去收斂很快 其餘兩者表現普普 Flow Control 在一些情況下，participant 交換訊息時更新頻率可能不同，所以會需要一個流量控制的演算法，去平衡一個 participant 想要增加更新頻率而另一個想要降低頻率的可能，要製造出這樣的不同更新頻率，但同時系統必須維持相同的最大交換頻率上限\n在 participant 交換 gossip 時，會連帶交換彼此預設更新的頻率 (ρp , ρq)以及最大值 (τp,τq )，當兩個 participant 在交換時會順便交換\n機制有點類似於 TCP 的 Additive Increase Multiplicative Decrease (AIMD)，逐漸增加發送的頻率但遇到錯誤時快速減少；\n如果要發送的 delta 數量高於 MTU，則線性增加，反之，則倍數減少\n實驗過程是在 t = 90 時限縮 mtu 從 100 降到 50，可以看到 90 之後 max out of date 大幅增加，之後才慢慢收斂，其中 scuttle-depth 在表現上比較穩定 這一章節比較不確定，如果有什麼錯誤麻煩指教 🙏\n總結 本篇提出兩個重點\n新的 reconciliation 機制，加速同步的效率，同時避免 starvation 引入 flow control 機制，讓 participant 可以用合理的速度更新 實作面 網路上找了一個 nodejs 版本的 gossip protocol 實作 node-gossip，看起來是使用 scuttle-depth 協議機制 計算與 peer 中的 delta 最多的，接著先按照 peer 中最舊的 version 開始排序\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // Sort by peers with most deltas deltas_with_peer.sort( function(a,b) { return b.deltas.length - a.deltas.length } ); var deltas = []; for(i in deltas_with_peer) { var peer = deltas_with_peer[i]; var peer_deltas = peer.deltas; // Sort deltas by version number peer_deltas.sort(function(a,b) { return a[2] - b[2]; }) if(peer_deltas.length \u0026gt; 1) { // console.log(peer_deltas); } for(j in peer_deltas) { var delta = peer_deltas[j]; delta.unshift(peer.peer); deltas.push(delta); } } ","date":"2020-10-28T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-10-28-gossip-protocol-%E4%BB%8B%E7%B4%B9-%E4%B8%8B-efficient-reconciliation-and-flow-control-for-anti-entropy-protocols%E8%AB%96%E6%96%87%E6%91%98%E8%A6%81/","title":"Gossip Protocol 介紹 (下) - 《Efficient Reconciliation and Flow Control for Anti-Entropy Protocols》論文摘要"},{"content":"Gossip Protocol 是一種通訊機制，應用於同一網路內機器與機器間交換訊息，原理類似於辦公室傳謠言一樣，一個傳一個，最終每一個機器都擁有相同的資訊，又稱 Epidemic Protocol\n實務上有幾個好處\n去中心化:\n機器與機器間直接溝通 (peer to peer) 容錯率高:\n即便節點與節點之間無法直接相連，只有有其他 節點 可以傳遞狀態，也可以維持一致的狀態 效率高且可靠 Gossip Protocol 被廣泛採納，如Cassandra / Redis Cluster / Consul 等集群架構，以下將從 Cassandra 的實作來理解 Gossip Protocol\nApple Inc.: Cassandra Internals — Understanding Gossip 先從實務面來看，Gossip Protocol 在 Cassandra 中主要用於同步 節點 的 Metadata，包含\ncluster membership heartbeat node status\n同時每個節點都會保存一份其他節點狀態的 Mapping Table 更具體來看節點狀態所保存的資料格式\nHeartbeatState:\n每一個節點會有一個 HeartbeatState，紀錄 generation / version，generation 是節點啟動時的 timestamp，用來區分機器是否重新啟動過；version 則是遞增數值，每次 ApplicationState 有值更新時就會遞增 所以同一個節點內的 ApplicationState version 不會重複，且 version 比較大一定代表這個鍵值比較新\nApplicationState: 一個由{enum_name, value, version}建立的 tuple，enum_name 代表固定的 key 名稱，version 則表示 value 的版本號碼，號碼大者則代表資料較新 EndpointState: 紀錄某一個節點下所有的 ApplicationState EndpointStateMapping: 一個節點會有一張針對已知的節點所紀錄的 EndpointState，同時會包含自己的狀態 如下圖\n1 2 3 4 5 6 7 8 9 10 EndPointState 10.0.0.1 HeartBeatState: generation 1259909635, version 325 ApplicationState \u0026#34;load-information\u0026#34;: 5.2, generation 1259909635, version 45 ApplicationState \u0026#34;bootstrapping\u0026#34;: bxLpassF3XD8Kyks, generation 1259909635, version 56 ApplicationState \u0026#34;normal\u0026#34;: bxLpassF3XD8Kyks, generation 1259909635, version 87 EndPointState 10.0.0.2 HeartBeatState: generation 1259911052, version 61 ApplicationState \u0026#34;load-information\u0026#34;: 2.7, generation 1259911052, version 2 ApplicationState \u0026#34;bootstrapping\u0026#34;: AujDMftpyUvebtnn, generation 1259911052, version 31 ..... 這邊可以看到節點 10.0.0.1 所保存的 EndPointStateMap 有兩筆 EndPointState，其中 10.0.0.1 的 HeartBeatState 是 generation 1259909635, version 325，這代表 10.0.0.1 是在 1259909635 時啟動的，並且他目前保存欄位中最新的版本是 325；\n接著看 ApplicationState \u0026quot;load-information\u0026quot;: 5.2, generation 1259909635, version 45，這代表節點內 \u0026ldquo;load-information\u0026rdquo; 這個 Key 對應的值是 5.2 以及當時收到的 generation 與 version，後兩者用來決定 這個 key 收到訊息後要不要更新的依據\nGossip Messaging 接著來看每次 Gossip 的實作流程，每個節點會在每一秒啟動一個新的 gossip 回合\n挑出 1~3 的節點，優先選擇 live 狀態的節點，接著會機率性選擇 Seed 節點 / 先前判定已經離綫的節點 傳遞訊息的流程是 SYN / ACK / ACK2 (類似於 TCP 的3次交握) 假設現在是節點 A 要傳訊息給 節點 B 關於節點 C 的 Gossip\nGossipDigestSynMessage :\n節點 A 要發送的 SYN 訊息包含 {ipAddr, generation, heartbeat}，需注意此時只要送 HeartbeatState，而沒有送詳細的 ApplicationState，避免多餘的資料傳輸 GossipDigestAckMessage :\n節點 B 收到後，會去比對他自己暫存節點 C 的狀態，運算兩者差異 a. 節點 A 的資料比較新，則節點 B 會準備跟節點 A 要新的資料\nb. 節點 B 的資料比較新，打包要更新的 ApplicationState 回傳 ACK 通知節點 A GossipDigestAck2Message :\n節點 A 收到 ACK 後，更新自己暫存的資料，並且根據 (2.a) 中節點 B 所需要的 ApplicationState，回傳 ACK2 會多一個 ACK2 是為了讓通訊更穩定，達到更快收斂的作用，但這邊如果 節點 B 沒收到 ACK2 是否會重試等如同 TCP 作法就沒有提及\n總結來看傳送訊息的過程，在 Cluster 沒有節點狀態異動下，傳送的訊息量是固定的，不會有 Gossip Storm 網路封包突然爆量的情況； 除非是有節點 新加入，多個節點 希望同步資訊才有可能\n來看實際案例，假設 目前有節點A (10.0.0.1)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 EndPointState 10.0.0.1 HeartBeatState: generation 1259909635, version 325 ApplicationState \u0026#34;load-information\u0026#34;: 5.2, generation 1259909635, version 45 ApplicationState \u0026#34;bootstrapping\u0026#34;: bxLpassF3XD8Kyks, generation 1259909635, version 56 ApplicationState \u0026#34;normal\u0026#34;: bxLpassF3XD8Kyks, generation 1259909635, version 87 EndPointState 10.0.0.2 HeartBeatState: generation 1259911052, version 61 ApplicationState \u0026#34;load-information\u0026#34;: 2.7, generation 1259911052, version 2 ApplicationState \u0026#34;bootstrapping\u0026#34;: AujDMftpyUvebtnn, generation 1259911052, version 31 EndPointState 10.0.0.3 HeartBeatState: generation 1259912238, version 5 ApplicationState \u0026#34;load-information\u0026#34;: 12.0, generation 1259912238, version 3 EndPointState 10.0.0.4 HeartBeatState: generation 1259912942, version 18 ApplicationState \u0026#34;load-information\u0026#34;: 6.7, generation 1259912942, version 3 ApplicationState \u0026#34;normal\u0026#34;: bj05IVc0lvRXw2xH, generation 1259912942, version 7 以及節點B (10.0.0.2)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 EndPointState 10.0.0.1 HeartBeatState: generation 1259909635, version 324 ApplicationState \u0026#34;load-information\u0026#34;: 5.2, generation 1259909635, version 45 ApplicationState \u0026#34;bootstrapping\u0026#34;: bxLpassF3XD8Kyks, generation 1259909635, version 56 ApplicationState \u0026#34;normal\u0026#34;: bxLpassF3XD8Kyks, generation 1259909635, version 87 EndPointState 10.0.0.2 HeartBeatState: generation 1259911052, version 63 ApplicationState \u0026#34;load-information\u0026#34;: 2.7, generation 1259911052, version 2 ApplicationState \u0026#34;bootstrapping\u0026#34;: AujDMftpyUvebtnn, generation 1259911052, version 31 ApplicationState \u0026#34;normal\u0026#34;: AujDMftpyUvebtnn, generation 1259911052, version 62 EndPointState 10.0.0.3 HeartBeatState: generation 1259812143, version 2142 ApplicationState \u0026#34;load-information\u0026#34;: 16.0, generation 1259812143, version 1803 ApplicationState \u0026#34;normal\u0026#34;: W2U1XYUC3wMppcY7, generation 1259812143, version 6 節點A 決定向節點B 發起 Gossip 產生的 GossipDigestSynMessage 會是類似於 10.0.0.1:1259909635:325 10.0.0.2:1259911052:61 10.0.0.3:1259912238:5 10.0.0.4:1259912942:18，主要是傳送 Node IP:generation:version\n節點B 收到 GossipDigestSynMessage 會有以下流程\n跟自己的狀態比較，從差異最多的遞減排序，這意味著優先處理差異最多的節點資訊 接著檢驗每一個節點的資料 -節點A 所保存的 10.0.0.1:1259909635:325 會大於節點B 所保存的10.0.0.1:1259909635:324，generation 一樣所以略過，但是 version 325 \u0026gt; 324，則代表節點B 需要向節點 A 索取 10.0.0.1 在 ApplicationState 在版本 324 之後的資料 10.0.0.2:1259911052:61 比節點B 保存的版本還要小，所以到時候會打包資料給節點A 10.0.0.3:1259912238:5 部分節點B generation 比較小，這意味著 10.0.0.3 有 reboot 過，所以節點B 需要更新全部的資料 10.0.0.4:1259912942:18節點B 根本沒有 10.0.0.4 的資料，所以需要全部的資料 組合以上結果 GossipDigestAckMessage的內容會是\n1 2 3 4 10.0.0.1:1259909635:324 10.0.0.3:1259912238:0 10.0.0.4:1259912942:0 10.0.0.2:[ApplicationState \u0026#34;normal\u0026#34;: AujDMftpyUvebtnn, generation 1259911052, version 62], [HeartBeatState, generation 1259911052, version 63] 這代表著\n請給我 10.0.0.1 在 generation 1259909635 中 version 324 以後的更新資料 請給我 10.0.0.3 在 generation 1259912238 全部資料 (version:0) 10.0.0.4 同上 這是你需要更新關於 10.0.0.2 的 ApplicationState 資料 節點A 回覆 GossipDigestAck2Message 節點A 收到後，更新完 10.0.0.2 的資訊後，接著回覆節點B 所要的資料\n1 2 3 10.0.0.1:[ApplicationState \u0026#34;load-information\u0026#34;: 5.2, generation 1259909635, version 45], [ApplicationState \u0026#34;bootstrapping\u0026#34;: bxLpassF3XD8Kyks, generation 1259909635, version 56], [ApplicationState \u0026#34;normal\u0026#34;: bxLpassF3XD8Kyks, generation 1259909635, version 87], [HeartBeatState, generation 1259909635, version 325] 10.0.0.3:[ApplicationState \u0026#34;load-information\u0026#34;: 12.0, generation 1259912238, version 3], [HeartBeatState, generation 1259912238, version 3] 10.0.0.4:[ApplicationState \u0026#34;load-information\u0026#34;: 6.7, generation 1259912942, version 3], [ApplicationState \u0026#34;normal\u0026#34;: bj05IVc0lvRXw2xH, generation 1259912942, version 7], [HeartBeatState: generation 1259912942, version 18] 這樣就完成一輪的 Gossip 了\n可以看出為什麼 HeartbeatState 的 version 會與 ApplicationState 共享，這邊的共享指的是在同一個節點下 ApplicationState 的 version 必須是獨一無二且遞增，這樣才能在 SYN 時只傳送 HeartbeatState 直接判斷有哪些欄位需要更新\n以上範例整理自 ArchitectureGossip\n其他集群上的管理 集群管理上，除了透過 Gossip Protocol 同步資訊外，還有幾個問題要解決\n誰在 Cluster 當中 如何決定節點的狀態是 Up / Down，這又會帶來什麼影響 何時要終止跟某節點的通訊 應該要偏好與哪個節點通訓 增加/移除/刪除/取代節點時如何實作 誰在 Cluster 當中 當一個新的節點要啟動時，他必須要知道集群中有哪些節點去 Gossip，在 Cassandra 的設定檔中可以指定 Seed，有不同的作法可以指定，常見是寫死某一些節點的 ip addr，節點啟動後就跟 Seed 節點溝通，後續就透過 Gossip Protocol 取得所有節點的狀態與 IP\n在 Consul 中，會自己透過廣播在 LAN 裡面自動發現有沒有其他節點，在 EC2 上還可以指定 EC2 Tag 去找出其他節點\nFailure Detection: 決定節點是 Up 或 Down 在 Cassandra 中，錯誤偵測是該節點在本地端決定某節點的狀態，而這個狀態不會隨著 Gossip 所傳送\nex.節點A 覺得節點B 是 Down，當節點A 跟節點C Gossip 時，節點C 不會因為節點A 而把節點B 判斷成 Down，節點 C 會自行判斷\n偵測的方式透過 Heartbeat，Heartbeat 可以是節點跟節點直接用 Gossip 通訊，也可以是從其他節點間接取得 Gossip；\n節點會計算每次 Heartbeat 的間隔，當超過 phi_convict_threshold 則判定為 Down，系統需要因應硬體狀態/網路環境去調整閥值，避免太敏感誤判或是太遲鈍而反應不及等狀況\n在節點斷線的時候，其他節點部分的寫入可能因此沒有收到 ACK 回覆，此時會暫存在本地當作 Hint，如果節點在一定時間內恢復，則會透過 Hint 重新傳送寫入，修復掉資料的可能\n如果節點重新恢復時，其他節點會定期重送 Gossip 給 Offline 節點，屆時就能把 Down 調整回 Up\n節點偏好 除了錯誤偵測外，Cassandra 內部有模組 Dynamic Snitch 專門做節點間的通訊品質偵測，每 100 ms計算與其他節點的延遲，藉此找出表現較好的節點；\n為了避免一時網路波動，每 10 分鐘就會重新計算\n其餘的節點管理就暫時略過，對於理解 Gossip Protocol 不大\n結語 在分散式系統中，節點的狀態同步十分基礎且重要，而 Gossip Protocol 目前是被廣泛應用的解法，模擬人類傳送八卦的方式，想不到在機器也一樣適用\n整體最有趣的設計應該在於 generation / version 的實作，透過 generation 可以知道機器重啟過後要不要重新要資料；透過 version 可以快速 diff 僅有哪些資料要更新，避免額外的傳輸浪費，下一篇將從理論上去分析 Gossip Protocol\n","date":"2020-10-26T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-10-26-gossip-protocol-%E4%BB%8B%E7%B4%B9-%E4%B8%8A-%E5%BE%9E-cassandra-%E5%85%A7%E9%83%A8%E5%AF%A6%E4%BD%9C%E8%AA%8D%E8%AD%98-gossip-protocol-%E7%9A%84%E4%BD%BF%E7%94%A8/","title":"Gossip Protocol 介紹 (上) - 從 Cassandra 內部實作認識 Gossip Protocol 的使用"},{"content":"雖然 Redis Cluster 推出已久，但公司最近才準備從 Sentinel 轉成 Cluster，架構上有不少的調整，以下閱讀文件 Redis Cluster Specification並摘要\nOverview 可以由一至多個 replica 組成，每一個 replica 分配固定的 slot，透過 CRC 將 key 做 hash 後分配至固定的 replica，slot 總數固定在 16358 個\n動態增加、刪除 replica，可以透過 Migrate 指令重新分配 slot 任何一個 Node 都能夠接收 request，但沒有 proxy 功能，會透過 MOVED 回傳 client 正確的 Node 可用性，遭遇網路分割時多數群體可以活下來，條件是 replica 至少有任一 Master 存在 (從 Slave Promote 成 Master 也可以) 所有的單一key 指令支援 Cluster，如果是 multi-key 指令例如 set union 等，Redis 會產生 hash tag 強迫所有的 multi-key 分配到同一個 Node 上\nNode 每個 Node 彼此互相透過 tcp 連接，稱為 Redis Cluster Bus，透過 gossip protocol 發現新的 node /ping 彼此 / 發送 cluster message 等\n每個 Node 都會有 160 bit亂數 ID，即使 Cluster ip 位置改變，ID 也不會改變，除非是 config 檔被刪除或是 admin 強制替換 Node 之間是走 TCP 並透過 16739 port，Node 會與其他 Cluster 內的所有 Node 相連行程 full mesh (N-1 connection)\nClusert Node 會接受任意的連線，並回應任意的ping，但其他訊息只有被視為 cluster 一部分的 Node 才會處理，否則直接丟棄 要加入 cluster 的驗證有兩種方式\nadmin 使用 CLUSTER MEET ip port 加入 已經互相驗證過的 Node 口耳相傳，例如 A -\u0026gt; B 驗證過，B\u0026lt;-\u0026gt;C驗證過，則 B 會推薦 C 給 A，則 A\u0026lt;-\u0026gt;C 也能成功建立連線； 所以 Node 與任一被驗證的 Node 許可後，後續其他 Node 就能自動發現 整體的 Cluster Node 數量上限建議在 1000 以內\n寫入遺失 因為是非同步副本，所以有小概率已經回傳成功給 client 的 write 會被覆蓋，Redis 採取 last failover wins，也就是最新的一個 Master 會覆寫其他副本的結果，所以會有一段空窗期遺失資料，空窗期長短要看發生網路分隔時 client 是連到多數還是少數群\nMaster 寫入後，還來不及複製到 Slave 就死掉了，如果沒有在一定時間 ( NODE_TIMEOUT )內恢復，Slave 被 promote 成 Master 那寫入的結果就掉了； 而 Master 被 partition 隔開，在一段時間後 Master 發現自己連不到多數Master 就會停止接受寫入請求\n可用性 例如說目前有 N 個 Master ，且對應搭配 N 個 Slave (1對1)，假設有任一個 Node 掛掉，目前為 2N - 1\n此時如果剛好殘存的 replica 那一個 Node 死去，整個 Cluster 才算掛掉，反之就能繼續運作，因為會有一部分的 slot 沒有 Node 負責\n所以 avalaibility 是 1 / (2N-1)\n效能 因為 Node 沒有 Proxy 功能，所以只會跟 client 說正確的 Node 在哪，接著 client 要再發起一次 request 才能完成操作，最終 client 會知道所有的操作該去哪個 Node；\n基本上不太需要擔心效能問題，假設 Cluster 有 N 個 Master，負載能力基本上可以視為單機 * N，雖然有上述在第一次要多查一次的問題，但因為 Client 對於每個 Cluster Master 都保持 TCP 長連結，所以不至於太擔心\n分散式模型 redis 將 key space 切割成 16384 等分 HASH_SLOT = CRC16(key) mod 16384，分配後會固定，該 slot 就會由同一個 Node 負責，直到有 reconfig 的指令；\nhash tag 是一個分散的例外，主要是為了支援 multi-key 可以分配到同一個 hash slot 上，如果 key 包含 {} ，則 hash key 產生是key 裡面第一組 {…} 中間的值 (沒有包含 {})\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 unsigned int HASH_SLOT(char *key, int keylen) { int s, e; for (s = 0; s \u0026lt; keylen; s++) if (key[s] == \u0026#39;{\u0026#39;) break; /* 沒有找到 {，則 hash 整個字串 */ if (s == keylen) return crc16(key,keylen) \u0026amp; 16383; /* 找到 { */ for (e = s+1; e \u0026lt; keylen; e++) if (key[e] == \u0026#39;}\u0026#39;) break; /* 沒有找到 }，hash 整個字串 */ if (e == keylen || e == s+1) return crc16(key,keylen) \u0026amp; 16383; /* 如果有 {} 配對，則 key 取 {...} 之間的字串 */ return crc16(key+s+1,e-s-1) \u0026amp; 16383; } Resharding Redis Cluster 在管理上以及使用上都有相當大的彈性，Cluster 可以任意增減 Node 數量以及調整 Slot 對應的 Node；而 Redis Client 可以與任意的 Node 連線\n如果 Node 收到 Client request 後\n如果該資料屬於他的 slot，則直接處理 反之則找出哪個 Node 應該負責，回傳 Moved 錯誤給 Client -MOVED 3999 127.0.0.1:6381，這代表 key 在 slot 3999，正確的 Node 是 127.0.0.1:6381 Client 在每次收到 MOVED 後都應該去記住哪一個 slot 對應哪一個 Node，這樣可以增加效率；\n又直接拉下整張 mapping，用 $cluster nodes 找出所以的 node，並結合 $cluster slots 直接查看 cluster 完整對照表，就可以完整知道 node 對應的 slot 以及 node ip 位置與 id\nRebalance Cluster 可以在運行中增減 Node，並且會自動搬移資料並更新 slot mapping，有幾個 command 可以動態影響 Node 與 Slot\naddslot : 指定 Node 增加 slot delslot : 移除 slot，原本負責的 Node 會遺忘這個 slot setslot : 轉移 slot 轉移的部分比較複雜，會需要先去調整 Node 狀態，例如說希望將 slot 8從 Node A 轉到 Node B\n在 Node B 上指定資料會從 Node A 過來: $CLUSTER SETSLOT 8 IMPORTING A 在 Node A 上指定要把 slot 8 轉移到 Node B 上: $CLUSTER SETSLOT 8 MIGRATING B 之後有一個背景程式 redis-trib 會主動將 slot 逐步搬移 也可以手動轉移\n列出該 slot 一定數量的 hash key: $CLUSTER GETKEYSINSLOT slot count 將這些 key 轉移: $MIGRATE target_host target_port key target_database id timeout，Migrate 會在轉移成功後才移除 Node B 的紀錄 過程可參考此問答 Redis cluster live reshard failure\n轉移過程 在轉移過程中，client 針對搬移中與搬入中的 Node 發出 Request 反應會有所不同\n搬出中的 Node : a. 如果 key 還在 Node中 則直接處理 b. 不再則回傳 ASK 錯誤，Client 接著必先向新的 Node 發出 ASKING，接著才傳送 Request 搬入中的 Node : 只處理有先傳送 ASKING 的請求，其餘回傳 MOVED 錯誤 這樣的目的是為了方便轉移過程，New Key 一定是在搬入中 Node產生，Migrate 則只需要處理 Old Key\n已經有 MOVED 錯誤碼仍然需要 ASK 的原因是 ASK 是一次性請求，我們會希望在搬移過程，假設 Client 如果有 slot 8 的請求，應該先去問 A 再去問 B，如果 A 沒有會用 ASK 轉去 B；\nMOVED 代表的是往後的永久性所有針對 slot 8 的請求都去 Node B，但顯然還在 migrate 的話會有大量的 key 還沒搬移，所以用 ASK 一次性的查詢當作過度\n如果是 multi-key 操作，假設 key 分散在兩個 node 之間，則會收到 TRYAGAIN 錯誤\nFault Tolerance Node 會發送 ping / pong 去確認其他 Node 是否還存活，兩者合併又稱 heartbeat，除了 ping 會觸發 pong 回復外，如果 Node 有 config 檔更新，也會主動發送 pong 給其他 Node\n在半個 NODE_TIMEOUT 時間內，Node 會送 ping 給其他所有的 Node 確保存活，在 NODE_TIMEOUT 時間點到前，Node 還會重新與其他 Node 建立 TCP 連線，確保ping 沒收到不是剛好這一次的 TCP 連線有問題\n所以 packet 交換數量與 Node 數量和 NODE_TIMEOUT 時間有關，例如 100 Node + 60 sec 的 timeout，則每一個 Node 30 sec 內會送 99 個ping 給其他 Node，換算後整個 cluster 是每秒 330ping\nheartbeat 封包共用 Header 大致如下\nNodeID: 160 bit 識別碼 currentEpoch: 遞增數字，用來表示訊息的先後順序 flag: 是 Master 還是 Slave bitmap: 用來表示負責的 slot sender 的 tcp port sender 認為 Cluter 是否還在運行 如果是 Slave 則需要包含 Master NodeID Fault Detection 在 Cluster 中，一個 Node 被判定錯誤是經由多數的 Node 所決定；\n而如果發生錯誤的是 Master，且沒有 Slave 被成功升級成 Master，則此時 Cluster 進入錯誤狀態，停止 Client 的請求\nNode 在時間內會隨機對數個 Node 發送ping 請求，此時如果超過 NODE_TIMEOUT 都沒收到 pong，則會標記該 Node 為 PFAIL，此時的 PFAIL 並不會觸發任何機制；\n當 Node 在回覆 pong 時，會把自己所維護的hash slot map也一並發送，所以每個 Node 在一定的時間內都會知道其他 Node 所維護的標記狀態清單\n假設 Node A 標記 Node B 為 PFAIL，接著 Node A 收到來自其他 Master Node 對於 Node B 的標記，如果在 NODE_TIMEOUT * FAIL_REPORT_VALIDITY_MULT 等待時間內，大多數的 Master 也都標記 Node B 為 PFAIL 或是 FAIL，則 Node A 轉標記 Node B 為 FAIL\n被標記成 FAIL 幾乎是不可逆，除了下述情況\nNode 是 Slave 且可以被連上 Node 是 Master 且可以被連上，同時沒有分配到任何 slot，此時會等待被加入 Cluster Node 是 Master 且可以被連上，同時 Cluster 過很長一段時間都沒有 Slave 被 Promote 成功，則可以考慮重新加入 Cluster 透過這樣的設計，最終 Cluster 中每一個節點的狀態都會是被同步的\nConfiguration handling, propagation, and failovers 這一章節主要談 Slave promotion 的過程\nCluster current epoch 在分散式系統中，必須有個方式決定重複或是衝突的事件該選擇哪一個，Redis Cluster 中採用 epoch (對比 Raft 中的 term) 是一個 64 bit unsigned int，初始化 Master / Slave 都是 0，在發送訊息時加 1，在收到訊息時如果對方的 epoch 高於自己，則更新 epoch 並加 1；\n遇到有衝突，則選擇 epoch 較高的那一則訊息\nMaster 在發送 ping 時會夾帶 configEpoch 且在回 pong 時會夾帶所屬的 slot mapping\nPromotion 過程 如果符合以下條件\nMaster Node 失敗 Master 有負責 slot Slave 並沒有與 Master 失聯超過一定時間，這個判定是為了避免 Slave 的資料太舊 則 Slave 會準備提起 Promotion，此時會增加 configEpoch 值，並希望取得多數 Master 的同意，發送 FAILOVER_AUTH_REQUEST 給 Master Nodes，接著 Slave 會等待 2 * NODE_TIMEOUT\n此時 Master 如果同意，則回覆 FAILOVER_AUTH_ACK，此時 Master 在接下來的 2 * NODE_TIMEOUT 不可以回覆其他 Slave Node FAILOVER_AUTH_ACK 訊息，避免多個 Slave 同時投票\n如果 Slave 在 2 * NODE_TIMEOUT 內收到多數 Master 同意，則選舉通過；\n反之如果失敗，則 4 * NODE_TIMEOUT 後開始下一次選舉\nHash Slot 維護 先前提到 Master 在送 pong 時會把自己管理的 slot 也一併更新，此時的 slot 會夾帶目前 Master 的 epoch 資訊，例如以下\nCluster 初始化，此時 Slot 都沒有對應的 Master，需要 CLUSTER ADDSLOTS 分配 1 2 3 4 5 0 -\u0026gt; NULL 1 -\u0026gt; NULL 2 -\u0026gt; NULL ... 16383 -\u0026gt; NULL 假設分配部分給 A，A 會順便標記 epoch，假設此時值為 3 1 2 3 4 5 0 -\u0026gt; NULL 1 -\u0026gt; A [3] 2 -\u0026gt; A [3] ... 16383 -\u0026gt; NULL 產生 Failover，B 上來頂替 A，此時會把 epoch + 1 1 2 3 4 5 0 -\u0026gt; NULL 1 -\u0026gt; B [4] 2 -\u0026gt; B [4] ... 16383 -\u0026gt; NULL 這也就是 last failover wins 策略，假使 A 此時網路連線回來要宣稱自己是 Master 也會被擋下來，因為 A 的 epoch 比較小\n實際案例 假設 Master 已經掛了，此時有 A,B,C 三個 Slave\nA 競選成功變成了 Master 網路區隔關係，A 被視為錯誤 B 因此競選而成 Master 接著換 B 被網路區隔 此時 A 成功連線回來 此時 B 失聯，A 以為自己還是 Master，此時 C 因為 B 失聯會想要去競選 Master\n此時 C 會成功，因為其他的 Master 知道 C 的 Master(B) 已經失敗 A 無法宣稱自己是 Master ，因為 hash slot 已經被更新成 B 的 epoch （第三步)，且 B 的 epoch 高於 A 的 epoch 接著 C 會更新 hash slot 的 epoch，Cluster 持續運作 結論 整個看過 Redis Cluster Spec 會對於實際操作比較有信心，下一篇來自己實際架設看看\n有一些細節比較瑣碎就跳過，有些章節讀起來比較不通順，自己重新理解後編排一下 (OS. 可能會變得更難理解 ?!\n","date":"2020-10-19T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-10-19-redis-cluster-%E4%BB%8B%E7%B4%B9/","title":"Redis Cluster 介紹"},{"content":"最近公司 API 服務被 Client 不預期的高頻存取，造成後端 DB 很大的負擔，開始評估各種 API Rate Limit 的方案，其中一個最常見的作法就是靠 Redis，但具體的方案其實有蠻多種，參考以下影片整理三種作法\n順便推薦一下 RedisLabs 所推出的 GUI 管理工具 RedisInsights，可以快速分析 Redis 中 Key Space 的使用 / Profiling 一段時間內哪些 Key 被大量存取等等，基本的 Redis CLI 操作就更不用提了，對比之前用的 medis 功能強化不少，尤其是管理/監控這一塊的功能\n目前是免費的，支援 Cluster Mode，連接 AWS ElasticCache 也沒問題，十分推薦\nRate Limit 全觀 要設計 Rate Limit 機制時需要考量幾個面向\nWho 該如何識別要限制的對象？\n最直覺是透過 IP，但是使用 IP 最大的風險是 如果是大客戶，他一個人的流量遠超過其他小客戶，對公司的價值顯然也是遠遠重要，如果用 IP 很容易有誤殺的情況，把有價值的用戶阻擋在外\n其他的作法可以用 JWT Token / API Key 等個別用戶識別的方式，需要針對自家的業務場景去判斷\nHow 該使用怎樣的方式計算限制的方式？\n通常是在某個時間區段內，限制只能存取多少次的計算模式，有三種方式可以參考\nstatic time window - 固定時間區段 例如說每一分鐘為一個單位，這一分鐘內只能存取五次\n這樣的方式十分簡單，但可能會有短時間內超量的問題，例如說 0:59 存取 4 次，接著 1:01 存取4 次，分開在兩個時間區段都是合法，但是才隔兩秒就存取 8 次，這可能不會是希望的結果\n實作方式，以目前每週 160 k 下載的 express-rate-limit 中 redis 版本 rate-limit-redis 是以下做法\n先計算出該時間段的鍵值，例如 01:00 ~ 01:59 的鍵值都是 01 1 var expiryMs = Math.round(1000 * options.expiry); 增加 key 並更新 ttl 時間，incr 會回傳當下增加後的值，藉此判斷是否超過限制 1 2 3 4 options.client.multi() .incr(rdskey) .pttl(rdskey) .exec() 因為有 ttl，所以不用擔心 key 的刪除，這個方法簡單直覺儲存成本也很低\n為了嚴格限制任意時間區段內的最大存取數量，參考以下文章提及兩種做法 Better Rate Limiting With Redis Sorted Sets token bucket 每一個用戶都有一個對應的 bucket，只有 token 足夠時可以進行操作，每隔一段時間會回補 token 數量，好處是可以制定多種操作的 token 需要數量，像是更繁雜的操作需要消耗更多的 token ，更有彈性應對不同的限制方案\n資料結構使用 Redis 的 Hash，演算法大致如下\n用戶要操作的時候，如果此時沒有紀錄，先插入一筆 Hash user: 當下 的 timestamp =\u0026gt; token 初始化數量 後續操作時，取出上一次操作的 timestamp，接著回補這一段時間需要補充的 Token 數量 接著扣除操作所需的 Token 數，查看是否有符合限制 需注意這種做法會有 Race Condition 問題，如果一個用戶同時有兩個操作，在第三步驟檢查時，會誤以為自己都有足夠的 token，除非使用 Lua script，Redis 才會將多個操作視為 atomic 避免 Race Condition\nnode-redis-token-bucket-ratelimiter 便是採用 Lua script 作法，讓我們來欣賞一下\n取得參數，並指定 redis.replicate_commands()，這是在調用 $ redis eval 時要產生隨機 IO 時需要提前執行的指令 Redis - EVAL script numkeys key，這一篇有易懂的解釋 Redis · 引擎特性 · Lua脚本新姿势，基本上就是為了符合 Redis 在持久化以及副本資料時的功能，在 5.0 以後是默認選項； 接著就是分別計算上一次更新時間 initialUpdateMS / 殘留的 token 數 prevTokens 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 -- valueKey timestampKey | limit intervalMS nowMS [amount] local valueKey = KEYS[1] -- \u0026#34;limit:1:V\u0026#34; local timestampKey = KEYS[2] -- \u0026#34;limit:1:T\u0026#34; local limit = tonumber(ARGV[1]) local intervalMS = tonumber(ARGV[2]) local amount = math.max(tonumber(ARGV[3]), 0) local force = ARGV[4] == \u0026#34;true\u0026#34; local lastUpdateMS local prevTokens -- Use effects replication, not script replication;; this allows us to call \u0026#39;TIME\u0026#39; which is non-deterministic redis.replicate_commands() local time = redis.call(\u0026#39;TIME\u0026#39;) local nowMS = math.floor((time[1] * 1000) + (time[2] / 1000)) local initialTokens = redis.call(\u0026#39;GET\u0026#39;,valueKey) local initialUpdateMS = false if initialTokens == false then -- If we found no record, we temporarily rewind the clock to refill -- via addTokens below prevTokens = 0 lastUpdateMS = nowMS - intervalMS else prevTokens = initialTokens initialUpdateMS = redis.call(\u0026#39;GET\u0026#39;,timestampKey) if(initialUpdateMS == false) then -- this is a corruption -- 如果資料有問題，需要回推 lastUpdateMS 時間，也就是用現在時間回推殘存 Token 數量的回補時間 lastUpdateMS = nowMS - ((prevTokens / limit) * intervalMS) else lastUpdateMS = initialUpdateMS end end 接著計算上一次到現在需要回補的 Token addTokens / 這一次運算配額夠不夠 netTokens / 如果下一次要嘗試需要等多久的時間 retryDelta 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 local addTokens = math.max(((nowMS - lastUpdateMS) / intervalMS) * limit, 0) -- calculated token balance coming into this transaction local grossTokens = math.min(prevTokens + addTokens, limit) -- token balance after trying this transaction local netTokens = grossTokens - amount -- time to fill enough to retry this amount local retryDelta = 0 local rejected = false local forced = false if netTokens \u0026lt; 0 then -- we used more than we have if force then forced = true netTokens = 0 -- drain the swamp else rejected = true netTokens = grossTokens -- rejection doesn\u0026#39;t eat tokens end -- == percentage of `intervalMS` required before you have `amount` tokens retryDelta = math.ceil(((amount - netTokens) / limit) * intervalMS) else -- polite transaction -- nextNet == pretend we did this again... local nextNet = netTokens - amount if nextNet \u0026lt; 0 then -- ...we would need to wait to repeat -- == percentage of `invervalMS` required before you would have `amount` tokens again retryDelta = math.ceil((math.abs(nextNet) / limit) * intervalMS) end end 如果成功操作 ( rejected == false )，則延長 key 的過期時間 1 2 3 4 5 6 7 8 9 10 if rejected == false then redis.call(\u0026#39;PSETEX\u0026#39;,valueKey,intervalMS,netTokens) if addTokens \u0026gt; 0 or initialUpdateMS == false then -- we filled some tokens, so update our timestamp redis.call(\u0026#39;PSETEX\u0026#39;,timestampKey,intervalMS,nowMS) else -- we didn\u0026#39;t fill any tokens, so just renew the timestamp so it survives with the value redis.call(\u0026#39;PEXPIRE\u0026#39;,timestampKey,intervalMS) end end sliding time window - 滑動時間區段 最後一個是使用 sorted set，可以使用 $ redis.multi 將多個 sorted set 的指令串再一起 Atomic 執行所以能夠避免 Race Condition 狀況\n具體想法是\n用一個 sorted set 儲存所有的 timestamp request 進來後，先用 ZREMRANGEBYSCORE 捨棄 time window 以外的 key 取得 sorted set 剩餘的所有元素 ZRANGE(0, -1) 加上這一次的操作 ZADD，並延長 sorted set 的 ttl 接著算整個 sorted set 的元素量，就知道存取幾次了 需要特別注意，這邊如果第五步判斷失敗也會被計算在 limit 當中，因為第四步已經先加上去了，如果在第三步先判斷數量夠不夠再去更新 sorted set，中間的時間差就有可能發生 Race Condition，所以要嚴格限制必須要這麼做，除非又要包成 lua script\n這會導致一個風險，如果 Client 真的失控一直打，那他會無止盡的失敗，因為每一次的失敗操作都會被加入 sorted set 當中，但其實都沒有真的執行到\n模組請參考 rolling-rate-limiter，程式碼在這\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 const batch = this.client.multi(); batch.zremrangebyscore(key, 0, clearBefore); if (addNewTimestamp) { batch.zadd(key, String(now), uuid()); } batch.zrange(key, 0, -1, \u0026#39;WITHSCORES\u0026#39;); batch.expire(key, this.ttl); return new Promise((resolve, reject) =\u0026gt; { batch.exec((err, result) =\u0026gt; { if (err) return reject(err); // 加完後才來計算是不是扣打足夠 const zRangeOutput = (addNewTimestamp ? result[2] : result[1]) as Array\u0026lt;unknown\u0026gt;; const zRangeResult = this.getZRangeResult(zRangeOutput); const timestamps = this.extractTimestampsFromZRangeResult(zRangeResult); return resolve(timestamps); }); }); 結論 Rate Limit 看似簡單，但也有不少的眉角要去考量，之前一直都沒有客製 Redis 中 lua script 的部分，也是蠻有趣的\n","date":"2020-10-18T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-10-18-%E4%BD%BF%E7%94%A8-redis-%E7%95%B6%E4%BD%9C-api-rate-limit-%E7%9A%84%E4%B8%89%E7%A8%AE%E6%96%B9%E6%B3%95/","title":"使用 Redis 當作 API Rate limit 的三種方法"},{"content":"平時喜歡運動，曾參加過龍舟、跑過全馬/半馬，維持一週至少三次每次一小時的運動習慣，在同事的推坑下，試騎了幾次後覺得騎車很舒適，在台北近郊有很多不錯的山路，也有河濱可以舒適的騎乘\n分享幾個當初選車的想法，包含預算等等考量，以及入坑一個月後的一些心得與避雷建議\n新手分享，如有錯誤歡迎糾正 🙏\n牽車後一週第一次上風櫃嘴\n*更新: 在 2020/11 完賽環花東，有興趣可以參考看看 一人備戰環花東365挑戰賽 - 備賽、比賽過程分享，希望讓這篇看起來更有可信度 (咦 ?!\n購車前的考量 因為畢竟是剛入坑，預算一開始是抓兩萬，這其實還蠻低的，只能買各家廠牌的入門車款，坦白說選擇不多，但我自己覺得這個價格是我心理可以負擔的，如果日後~不幸~騎出興趣，要再換也還是可以的\n一般自行車類型有分成 公路車、登山車、三鐵車等等，公路車把手部分又有 平把 與 彎把，平把相對坐姿較高，所以長途相對比較不會累，一開始試騎也是用平把；\n彎把 就是相對坐姿較趴，把手有多種手勢可以變換，相對身體負擔就比較大一些，但平常有運動、重訓習慣應該是蠻能適應\n蠻建議購車前先去跟朋友借騎看看，當初也是感謝同事領騎了三四次，確認喜歡騎乘在入手會比較好\n購車評估 因為預算抓在兩萬，基本上只能選鋁合金的車，車款也沒太多可以挑的，對於新手入坑似乎也是好事，不然有選擇障礙 XD\n究竟要不要直上碳纖也說不準，騎山路車重應該影響蠻多，但我騎鋁車還是能上風櫃嘴、冷水坑，騎環花東也不輸碳纖的車 (排名都在前 50%)，除非是志在參加 KOM\n後來有幸騎到朋友破十萬的碳纖車，一騎上去就發現「哇！也太好騎了吧」騎起來更結實、力量更直接傳輸出去的感覺，即使騎平路感受也完全不同，所以預算夠直上也無仿 XD\n但碳纖一級車架跟二級車架的差距沒有感受過也還不知道\n車子本體 公路車基本上越輕越貴，入門價大約都從 1、2 萬開始，這時候通常都是買成車，廠牌爬了文大概就是 Giant / Performer / KHS / Fuji / Merida 等，之前只有聽過捷安特的大名，後來考量了一下決定買 Performer 的 Storm Dark\n主要是聽店員介紹說，一般入門車款各大廠差異不大，就是看車型、品牌偏好、顏色等等的選擇，當初最吸引我的是 Performer 有提供工廠客製化烤漆，覺得多花一點錢烤成自己喜歡的配色很不錯\n但後來沒有烤漆，因為疫情關係腳踏車訂單暴增，要排烤漆需要很久，後來想說等之後有機會再說\n這邊推薦一下購買的車店小哲居單車，購買後有幫忙調整坐高、把手等等基本的 Fitting，建議第一次買還是去實體店面，請店家幫忙抓車架大小等等，這間服務還不錯推薦給大家\n後來在網路上看到專業 Fitting 師舒迷說的分享，也蠻推薦大家可以收看這個頻道，可以學習到很多關於自行車以及如何騎乘等知識\n變速系統 變速系統規格可以選擇，通常搭配的廠商是日商 Shimano，他們家的變速器有分幾個 Level，我選擇是 SORA，店員是說入門建議就從這款開始，更低階就不推，至於更高階的 105 / Ultegra 等還沒有嘗試過，稍微詢問一下等級差異，主要在於變速手感、精準度與速度的差異；\n低階的基本上都是機械變速，還有更貴的電子變速，價格都要好幾萬\n大齒盤與飛輪 自行車的傳動系統，前方是大齒盤，後方是飛輪，大齒盤通常是兩盤: 大盤加小盤，後方飛輪則是算最小齒到最大齒\n齒輪比會影響騎乘時踩踏的力道，齒輪比高就費力，適合下坡跟平路，反之則上坡比較輕鬆，看一些文章有提到齒輪比對於騎乘影響很大，理想是腳踩踏的輸出功率穩定，也就是力道與頻率固定，透過變速來適應地形的變化\n我後來飛輪選擇 11T~34T，主要是預期會很常爬坡，34T 爬坡踩起來就很輕\n卡鞋 另一個在考慮的是卡鞋，上卡基本上要三千起跳，聽朋友說這會影響發力的方式，因為腳與踏板合體，所以往回拉大腿肌也會出力，而不像一般騎乘只有下踩的時候能出力\n除了價格外，穿上卡鞋危險性會高一些，可能臨時要停車但脱卡不順就會摔車，考量後決定先一般騎乘就好，把肌力練起來之後再說\n配件 其他配件部分，推薦幾個好東西要準備\n手電筒 / 後方警示燈:\n這是上路必備裝備，否則會有行車安全與法規問題喔! 晚上/視線不好一定要開燈 手機架:\n架在龍頭上方便導航，但如果希望騎山路或長時間(超過兩小時)，建議買車錶 水壺架:\n通常能夠在車架上裝兩個，建議兩個都裝，一個放水壺，一個放維修罐\n維修罐目前個人準備了 CO2 鋼瓶 / 內胎 / 挖胎棒，但目前還沒用過 腳踏車包:\n強烈推薦，總是要有個地方放雨衣、錢包、鑰匙等雜物，買一個十方方便；\n另外不推薦放在前方的前管腳踏車包，站立抽車等會影響騎乘，除非是有特別設計的跟前管差不多寬度不影響騎乘，否則買綁在坐墊下方的個人覺得不錯 以上我都在交車時一次請自行車店組裝，配件大概也花了 3000 多，總共花了 2 萬出頭在車子上，但坑之所以為坑就是他會比你想像中的深\u0026hellip;.\n除了上述車用品外，還有幾項人身配件\n安全帽:\n基本上路大家都會帶，畢竟公路車時速也是蠻快，有些公路有公車、汽機車混雜，所以安全是蠻基本的，我是在迪卡儂買個簡便的 999 元安全帽 車衣車褲:\n因為坐墊其實蠻硬的，要舒適的長時間騎乘 (一小時以上)，建議買個車褲，我在蝦皮買一件約 500，還算可以，後來發現還是要買好一點的，不然胯下跟大腿騎個一小時就會發麻，後來選購 Monton 台灣品牌覺得不錯，約 $2000 上下；\n車衣我覺得看個人，穿一般運動裝也是可以，只是車衣都會在背後設計口袋，如果要騎比賽還是要買車衣比較推薦，補給放口袋很方便 自行車架:\n一般不會裝停車柱，因為重量與美觀考量，平常都是斜靠在牆上，但是偶爾要清理車子還是要買個車架，極度不推薦以下這款，這種通常一兩百元就有，但是超級不穩定 我是自己另外替換螺絲才勉強剛好，不然第一次洗車一直滑落超級不爽\n4. 清潔用品\n鏈條為了要維持滑順的運作，需要上鍊條油，但是路面的灰塵、小石子會被油沾黏，所以久了就會黑黑的，需要去油清潔後後重新上油，所以需要去油污清潔劑以及鍊條油，通常騎乘後就要稍微清潔跟保養，至少淋到雨一定要保養\n我自己是用廢棄牙刷跟廚房的雙色洗碗刷清潔，上鍊條油後用廚房餐巾紙輕輕的把多餘的油擦掉避免沾染灰塵\n5. 打氣筒\n每次出發前都要檢查胎壓，我基本上都會打到 110 psi，買了一個捷安特的打氣筒 CONTROL TOWER 1+ 1000元還蠻好用，但感覺可以不用買到這麼貴，當初不小心手滑\u0026hellip;\n以上大概又花了 5000元\n目前總價 還有一些零零總總共 27000 30000 元，還有一些商品是觀望中，列出來僅供參考\n車錶與監控設備 如果希望騎的更專業，就需要更專業的設備去監控每次騎乘的效果，會有很多的感測器去監測踏頻、速率、心率、功率，以上跟騎乘表現有很直接相關\n如同跑步大家都會問候你的分速「跑一公里需要幾分鐘」，腳踏車談的是「功率」，一小時能夠輸出幾焦耳的能量，所以需要功率計去觀測腳輸出的力量，但功率計非常貴，都要一兩萬起跳，新手入門我就先不考慮了\u0026hellip;\n可以參考以下影片對於功率的分級，很有趣\n第二個是心率，因為自行車通常也是走長途耐力賽，所以要維持在一定的心率強度訓練，可以觀察自己是否太快或太慢，就跟跑步觀測的指標一樣\n可以買專業的心率帶，或是有手錶，但因為價格考量一樣先略過\n最後就是速率與踏頻，我選擇先買一個踏頻感測器，我選擇 Bryton 踏頻感測器約 800 元，主要考量是踏頻是最基本的指標，看了一些教學影片都推薦新手先訓練踏頻穩定度開始，而且踏頻太低會傷膝蓋，目前是抓平路 90~100，上坡至少 65~70，可以用手機搭配 App，iOS 我用 「ALA Cycling」，免費 App 還算差強人意\n後來女友貼心的買了 Bryton 320 ，有車機真的方便很多，Bryton 入門使用還算不錯，搭配踏頻 / 心率帶有數據可以監控自己運動的狀況很不錯，終於可以讓手機退役，可以的話建議搭配，要訓練基本上就是必備；\n高階車錶會搭配全彩螢幕與 GPS 導航功能，我個人上路是都不太看導航，頂多手機稍微查一下，覺得導航功能性好像普普，如果騎固定路線居多建議可以不用先考慮 GPS 功能\n其餘雜七雜八的推薦 水管 先推薦一些 Youtube 頻道\n可利乎，我最喜歡的頻道，大叔的聲音很有磁性 XD 是自行車店老闆與愛好者，看起來有在國外生活，介紹的範圍很廣，從洗車、車款介紹、自主維修、新品介紹等等我都很喜歡 脆瓜城主，有一些路線推薦、進階知識的分享 k2，主要看一些組裝等等介紹，覺得影片內容、節奏都蠻喜歡的 一輪的運動日常eLun_fitnessTW、Linda Loves Cycling、伊娃 Eva，看正妹的頻道就是很棒 XD 有很多的騎車紀錄、自行車生活相關的分享，充滿正能量 App Strava: 社群+運動紀錄，每次都可以記錄自己的騎乘，還幫你切路段比較速度超有趣的，可以看到自己一次比一次進步就很舒壓，通常各大車錶也都支援將紀錄彙整到 Strava，像 Bryton 也有支援\n路線紀錄 路段計時，可能是翻譯問題所以有的路段名稱很怪，有的有在地化 Velodash:\n據說是台灣人經營的，相對 Strava 有更多在地的元素，尤其是路線探索很好用，紀錄頁面我個人也覺得比 Strava 好看，可惜沒辦法連結裝備 ALA Cycling: 為了把手機當車錶用的 App\n總結 長路漫漫，最近騎的還算蠻有興趣，希望能夠稍微幫助到剛準備入坑的人\n平常蠻喜歡騎環小台北、河濱、風櫃嘴\n","date":"2020-10-08T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020-10-08_%E5%85%AC%E8%B7%AF%E8%BB%8A%E6%96%B0%E6%89%8B%E4%B8%8A%E8%B7%AF-%E5%85%A5%E5%9D%91%E4%B8%80%E5%80%8B%E6%9C%88%E7%9A%84%E5%BF%83%E5%BE%97/","title":"公路車新手上路-入坑一個月的心得"},{"content":"近日因配戴兩三年的眼鏡磨損過於嚴重，加上戶外運動需求希望搭配自動變色鏡片，降低紫外線對於眼睛的傷害，決定預算拉高一次配到好；\n在網路上查評價，看到最後就是這兩間在猶豫 靈魂之窗 vs 光明分子，PTT 板上大多推薦驗光很厲害，價格也很厲害，但沒有親身體驗也不太確定，剛好女友也要換眼鏡，就決定一人一間來比對服務，分別是選靈魂之窗中山店和光明分子敦南店\n以下不專業心得，僅供參考 (應該不會有人懷疑是業配吧 XD\nTLDR；\n兩間服務都很讚，都會很仔細驗光，並介紹每一步驟的目的跟意義，也對自己的眼睛有更深入的了解；\n個人體驗上的差異\n靈魂之窗價格帶較寬，高規到低規都有； 光明分子整體體驗比較高級，適合很講究品味的人 實際搭配的眼鏡價格:\n店家 鏡架 鏡片 總價 靈魂之窗 不鏽鋼台灣品牌，原價 打6折 7000多 依視路全視線+曲率1.6 約 7000多 現金價可多打 95折，總共約 14000 元 (期間搭配振興券可以多折 1000) 光明分子 日本品牌 Mr. gentleman，有點忘記工藝、材質上有沒有特殊眉角，約 14000 介紹只有蔡司型錄，曲率選 1.67 的新一代鍍膜 ，因閃光過重所以需要客製化鏡片約 9900 23000 左右，沒有特殊的折扣 預算夠就到這兩間配配看，跟一般的眼鏡行服務、專業度真的是有很大的落差\n驗光 本人體驗的是靈魂之窗，全程女友陪同在旁，事後比對驗光流程兩間幾乎一模一樣，只有順序不同\n角膜狀況 用一台有多個圓圈組成的儀器去拍攝角膜，據驗光師說人眼 70% 度數來自角膜，也就是眼睛最外圍的一層，先拍攝角膜是為了確認健康狀態，如果有受損量測視力也不準，也就沒有後續驗光的必要\n量測出來會有等高線圖，顯示角膜曲度的狀態，如果是有閃光會呈現八字形\n除了角膜外，還會觀察瞳孔大小，個人瞳孔偏大，所以驗光師說會有日夜視差大以及畏光等缺點\n測量度數 這部分有點類似一般眼鏡行的驗光，會看翻滾 E 的缺口 / 紅色綠色哪邊顯眼，後來才知道如果紅色明顯代表調整視力太淺，綠色明顯代表調整視力過深，要兩邊一樣清楚才是正確的\n因為本身有閃光，驗光師會去量測閃光的角度，相聚於正常的眼睛呈現球體，閃光的眼睛是屬於橢圓形，會透過鏡片磨製去矯正，但橢圓傾斜的角度就會影響鏡片磨製的方向，所以需要去調整角度\n接著還有測量 斜視跟斜位，斜視是向前看時瞳孔不是指向正前方，所以面對面說話會以為對方視線不在自己身上；\n而斜位是眼睛閉起時會往旁邊位移，等到要直視時會透過肌肉拉扯把瞳孔對正，但如果肌肉無力就會有斜視的問題，可以理解成眼睛會找時間偷懶的概念 XD\n測量方法是用眼睛直視前方，用特殊的鏡片去檢測眼睛是否有上述狀況，太嚴重者要矯正，本身有稍微斜位，但是不嚴重不需要另外矯正\n最後驗光師會把剛剛的數據，再做調整，靈魂之窗的驗光師直接拿出白板畫圖講解，個人十分喜歡這種教育顧客的作法，後面補充說明了\n調整視力只是要讓成像位置準確落在視網膜上，而視力矯正的極限(1.0 還是 2.0)是看視網膜細胞數量，細胞越多成像效果越好，就像拍照同樣的鏡頭底片素質不同拍攝效果也不同的道理 量測度數可以微調，一般量測的視力是看遠物，也就是六公尺以上，但如果平常都是看近物為主，可以微調降近視度數，人眼透過睫狀肌跟水晶體去調整聚焦的位置，只能把成像位置往後拉不可能往前拉，原理像是近視的人東西拿近就看得到，因為成像位置會往前拉剛好落到視網膜上 全視線 是一門專利技術，授權給鏡片製造商去生產，所以好像有蠻多間廠商都有提供，另外全視線是隔絕紫外線而非光量，也就是陰天可能鏡片也會變色，但開車可以不用擔心，因為擋風玻璃通常會隔絕紫外線，不太需要怕進入隧道會太暗 現在抗藍光技術有提升，僅吸收部分對人眼有害的部分藍光波長，顏色不會變黃很多，有實際拿鏡片測試確實工程師的眼睛看不出色差 XD 以上驗光時間約 45 分鐘，這兩間都差不多，遠比其他間驗光還要仔細的多，記得網路先預約喔\n測量鏡框在臉上的位置 以前配眼鏡，都是驗光完挑完鏡框設計就結束，但其實因為每個人臉型不是完美對稱，喜歡帶的眼鏡高度也不同，這些都會影響到鏡片，所以鏡框的微調也是很重要的\n這次去靈魂之窗，有儀器搭配 app 拍攝正面與側面，正面去看瞳距，側面看眼睛距離鏡片的長度，這些數據會影響鏡片磨製的角度\n光明分子這方面做得更加的先進一些，首先是儀器使用蔡司提供的，共有多顆鏡頭所以站定點拍一次搞定，另外側臉有去調整眼鏡的傾斜的角度，避免貼臉太近或太遠，這部分明顯好一些\n其餘 講一些其他雜七雜八的感受好了\n服務 兩間驗光師都很棒，服務態度都很好，像是有問全視線是什麼、眼睛能不能矯正到 2.0 等等雜七雜八的問題，都有很完整的解答，但兩人比較後稍稍偏好靈魂之窗的解說，因為用白板太認真了 XD 而且整體上講得比較全面\n店面 地點都非常方便，裝潢部分光明分子就是簡約但是又透露著奢華，讓本宅穿著夾腳拖進去有點不太自在 (窮人貌)，營造著時尚、年代感的氣氛，鏡架也都是設計品牌，所以很在意設計感、外觀的人可以考慮； 靈魂之窗相對就很明亮，比較是平日會去的眼鏡行，價格帶很寬，便宜的幾千元到上萬元都有，市井小民想要配個好眼鏡去這邊應該比較自在\n至於鏡片部分蔡司跟伊視路在耐用性、視覺清晰度、透亮度等等的比較，因為也不是專業的就不多說，僅憑一個門外的消費者分享自己體驗的過程，如果有任何講錯的地方歡迎留言\n","date":"2020-10-04T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-10-04_-%E9%85%8D%E7%9C%BC%E9%8F%A1%E6%8E%A8%E8%96%A6-%E9%9D%88%E9%AD%82%E4%B9%8B%E7%AA%97%E8%88%87%E5%85%89%E6%98%8E%E5%88%86%E5%AD%90/","title":"配眼鏡推薦 - 靈魂之窗與光明分子"},{"content":"STUN 應用於處理 NAT 穿越技術 (如 ICE ) 下的一種工具，本身並非 NAT 穿越的解決方案，主要功能為確認兩個在 NAT 背後節點的 IP / Port，本篇為 RFC 5389 的閱讀心得，分享 STUN 背後的設計原理與結構\nSTUN 之上還有擴增一個 TURN 協定，提供 relay 連線的功能，一般的 TURN Server 會同時提供 STUN 的服務，如果想知道 TRUN Server 架設，可以參考另一篇文章 AWS Coturn server架設教學\n如果你有以下疑惑，那這篇文章應該可以幫助到你\nSTUN Server 究竟是如何運作 STUN Server 是否有驗證機制 想知道 Coturn config 中的參數含義，如 Fingerprint / Realm / Nonce 與舊版 STUN 的差異 先前有一版 STUN 的定義 RFC-3489，當時的 STUN 被定義成完整的 NAT 穿越解決方案，但是遭遇了以下問題才改用這一版 5389 取代了舊版\n無法正確區分 NAT 類型，導致可能連線有問題\nNAT 共有四種類型，其中 symmetric NATs 是每次通訊沒有固定的 Public IP / Port，所以只能透過 TURN 來解決雙節點的連線問題\n但是舊版 STUN 演算法設計無法區分 NAT 類型，所以有可能造成部分的連線異常 支援 TCP / DTLS:\n舊版只支援 UDP Security 考量 新版 STUN 不再是完整的 NAT traversal 解決方案，只專注於找出節點外層 NAT 對外的 Public IP/Port，完整的解決方案如 ICE / SIP Outbound 等等\n也因此 STUN SERVER 預設 port 是 3489 / TLS port 是 5398\n架構介紹 可以看到 STUN Client 躲在兩層 NAT 之後，而 STUN Server 就是要通知 STUN Client 最外層 NAT 所對應的 Public IP / Port\nSTUN 採用 client/server 架構，支援兩種溝通方式(transaction)\nrequest / response indicate (送出去不等回覆) 每個 transaction 都有 96bit random id\n傳送機制 每個 Transaction 會定義類型 (Action)，目前 Spec 僅定義 Binding action\n運作機制如下\nclient 送出 request NAT 會修改 client package 封包的 IP，並自主管理 client private ip port 與 NAT 對應出去的 ip / port 一路到 server 手上只會拿到 NAT 的 public ip / port，稱之為 server reflexive transport address (srflx)，如果有用過 WebRTC 應該有看過 srflx ，這就是代表用戶走 STUN 接著 server 把這個 public ip / port 當作內容 XOR-MAPPED-ADDRESS 傳回去給 client NAT 接著會一路改 ip port，但因為內文不改所以 client 會知道最外層 NAT 的 public ip + port Packet Format 1 2 3 4 5 6 7 8 9 10 11 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |0 0| STUN Message Type | Message Length | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Magic Cookie | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | | Transaction ID (96 bits) | | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ 封包格式介紹\n最前方兩個 bit 固定為 0 ，主要是用來區分是不是 STUN 的 packet Message Type 包含 Transaction 類型 / response 成功或失敗 magic cookie 固定為 0x2112A442 Transaction ID 用來區分 STUN transaction STUN message 不可以超過 MTO 如果傳輸透過 UDP:\nClient 要自己處理 retransmit，預設 timeout 500ms ~ 3000 ms 間，之後每次 retry Timeout double 如果傳輸透過 TCP:\n不要增加而外的 framing / demultiplexing 已經保障資料可靠性，預設 Timeout 為 39.5s Server 應該等 Client 主動斷線，除非遇到 timeout 驗證訊息 Server 收到訊息後，會先做基本的驗證，如果有開啟 fingerprint extension 則驗證，如果發現有不支援的屬性則回傳錯誤； Server 回傳錯誤時，必須指定 error code，並挾帶 error code attribute，例如說有不支援的屬性則回傳 420 + UNKNOWN-ATTRIBUTES，如果是 401 驗證失敗則必須對應回傳驗證方式，這邊的 error code 參考 HTTP 所以會有種似曾相似的分類；\n如果訊息成功則夾帶 XOR-MAPPED-ADDRESS 回傳\nFINGERPRINT 再Multiplexing 下，會有多個不同 protocol 的 packet 傳送到相同位址的，例如說 RTP，加上 fingerprint 可以方便 STUN Server 區分 STUN message\nDNS 透過 SRV 紀錄回傳 STUN Server 相關的服務，SRV 格式如下\n1 _service._proto.name. TTL class SRV priority weight port target. 如果是以 STUN Server 開放 TCP / UDP ，假設 STUN Server domain name 是 stun.example.com的話\n1 2 _stun._udp.example.com 86400 IN SRV 0 5 3489 stun.example.com. _stun._tcp.example.com 86400 IN SRV 0 5 3489 stun.example.com. 使用 SRV 好處是可以改變預設的 Port；\n如果不用 SRV，也可以用 A / AAAA 紀錄返回 IP List，但 Port 就只能用預設的\n身份驗證與訊息完整性檢查 共有兩種身份驗證的方式，\n短期 Client / Server 會預先共享同一個 secret，後續 Client 產生一個有時間限制的 credential (password)，Server 收到後會用相同的 secret 做驗證；\n確保訊息完整性則透過 MESSAGE-INTEGRITY 欄位，此欄位產生的方式是把 STUN Message 做 HMAC_SHA1；\nServer 收到STUN Message 後的驗證流程如下\n沒有 MESSAGE-INTEGRITY 和 USERNAME 欄位則回傳 401 檢查 USERNAME 是否合法 用 passwrod 與 username 計算出 message intergrity 的值並比對 都通過則產生 response，response 同樣要包含 MESSAGE-INTEGRITY Client 收到 response 也需要檢查 MESSAGE-INTEGRITY\n因為 credential 有時間限制，所以不會遇到回放(replay)攻擊\n長期 Server / Client 固定長期使用相同的 username / password\n步驟\nClient 不帶任何 credential 發起 Server reject，並分配 realm (指引 client 選擇 credential) 與 nonce (類似於 cookie，指定 duration / client identity 等) 多一層保護 Client retry ，戴上 credential 與 nonce + message-integrity (對整個 request 做 HMAC) Server 檢查 auth 與 integrity Server 收到訊息時會依序檢查\nMESSAGE-INTEGRITY: 沒有回傳 401 如果沒有 username / password / realm / nonce ，回傳 400 Nonce 過期了，438 Username 無效，401 Message-integrity 錯誤，回傳 401 ALTERNATE-SERVER Mechanism 如果 Server 希望 Redirect Client 去別的 Server，可以回傳 error code 300 並指定 ALTERNATE-SERVER Client 收到後，用相同的 transport protocol / credential 對新的 Server 重啟 transaction 之所以開頭要產生一次 auth failed 的 request是要了去跟 Server 拿 Nonce，主要是為了避免回放攻擊；\n回放攻擊主要是 如果有中間人，他拿到 Client request 記錄下來，把同樣的 request 往 Server 送，因為 credential 是長期有效所以中間人也能夠通過驗證，即使是有 TLS 保護 / 中間人不知道 username, password 回放攻擊都有用；\n所以才需要 Nonce 一個由 Server 核發一次性的 Token，包裝在 STUN Message 中，超出時間就會被認定無效，從而避免回放攻擊\nSTUN Attributes 再 STUN Header 之後，可以接零至多個 attribute，attribute 採用 TLV encode\n1 2 3 4 5 6 7 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Type | Length | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Value (variable) .... +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ 如果 response 中有重複的 attribute，只有 第一個 會被考慮，其餘會被捨棄 Type 再 0x0000 and 0x7FFF 是必須要被處理的，如果 STUN agent 無法處理則會失敗 0x8000~0xFFFF 則是 optional 以下舉幾個常見的 attribute\nMAPPED-ADDRESS 主要是為了兼容舊版 STUN，顯示 Client 最後一個 Public NAT 的 ip / port\nXOR-MAPPED-ADDRESS 雷同於上者，但是 ip / port 都是跟 magic-cookie 前 16 bits 做 XOR 儲存 (ipv4 / ipv6 xor 方式不同)，原因是發現有些 NAT 如果看到 payload 是自己的 public IP 會去修改，xor 之後就沒這個問題\n原文解釋 deployment experience found that some NATs rewrite the 32-bit binary payloads containing the NAT\u0026rsquo;s public IP address, such as STUN\u0026rsquo;s MAPPED-ADDRESS attribute, in the well-meaning but misguided attempt at providing a generic ALG function.\nUSERNAME 用來驗證用的，必須是用 utf-8 encode 並小於 513 bets SASLprep\nMESSAGE-INTEGRITY 對 STUN Message 取 HMAC-SHA1，固定長度為 20 bytes (因為 sha1 ) HMAC key 會因為 credential 不同而有所不同\nLong term: key = MD5(username “:” realm “:” SASLprep(password)) Short term: key = SASLprep(password) 需注意 hash 包含整個 STUN Message，同時也包含了 Message length，所以在產生 hash 前，MESSAGE-INTEGRITY 也必須先安插進去 STUM Message 中並帶 dummy content，其他在之後的屬性則被排除在外 驗證時也必須遵守相同的流程\nFingerprint 對整個 STUN Message (排除自己) 取 CRC-32，接著與 0x5354554e 做 XOR ( 避免其他 packet 也用 CRC-32 Fingerprint 必須是最後一個屬性\n1 the FINGERPRINT attribute MUST be the last attribute in the message, and thus will appear after MESSAGE-INTEGRITY. Error Code 包含數值的 error code 從 300~699，保持與 SIP / HTTP 相似的羽翼 再加上 rease: utf-8的文字描述 ，主要是讓用戶可以理解\nREALM 如果 STUN Server 同時支援多個 domain，透過 REALM 可以區分不同 domain 使用的設定\nNONCE 用於每次連線避免 replay 問題的方式，類似於 web 的 cookie\nUNKNOWN-ATTRIBUTES 再 error code 420 時出現\nSOFTWARE 描述軟體的版本 / 製造商等資訊\nALTERNATE-SERVER Server 要求轉換時\nSecurity 以下條列幾種被攻擊的可能與防範措施\nAttacker 可能竄改 STUN 訊息:\n但可以用 message integrity 驗證防止 Attacker 可以居中回傳 error response\n在某些如驗證失敗的訊息，這個就不好防堵，除非使用 TLS 才能杜絕問題 HMAC可能遭受字典攻擊:\n因為 STUN 利用 HMAC，可能遭受字典攻擊，請確保 password 足夠複雜，或是用 TLS 防止問題；不排除 SHA1 之後被攻破，未來可能增加新的欄位與新的 hash 機制 DoS 部分: STUN server 是 stateless，所以比較不會被 DoS 打垮；\nAttacker 可能假冒 source IP，讓 STUN server 去攻擊受害者，攻擊不會被跨大，要從 ingress 去過濾 ip； SOFTWARE 揭露版本資訊，可能變成潛藏的落點\nSTUN Server 應該要有對應的設定去關閉此選項 修改 source ip\nAttack 居中的話，可以攔截client 的 source ip 並修改，server 收到錯的 ip 就會用 XOR-MAPPED-ADDRESS 回傳回，這幾乎不可能阻擋，因為正常的 NAT 也會去修改 source ip；\n只能在更上層的協議，例如 ICE 去驗證 address 的正確性 ","date":"2020-09-22T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-09-22-rfc-5389-stun-%E5%8D%94%E5%AE%9A%E4%BB%8B%E7%B4%B9/","title":"RFC 5389 - STUN 協定介紹"},{"content":"公司 P2P 通信採用 WebRTC tech stack，近日希望自建 STUN/TURN Server，決定採用 Coturn 這套知名的解決方案，在 AWS 架設過程遇到一些坑，決定分享如何架設，並分享設定檔如何設定與操作\n以下包含\nCoturn 於 AWS 上的架設與測試 介紹設定檔內容 上 Production 的考量 補充 NAT 與 STUN/TURN 關係 如果想知道更詳細的協定介紹，請參考\nRFC 5398 - STUN 如果想要用 Container 架設，可以參考我用 docker-compose 架設的方式 Running Coturn + Promethes + Grafana in Docker\nCoturn 於 AWS 上的架設 使用 Ubuntu 18.04 非常簡單，只要以下指令就能啟動 Coturn\n1 2 $ sudo apt-get -y update $ sudo apt-get -y install coturn 強烈建議使用 Ubuntu 18.04 而不要用 Amazon Linux 2，Amazon Linux 2 要自己處理各種套件的相依性，架設過程花了半天還沒架起來\n用 apt-get install 的話，會被限制版本，如果有其他需求，例如支援除了 SQLite 以外的 DB，或是要安裝 Prometheus，建議從 source code build 起，可以參考另一篇文章\n安裝後會有幾個 command 能夠使用\nturnserver\n啟動 STUN/TURN server instance turnadmin\n介面管理後台\n其餘都是測試用工具 turnutils_peer\nUDP-only echo server，檢測連線使用 turnutils_stunclient\n呼叫 STUN server 並取得回應 turnutils_uclient\n可以模擬多人連線 TURN server 並取得回應 上一步安裝後預設會自行啟動 Coturn，但為了後續的實驗，先把 turn service 關閉\n1 2 3 $ sudo service coturn status // 如果有在運行，先關閉 $ sudo service coturn stop 接著啟動 turnserver\n1 $ sudo turnserver 稍微看一下 command line 跳出的訊息，大致說明\nfile descriptor 上限，這會影響最大連線數，可以透過 $ sudo ulimit -n {number}去調整 支援的通訊協定，預設 STUN 支援 UDP / TCP / DTLS，但因為目前沒有指定憑證，所以 DTLS 不支援 採用的 Database，預設使用 SQLite，主要用來儲存 admin 資訊 / turn 連線資訊等，同時支援 MySQL / Redis / PostgreSQL / MongoDB，可以自由替換；\n根據 Spec STUN/TURN Server 處理連線是 State-less，意即 Database 不是用來儲存連線資料，所以不用擔心會是 bottleneck (如果 Coturn 遵守 Spec 的話) 看一下有沒有什麼錯誤，cli-password 錯誤可以先忽略\n啟動 turnserver 要測試之前，我們需要先設定 AWS security group，開放以下的 port\n1 2 3 3478: UDP+TCP // TURN Server 接收 request 的 port 5394: UDP+TCP // TURN Server 接收 TLS request 的 port 49152-65536: UDP+TCP // 實際連線的 Socket Port range 以上三個都能夠調整，但就先採用預設\n接著啟動 turnserver\n1 sudo turnserver -v --lt-cred-mech --user hello:world --realm \u0026lt;your domain name\u0026gt; --external-ip \u0026lt;your instance public-ip\u0026gt; 以上的參數代表\n-v: 顯示詳細的 log --lt-cred-mech: 指定為 long term credential，稍後解釋 --user {username}:{password}: 搭配 long term credential，指定 username / password --realm: 指定 TURN server 對應的 domain name，提示 client 要採用對應的驗證方式 --external-ip: 指定 external ip，在 Coturn 文件寫到，如果是在 AWS EC2 上架設 external-ip 只要指定 public ic 記得要去將綁定 domain name A record 指向 AWS instance\n接著，有幾種測試方法\n1. 使用 Coturn 自帶的 test tool 1 2 $ turnutils_uclient -T \u0026lt;server ip\u0026gt; $ turnutils_stunclient \u0026lt;server ip\u0026gt; 第一個會嘗試傳送封包，可以看 packet 的 loss rate 是否為 0 /\n第二個會回傳主機 public ip (如果前面沒有 NAT 的話)，也就是 STUN 最主要的功用\n2. 用瀏覽器開啟 WebRTC samples Trickle ICE 輸入 server domain 時記得要加 turn:{domain name}，就可以成功拿到 STUN (srflx) / TURN (relay) 的紀錄\n測試了一下，Chrome / Firefox 不支援 no auth 設定，Chrome 不支援 ip based 的 server\n這樣就完成第一步的設定與測試，接著看 Coturn 的完整介紹與設定\n設定檔 設定檔的預設路徑為 /etc/turnserver.conf，也就是等等會修改的文件，可以放在其他地方用 cmd 指定\n也可以從網路上看到官方的預設設定檔 # Coturn TURN SERVER configuration file\n驗證 STUN 的費用很便宜，只有簡單的 request / response，但是 TURN 就非常貴，因為要回放(Relay) P2P 的 media stream，所以 Bandwidth 相當驚人，這時候就需要加上帳號密碼的檢查\nTURN 支援兩種模式，建議是兩者選其中一者\nlong term credential 長期憑證屬於靜態類型，也就是 Client / Server 共用固定的帳號密碼，例如上述的 hello:world\n在設定檔中\n1 2 3 4 lt-cred-mech user=hello:world user=hello2:world2 .... short term credential 如果希望發給 Client 短期的憑證，或是希望多一層授權的流程例如從 API Server 給予等等，可以使用短期憑證\nTURN 實作的方式是 Client / Server 共享一個固定的 secret key，接著使用 HMAC_SHA1 將 username hashed，username 的前半段是 unix timestamp\n所以 TURN server 收到後，從 username 可以看出過期時間，透過 HMAC_SHA1 可以確保是由合法的 Client 所送出\n設定檔寫法\n1 2 use-auth-secret static-auth-secret={secret} Nodejs 版本的產生規則如下，參考自 CoTURN: How to use TURN REST API?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 var crypto = require(\u0026#39;crypto\u0026#39;); // name 隨便填沒有關係 function getTURNCredentials(name, secret){ var unixTimeStamp = parseInt(Date.now()/1000) + 24*3600, // this credential would be valid for the next 24 hours username = [unixTimeStamp, name].join(\u0026#39;:\u0026#39;), password, hmac = crypto.createHmac(\u0026#39;sha1\u0026#39;, secret); hmac.setEncoding(\u0026#39;base64\u0026#39;); hmac.write(username); hmac.end(); password = hmac.read(); return { username: username, password: password }; } 網路設定 網路設定就放在一塊看\n1 2 3 external-ip=\u0026lt;public ip\u0026gt; fingerprint realm=turn.yuanchieh.page 以上是必須設定的參數\nexternal-ip:\nTURN server 的 public ip，看文件完整的設定是 external-ip=\u0026lt;public ip\u0026gt;/\u0026lt;private ip\u0026gt;，但因為在 AWS 上 EC2 處於 NAT 之後，所以只能放 public ip fingerprint:\n主要是 TURN Server 用來區分 packet，後續的 Spec 會有更詳細介紹 realm:\n設定為自己指向 TURN Server 的 domain name，文件表示 TURN Server 可以指定多個 realm，每個 realm 有各自的 user 權限管理，Client 表明所屬的 realm 就能用對應的 user 檢查 其餘還有非常多的設定，例如說\n是否要開放 UDP / TCP / DTLS / stun-only 設定不同的 port / port ranage 指定的 DB / Log 等等 每次連線 session 的時長 / 每個 user 的連線 quota 等等 這些細部的設定可以在慢慢看或是保留預設即可 以下是我測試過成功的設定檔\n1 2 3 4 5 6 external-ip=52.72.33.185 verbose fingerprint use-auth-secret static-auth-secret=north realm=turn.yuanchieh.page 啟動 turnserver 時透過 -c 指定 turnserver.conf 的位置，完成後建議在使用測試工具測試過\nSample Code 為了更方便測試 TURN Server，自己寫了一個 Sample Code Connection through self-hosted TURN server，或是直接看 Demo Page https://webrtc-turn-server-test.vercel.app/，輸入對應的帳號密碼，會主動生成對應的 iceServers\nGo To Production 準備上正式環境時，還有監控以及可用性的調整\n監控 2020/12/04更新：目前 4.5.2 可以支援 prometheus 囉，可是只有支援 Debian，其他平台尚未支援，另外要注意直接用 apt-get install coturn 版本目前是不支援 prometheus，需要自己手動編譯喔\n1 2 3 4 Version 4.5.2 \u0026#39;dan Eider\u0026#39;: - fix null pointer dereference in case of out of memory. (thanks to Thomas Moeller for the report) - merge PR #517 (by wolmi) * add prometheus metrics Warning: 撰文時開啟 prometheus 會有記憶體問題，建議先不要使用喔，詳見 github issue https://github.com/coturn/coturn/issues/666\n看到文件的範例以及設定檔支援 prometheus 這套開源的監控工具，但可惜實測下來暫時無法使用(4.5.1.2)，雖然說有相關的 branch 已經 merged 但是 Issue 還是開著 support export metrics to prometheus #474\n可用性 如果只有單台 TURN server 掛掉導致服務中斷就很慘了，官方有幾個可用性/Load Balance 作法\nTCP Level LB Proxy:\n需注意如果有使用 TURN 功能，必須確保同一個 Client 持續連到同一台 TURN Server，否則普通的 TCP LB 即可 DNS:\n透過 DNS Round-Robin 紀錄，讓 Client 連到對應的 TURN Server 內建的 ALTERNATE-SERVER:\n需要在所有的 TURN Server 之前建一台 LB，這台 LB 按照流量回給 Client ALTERNATE-SERVER 的錯誤，Client 就會按照指定的 Server 去走，達到 LB 的效果 看來看去，採用 DNS 比較方便，AWS Route53 支援定期檢查 Server 狀態，只會回傳健康的 Server，同時能採用 Latency based 或是 Region based 的 DNS 紀錄，讓全球部署更加方便\nNAT 介紹與 STUN/TURN 使用 先前有提到因為 NAT 關係，所以要建立 P2P 連線會需要 STUN Server 的幫助，但有一種 NAT 是必須透過 TURN Server，以下解釋這部分的狀況\nWiki NAT 網路位址轉換\nNAT的四种类型\n簡單整理上面文中的重點\n1 ClientA (192.168.9.1:3000) ---\u0026gt; NAT (8.8.8.8:800) ---\u0026gt; Server1 (1.1.1.1:1000) ClientA 預先與 Server1 取得連線，此時假設 Server2 (2.2.2.2) 想要從 (8.8.8.8:800) 送資料給 ClientB\n完全圓錐型NAT 允許 受限圓錐型NAT 必須 ClientA 也送過請求給 Server2 (2.2.2.2)才可以 埠受限圓錐型NAT 必須 ClientA 也送過請求給 Server2 (2.2.2.2:1000)才可以，相較於上者 Port 必須固定 對稱NAT\n不允許 所以在 WebRTC 下，如果 Client 在完全圓錐型NAT，任一方發請連線都可以；\n如果是在二、三種，則 Client 必須雙方同時發送請求，才符合 NAT 轉發條件；\n第四種則不允許直接的 P2P\n","date":"2020-09-21T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-09-21-coturn-server-%E6%9E%B6%E8%A8%AD%E6%95%99%E5%AD%B8-on-aws/","title":"Coturn Server 架設教學 - on AWS"},{"content":"\n每家知名的巨頭，都有包裝後漂亮的「公關」創業故事，像是 Netflix 就是創辦人 Reed Hastings 去百視達租片因為晚還被罰錢，憤而創辦的故事\n這個故事的包裝是因為當時 Netflix 被百視達拒絕收購後，為了簡潔、有吸引力順便推銷自家產品特色 (沒有遲還罰款) 所編造的，這確實是個很棒的腳本，但真實的創業故事遠比這一小段精彩的很多\n這本書有趣的地方在於，作者 Marc Randolph 鉅細彌遺地從點子發想到創辦前緊鑼密鼓的籌劃，公司經歷多次 Pivot 終於摸索出自己的一套商業模式，描述的方式連當天吃什麼 / 跟家人的互動 / 與公司成員的對話等都撰寫下來，讓人身歷其經跟著 Neflix 一起走過那一段歲月的感覺，但又不會撈叨到如日記般的瑣碎\n以下重點整理幾點個人覺得很精彩的部分\n一千零一個點子 Marc 因為公司被收購而與 Reed 所共事，因為公司要跑行政流程，兩人接下來數個月都無所事事，在每天通勤路上，Marc 會不斷地丟出新點子與 Reed 討論，「客製化棒球棍」/ 「郵寄洗髮精」，Marc 擅長 Marketing / 挖掘新點子，而 Reed 則是有理性且有邏輯的分析每一個點子，一路上兩人不斷的爭執、持續激盪新的想法\n每一個好點子的背後都有堆積如山的爛點子，甚至有時候好點子出現時你還無法察覺\n後來 Marc 在每一夜要哄小孩睡覺時，拿起來錄影帶《阿拉丁》，跟 Reed 討論時正好他被百視達坑了 40 美金，在多個爛點子轟炸下，這似乎是一個有前景的點子\nMarc 接著組織團隊，開始想如何打造「郵寄出租錄影帶」事業，跟團隊討論後發現，為了方便用戶，公司要負擔來回的郵費 / 包材等，算一算成本高的驚人，經過試算後必須要出租一百次才有辦法還本，這根本就行不通\n但是當時(1997年)新的影音格式 DVD 即將問世，扭轉了錄影帶所造成的問題\n一張薄如 CD 的體積與重量 加上之前廠商因為規格之亂 wiki: 錄影機格式戰，所以大家比較願意組織聯盟遵循同一套規範 媒體產業背景的歷史因素，好萊塢等娛樂產業不滿百視達壟斷租片市場，DVD 預計發行的價格是錄影帶的 1/5\n瞬間讓郵寄出租 DVD 這個點子可行 雖說 DVD 前景看起來很誘人，回溯時空當時還是極為小眾的市場，甚至 Marc 和 Reed 都沒看過 DVD，但 Marc 極力說服 Reed，最後兩人賭說「如果郵寄 CD 可以成功」，他們就認真考慮開公司做郵寄 DVD 的生意\n聚集頂尖人才並放手讓他們發揮 「真正的創新，不會來自由上而下的發號施令，也不會來自定義狹隘的任務。你要雇用一群專注於大目標的創新者，他們有辦法找出自己身處何方，著手解決問題」 直接從書中摘錄這一段，描繪了 Marc 最一開始建立團隊與運營的方向，放手讓專業的人才去自由發揮，Marc 只抓住大方向，並持續找尋外部合作機會，例如與索尼 / 東芝談合作，這兩家是當時販售 DVD 播放器的大廠，推出了買新機免費租片的行銷方案\n回顧 Marc 本人的生命經歷與生活哲學，他十分重視以前在野外求生的訓練，在野外攀岩 / 溯溪 / 登山時，你必須環顧整個環境，找出任何可能發生意外的地方並設想如何處理，同時要不斷在沒有明確方向、混亂的現實中持續往目標邁進\n就像是 Netflix 在上線前的準備，他們不斷的改良包裝 / 回郵的方式；倉儲人員持續找出如何有效管理庫存、找出每一家郵局的最短路徑與寄送時間；內容營運人員去各大 DVD 論壇臥底持續推播訊息，所有人都緊鑼密鼓準備服務上線的那一刻\n時間快轉到服務即將上線的前一刻 (1997/4/14)，伺服器都架設完成，連接了一個小鈴鐺如果有訂單進來就會聲響，等到九點鐘一上線，「鈴鈴鈴」服務比想像中火熱，伺服器過了一個小時就支撐不住，IT 人員不斷往訪大賣場組建新的伺服器，所有人既緊張卻又感到欣慰，郵寄 DVD 這門生意行得通!\n因為生意比想像中火爆，這時候倉儲人員先前研究的各大郵局關門時間與最短路徑就非常重要，把要做的細節都實際的測試/調整過才能應付各種突發的狀況\nPivot Pivot 在 Pivot 這一段 Pivot 的轉折十分精彩，散落在時間軸的不同部位，Netflix 雖說一開始是打算郵寄出租 DVD，但是最一開始的營收高達 9 成都是來自販售，而非出租，他們就是搞不懂為什麼用戶不會想要持續的租借；\n雪上加霜的是，Amazon 前些陣子才上市，找上 Netflix 希望談併購，亞馬遜不滿足於線上賣書而已，而是希望跨足成為全都賣的電商平台，不論併購是否成功，Amazon 都決心踏入 DVD 線上販售的市場，對 Netflix 會是一個非常大的打擊\n後來 Marc 跟 Reed 因為收購價格太低，所以拒絕了 Amazon 的邀請，但兩人意識到公司危在旦夕，所以心一橫把販售的事業停止，專心做出租，此時出租的事業雷同線上版的百視達，一樣會收取遲還罰款\n加拿大原則\n這是 Netflix 早期做商業決策時的重要依據，他們最一開始只專注於美國市場，但後來調查如果增加加拿大，可以立刻多一成的收入，但是後來想了想，因為還會有法規 / 郵寄限制 / 語言 (加拿大有法語區)，他們決定只專注於一件事情上，所以就稱為「加拿大原則」\n出租與販售 DVD 的事業也是，雖說有 Amazon 這個外敵存在，但他們內部早已再討論兩種商業模式的取捨，美其名能滿足更多元用戶的需求，但也無形中讓用戶更加的困惑\n割捨掉販售 DVD 後，Netflix 持續的燒錢，用戶似乎出租過一次就不想租第二次，Marc 與他的團隊持續想問題在什麼地方，最後他們想出了現今 Netflix 的商業模式\n訂閱制 + 沒有遲還罰款 + 不斷送到家服務\n每一個月 19.99 美金，一次可以租四片，在網站上可以建立待看清單，每還一片就立刻送下一片給你，沒有任何的退還期限!\n這個商業模式徹底引爆了，用戶愛死這個商業模式，在推出第一個月免費試用後，近乎九成的用戶次月續訂，這個模式更符合一般人看影片的需求\n雖然找到適合的商業模式，但還是很燒錢，因為提供第一個月免費試用，所以公司必須負擔用戶第一個月的 DVD / 郵寄成本，現金流壓力很大，Marc 與 Reed 找上百視達談併購，最後無疾而終，Marc 在書末酸了百視達一下 :P\n在當時 2000 年初遇到 .com 泡沫後，網路服務的募資極度困難，Netflix 不得已開始了第一波的大裁員，包含很多的創始團隊成員的離去，經過組織瘦身後所有人更加專注於目標上；\n在書的過程中，Marc 本人的權利也慢慢的被剝奪，其實 Marc 才是 Netflix 最一開始的創辦人，Reed 出資並擔任董事的角色，但後來 Reed 坦白地指出 Marc 缺失，改由 Reed 出任執行長變成雙首長制，後續 Marc 在上市後一段時間，離開了待七年多的 Netflix\nMarc 很勇敢地承認自己的不足，Reed 也很誠實地以好友/董事的身份直接指出，最後也在他領導下讓 Netflix 變成市值一千五百億美元的大公司；\n但是看到整個過程還是不勝唏噓\nThat will never work -\u0026gt; Nobady knows anything That will never work 是原文書名，代表著當初Marc 的點子不被眾人看好，Nobady knows anything 則是 Marc 給出的回應，沒有人知道任何事，只有試了才知道\n這本書十分的精彩，過程有很多創業的小故事，例如最一開始 Netflix 叫做 kibble (狗食)，起因是 Marc 好友推薦先幫公司取的爛名字，這樣到服務快上線，即使你忙到焦頭爛額，看到名字這麼爛才會有動力去想真正的好名字 /\n他們曾經有一次空前絕後的好機會去行銷，結果不小心把 A 片寄給用戶，反而造成很大的公關危機！\n十分推薦喜歡創業 / 單純對 Netflix 發跡 有興趣的朋友去觀看\n","date":"2020-09-17T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-09-17-%E4%B8%80%E5%8D%83%E9%9B%B6%E4%B8%80%E5%80%8B%E9%BB%9E%E5%AD%90%E4%B9%8B%E5%BE%8Cnetflix%E5%89%B5%E5%A7%8B%E7%9A%84%E7%A5%95%E5%AF%86%E9%96%B1%E8%AE%80%E5%BF%83%E5%BE%97/","title":"《一千零一個點子之後：NETFLIX創始的祕密》閱讀心得"},{"content":"以下內容包含基本的 CRUD 操作，Elasticsearch 提供良好的 REST API 呼叫介面，以下模擬情境為書店，旗下有 amazon / eslite 多家書店，每一書店儲存書本相關的資料，如書名、頁數、簡介等\n另外還有一些系統配置與進階功能，看到 Alias 功能覺得十分有趣，讓維運有更多的彈性跟方法去調整資料儲存與硬體架構\n如果想之前架構層面，可以參考 Elasticsearch 系統介紹與評估\n基本名詞解釋，Index = MongoDB 的 Collection / MySQL 的 Table； Document = MongoDB Document / MySQL Row\n基本操作 CRUD 常見的回傳值 不論請求是否成功，通常會返回 _index / _type / _id / _version / _shard 等資訊\n_version 是用來追蹤 document 被改動的次數；\n_found 代表文件是否存在\n建立 如果系統已經有規劃 _id 1 $ curl -XPUT http://localhost:9200/amazon/book/1?op_type=create 如果沒有 _id 則由 Elasticsearch 生成，預設生成的 _id 是 22字元長 + Base64 編碼 + URL 合法的字串 1 $ curl -XPOST http://localhost:9200/amazon/book 刪除 1 $ curl -XDELETE http://localhost:9200/amazon/book/1 可以看到回傳值 _version 也會被增加\n更新 / 部分更新 1 2 3 4 $ curl -XPUT http://localhost:9200/amazon/book/1 // 部分更新 $ curl -XPOST http://localhost:9200/amazon/book/1/_update 對 Elasticsearch 來說，所有的資料都是不可變的，所以更新其實是建立新的文檔並刪除舊的\n另外有個動詞是 Indexing，也就是建立與更新合在一起，沒有文檔時建立、存在時就更新\nscripting 有時候我們會希望基於現有的 document 欄位進行更新，例如說瀏覽次數 +1 等等的功能，可以透過 scripting 語法\n如\n1 2 curl -XPOST curl http://localhost:9200/amazon/book/iVKeNXMBpRlP8dKifEma/_update -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;script\u0026#34;: \u0026#34;ctx._source.page_num += 1\u0026#34; }\u0026#39; 在 Body 中夾帶 script，並指定欄位與操作即可，像這邊我是將書籍的頁面 +1\nupsert 有時候欄位要更新時可能不存在，可以透過 upsert 指定欄位不存在時的行為\n1 \u0026#39;{ \u0026#34;script\u0026#34;: \u0026#34;ctx._source.page_num += 1\u0026#34;, \u0026#34;upsert\u0026#34;: { \u0026#34;page_num\u0026#34;: 100 } }\u0026#39; 樂觀鎖與 _version 當每次有寫的操作，document 的 _version 都會被 +1，這是為了實踐樂觀鎖提供在併發狀況下的保護，在所有的操作中可以加入 querystring ?version= 確保版號 例如我要查找 id=\u0026ldquo;iVKeNXMBpRlP8dKifEma\u0026rdquo; 的書籍且確保是 version 1\n1 curl http://localhost:9200/amazon/book/iVKeNXMBpRlP8dKifEma?version=1 如果有其他人已經更動過書籍導致 version 不再是 1，此操作會拋出 409 錯誤\n1 \u0026#34;reason\u0026#34;:\u0026#34;[iVKeNXMBpRlP8dKifEma]: version conflict, current version [4] is different than the one provided [1]\u0026#34;,\u0026#34;index_uuid\u0026#34;:\u0026#34;gSaILK3KSH6UCpOeYBGKcQ\u0026#34;,\u0026#34;shard\u0026#34;:\u0026#34;0\u0026#34;,\u0026#34;index\u0026#34;:\u0026#34;amazon\u0026#34;},\u0026#34;status\u0026#34;:409 Warning：樂觀鎖僅適用於單文檔更新，Elasticsearch 沒有 Transaction 概念，所以沒有多文檔更新的一致性保證\n查詢 1 $ curl -XGET http://localhost:9200/amazon/book/1 如果想要針對多筆 document 查詢 需指定多筆的 index/type/id，如果某一個檔案不存在回傳值得 _found 就會是 false\n1 2 3 4 5 6 7 跨 index 查詢 $ curl http://localhost:9200/_mget -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;amazon\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;book\u0026#34;, \u0026#34;_id\u0026#34;: 5 }, { \u0026#34;_index\u0026#34;: \u0026#34;amazon\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;book\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;iVKeNXMBpRlP8dKifEma\u0026#34; } ] } 也可以在同一個 index/type 底下查詢，只要標注 id 就好 $ curl http://localhost:9200/amazon/book/_mget -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;ids\u0026#34;: [ 1, \u0026#34;iVKeNXMBpRlP8dKifEma\u0026#34; ] } 批次寫入操作 如果我們想要一次建立多個檔案，或是刪除等，甚至是混雜各種查詢一次性呼叫，可以使用 batch\nElasticsearch 執行 Batch 時會同時獨立處理，結果也需要去對應的 Response 查詢，如果有 Shard 就會分散到對應的 Shard 最後再把結果合併 如果操作是 create/index/update 的話，下一行是放 document 內容\n1 2 curl http://localhost:9200/_bulk -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d $\u0026#39;{ \u0026#34;create\u0026#34;: {\u0026#34;_index\u0026#34;:\u0026#34;amazon\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;book\u0026#34;, \u0026#34;_id\u0026#34;: 2 }\\n {\u0026#34;name\u0026#34;:\u0026#34;....\u0026#34;,\u0026#34;page_num\u0026#34;:991,\u0026#34;publish_date\u0026#34;:\u0026#34;2017/05/16\u0026#34;,\u0026#34;intro\u0026#34;: \u0026#34;....\u0026#34;}\\n{\u0026#34;delete\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;amazon\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;book\u0026#34;, \u0026#34;_id\u0026#34;: 5 } }\\n 需注意 Body 中是以 \\n 當作新的指令開始，且最後一筆紀錄也要以 \\n 結尾\n為什麼 Elasticsearch 不用 JSON Array 當作 Body 呢？\n這是因為如果是 Array 的話，Node 接收到之後還要去拆解 Array，接著決定哪些 Query 是屬於哪個 Shard 在包一層 Array； 直接使用 JSON Object 加上 \\n 可以不用額外的記憶體空間，用換行字符拆解 Query 就好，節省許多不必要開銷\n搜尋 Elasticsearch 在搜尋上彈性很大，可以跨 index / 跨 type 搜尋\n1 curl http://localhost:9200/_search 指定條件部分，可以用 querystring 或是 Body 夾帶，推薦後者因為彈性與可維護性更高\n指定某欄位的條件搜尋 1 curl http://localhost:9200/amazon/book/_search?q=name:Elasticsearch 用 q=${欄位名稱}:${條件}，如果有多筆則用+連結\n某文檔任一欄位符合條件搜尋 在 Elasticsearch 中，每個文件都有一個特出欄位 _all，也就是把文件中所有的字串格式欄位都拼接起來\n1 curl http://localhost:9200/amazon/book/_search?q=Elasticsearch Mapping 為了更好的支援搜尋，Elasticsearch 在寫入文件時會有建立 Schema，可以透過以下指令查詢\n1 curl http://localhost:9200/amazon/_mapping 型別有分成 text / number 系列(int, long) / date / boolean / object / geo_point 等等 非常多種\n如果欄位是 Array，則以第一個元素的型別為主，且同一 Array 中元素必須都是同型別\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { \u0026#34;amazon\u0026#34;: { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;intro\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;keyword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;ignore_above\u0026#34;: 256 } } }, \u0026#34;page_num\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, \u0026#34;publish_date\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy/MM/dd HH:mm:ss||yyyy/MM/dd||epoch_millis\u0026#34; }, ... } } } } 也可以主動針對 Index 設定 Schema 與調整型別設定，例如說索引的深度與數量等，預設 Elasticsearch 會把每個欄位都建立索引，所以記憶體消耗非常驚人\nWarning: 型別設定後只能新增欄位，不能更動既有的型別或 Indexing，最好是一開始就設定好，不然要 ReIndex ，詳見後續\n建立 Mapping 先刪除 amazon Index，重新建立 Index 與 Mapping\n假設我希望 page_num 欄位是 Integer 且不要建立索引\n1 2 $ curl -XPUT http://localhost:9200/amazon/_mapping -H \u0026#39;Content-Type: application/json\u0026#39; \\ --data \u0026#39;{ \u0026#34;properties\u0026#34;: { \u0026#34;page_num\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;index\u0026#34;: false } } }\u0026#39; 重新將資料寫入後，其餘的欄位 Elasticsearch 會幫忙補上型別，page_num 因為已經存在則不會改變 需要注意如果 index指定false 則使用 /_search?q 會無法搜尋\nAnalyzer 如果只能針對條件做篩選，這一般的資料庫也做得到，真正讓 Elasticsearch 區別於一般資料庫的地方在於 Analyzer\n每個文檔的欄位除了型別定義與索引外，還可以指定該欄位如何被分析，例如說最基本的斷詞 \u0026ldquo;中華民國\u0026rdquo; 要拆成 \u0026ldquo;中\u0026rdquo;、\u0026ldquo;華\u0026rdquo;、\u0026ldquo;民\u0026rdquo;、\u0026ldquo;國\u0026rdquo; 還是 \u0026ldquo;中華\u0026rdquo;、\u0026ldquo;民國\u0026quot;等有多種方式，決定如何斷詞會影響查詢\n另外像語意分析，如果我們想搜尋「Quick fox jumps」，我們不單希望字面上完全符合，而是找到類似下者的文檔 A quick brown fox jumps over the lazy dog\n所以 Analyzer 主要分成三個部分\ncharacter filter\n決定字元如何處理，像是轉換數字格式 / 去除 HTML tag 等 tokenizer\n決定字元如何組合成字串，英文預設是用空白，每個 Analyzer 一定也只能有一個 tokenizer token filter\n將字串做處理，例如全部轉小寫 / 過濾同義詞等 測試 Analyzer 如果不知道要怎麼選擇 Analyzer，可以看文件找出內建的 Analyzer並透過 API 去測試，指定不同的 Analyzer 與測試字串\n1 2 3 4 5 $ curl http://localhost:9200/_analyze -H \u0026#39;Content-Type: application/json\u0026#39; \\ --data \u0026#39;{ \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;text\u0026#34;:\u0026#34;this is a test\u0026#34; }\u0026#39; $ curl http://localhost:9200/_analyze -H \u0026#39;Content-Type: application/json\u0026#39; \\ --data \u0026#39;{ \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;], \u0026#34;char_filter\u0026#34; : [\u0026#34;html_strip\u0026#34;], \u0026#34;tokenizer\u0026#34;: \u0026#34;whitespace\u0026#34;, \u0026#34;text\u0026#34;:\u0026#34;this \u0026lt;a\u0026gt;iS\u0026lt;/a\u0026gt; A Test\u0026#34; }\u0026#39; 可以看到回傳值是 Analyzer 會如何 parse 字串並產生索引的 keyword，更多的參數可以參考文件 Analyze API 如果需要支援中文，則需要另外安裝 plugin\n調整欄位的 Analyzer 可以在 Index 下建立客製化的 Analyzer，例如我建立一個 my_intro_analyzer\n1 2 $ curl -XPUT http://localhost:9200/amazon/_settings -H \u0026#39;Content-Type: application/json\u0026#39; --data \\ \u0026#39;{ \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_intro_analyzer\u0026#34;: { \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;], \u0026#34;char_filter\u0026#34; : [\u0026#34;html_strip\u0026#34;], \u0026#34;tokenizer\u0026#34;: \u0026#34;whitespace\u0026#34; } } } }\u0026#39; 透過 Mapping API 去更改欄位的 Analyzer，同樣是不能更改既有的 Index，且只能指定 Analyzer 而不能在欄位中自訂 tokenizer 等\n1 2 $ curl -XPUT http://localhost:9200/amazon -H \u0026#39;Content-Type: application/json\u0026#39; --data \\ \u0026#39;{ \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;intro\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_intro_analyzer\u0026#34; } } }, \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_intro_analyzer\u0026#34;: { \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;], \u0026#34;char_filter\u0026#34; : [\u0026#34;html_strip\u0026#34;], \u0026#34;tokenizer\u0026#34;: \u0026#34;whitespace\u0026#34; } } } } }\u0026#39; 看起來 Indexing 深度與 Analyzer 分析的 Token 數都需要設定上限，否則預設值會變成記憶體怪獸!\nDSL 前面提到搜尋時可以用 querystring 加上條件，但為了設定更複雜且彈性的查詢語法，可以使用 Elasticsearch 自訂的查詢語言\nQuery 如果今天想要找欄位中是否有相近的值，可以用 match\n1 2 $ curl http://localhost:9200/amazon/_search -H \u0026#39;Content-Type: application/json\u0026#39; --data \\ \u0026#39;{ \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Distributed\u0026#34; } } }\u0026#39; Filter 如果今天想要找欄位中完全一模一樣值，可以用 term 接篩選條件\n1 2 $ curl http://localhost:9200/amazon/_search -H \u0026#39;Content-Type: application/json\u0026#39; --data \\ \u0026#39;{ \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Elasticsearch: The Definitive Guide: A Distributed Real-Time Search and Analytics Engine 1st Edition, Kindle Edition\u0026#34; } } }\u0026#39; 組合多種條件 如果今天要搜尋的條件比較複雜，例如說我希望名稱一定要包含 Distributed，頁數最好在200至500頁或是出版年份在今年(但兩者必須至少符合一項)\n可以用 bool 搭配 must 必須符合 + should 應該符合，搭配 minimum_should_match 可以決定條件的符合程度\n1 2 curl http://localhost:9200/amazon/_search -H \u0026#39;Content-Type: application/json\u0026#39; --data \\ \u0026#39;{ \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Distributed\u0026#34; } }, \u0026#34;minimum_should_match\u0026#34;: 1, \u0026#34;should\u0026#34;: [ { \u0026#34;range\u0026#34;: { \u0026#34;page_num\u0026#34;: { \u0026#34;gt\u0026#34;: 200, \u0026#34;lt\u0026#34;: 500 } } }, { \u0026#34;range\u0026#34;: {\u0026#34;publish_date\u0026#34;: { \u0026#34;gt\u0026#34;: \u0026#34;2020/01/01\u0026#34; } } } ] } } }\u0026#39; 系統配置與設定 Sharding 相關 1. 設定 Index 的 sharding 與 replica 數量 有兩種方式，一種是建立 Index 時就指定，第二種是建立 Index 後續調整\n1 2 3 4 5 6 7 1. 建立 Index 指定 curl -XPUT http://localhost:9200/amazon -H \u0026#39;Content-Type: application/json\u0026#39; --data \\ \u0026#39;{ \u0026#34;index\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 2, \u0026#34;number_of_replicas\u0026#34;: 0 } }\u0026#39; 2. 後續動態調整 curl -XPUT http://localhost:9200/amazon/_settings -H \u0026#39;Content-Type: application/json\u0026#39; --data \\ \u0026#39;{ \u0026#34;index\u0026#34;: { \u0026#34;number_of_replicas\u0026#34;: 0 } }\u0026#39; 需注意 sharding number 最好一開始就設定好，可以配置稍微多一點 Primary shard 方便後續 scale out\n後續如果要動態調整很麻煩，要把 Index 設成 read-only 並透過 Split API 修改 / 或是用 Reindex 方式重建新的 Index\n2. 指定 Index 使用的機型 有時候我們會希望某些熱門的 Index 使用較好的硬體，其他冷門的使用差一點的硬體，Elasticsearch 在機器運作時可以打上標記 --node.box_type\n1 $ ./bin/elasticsearch --node.box_type strong 指定 Index 要存放的機型\n1 2 3 4 $ POST /logs_2014-09-30/_settings { \u0026#34;index.routing.allocation.include.box_type\u0026#34; : \u0026#34;strong\u0026#34; } 結合後續的 Alias 可以配置出更符合實際應用的設定\nIndex template 當建立新的 Index 時，可以指定 template 套用設定，就不用每次都要打設定，Elasticsearch 在 7.8 版本中，可以指定 component template 與 index template；\ncomponent template 是小單位可以被用來組合的；而 index template 則是 Index 直接套用的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 // Component template PUT _component_template/other_component_template { \u0026#34;template\u0026#34;: { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;ip_address\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;ip\u0026#34; } } } } } // Index template，只要 Index 開頭是 bar 就會套用 PUT _index_template/template_1 { \u0026#34;index_patterns\u0026#34;: [\u0026#34;bar*\u0026#34;], \u0026#34;template\u0026#34;: { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 1 }, \u0026#34;mappings\u0026#34;: { \u0026#34;_source\u0026#34;: { \u0026#34;enabled\u0026#34;: false }, \u0026#34;properties\u0026#34;: { \u0026#34;host_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;created_at\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;EEE MMM dd HH:mm:ss Z yyyy\u0026#34; } } }, \u0026#34;aliases\u0026#34;: { \u0026#34;mydata\u0026#34;: { } } }, \u0026#34;priority\u0026#34;: 10, \u0026#34;composed_of\u0026#34;: [\u0026#34;component_template1\u0026#34;, ....], \u0026#34;version\u0026#34;: 3, \u0026#34;_meta\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;my custom\u0026#34; } } Reindex 與 Alias 先介紹 Alias，看到這個功能讓我覺得十分驚豔，可以將維運與開發拆分的更加獨立\n今天假設因為資料的格式問題 / Sharding 重新分配等問題需要Index 需要重建，Alias 就能派上用場\n他的概念就好像檔案連結，可以取一個連結名稱，但同時對應到多個實際的檔案路徑，例如說我有兩個 Index 想要分開儲存 /amazon + /eslite\n但我希望查詢時可以有一個共同的 endpoint 取名叫 /bookstore，就可以用 Alias 連結 /amazon 與 /eslite\n1 2 3 4 5 6 7 8 9 10 11 $ curl -XPUT http://localhost:9200/eslite/_alias/bookstore $ curl -XPUT http://localhost:9200/amazon/_alias/bookstore $ curl http://localhost:9200/boostore/_search // 同時返回 amazon / eslite 底下的文檔 $ curl http://localhost:9200/*/_alias/bookstore // 查詢哪些 Index 有設定 alias 為 bookstore $ curl -X DELETE http://localhost:9200/amazon/_alias/bookstore // 刪除 alias 雖然搜尋的時候也可以直接指定多個 Index 如 curl http://localhost:9200/amazon,eslite/_search，但如果要增減 Index 項目就需要改程式碼，十分不乾淨\n另外更大的好處在於同一個 Index 要升級時，實際儲存可以用版號如 index_name_v1，用 alias 指定 index_name；\n接著在建立新的 index_name_v2 換成新的 Index，完成後在切換 index_name 的指向就能 zero downtime 切換 index 了\n查詢時指定 alias 就可 / 寫入時如果 alias 下只有一個 index 就不用指定；超過一個必須指定寫入的 index\n拆分 Index 與 Alias 應用 假設今天我們拿 Elasticsearch 當作 Logging Service，通常是越近期的資料越熱門，時間久之後舊資料可能要移除或轉出保存\n在系統設計上，我們要考量幾個點\n儲存時區分新資料與舊資料 搜尋時希望新資料與部分舊資料都可以被查詢 舊資料的定期刪除與冷保存 書中建議，Log 依照時間區間建立新的 Index，例如每個依照每個月份儲存 2020-05 就單放五月份的 Log\n建立新的 Index 時可以透過 template 綁定預設值，就不用每次都要手動預先建立 Index 了\n假設查詢時會希望搜尋近期 3 個月的資料，與其每次都指定 3 個 Index，可以透過 Alias 簡化查詢語法\n拆分多個 Index 好處是調整非常彈性，例如說舊的 Index 可以取消 Replica / 移到較差的硬體 / 單獨備份 / 整個砍掉(效率遠比砍 document 好)\n總結 礙於篇幅，其他還有處理自然語言 / 地理位置資料 / 實際上線的注意事項等等進階議題，只能等之後真的有用上再來分享\n","date":"2020-07-15T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-07-15-elasticsearch-%E6%95%99%E5%AD%B8-api-%E6%93%8D%E4%BD%9C/","title":"Elasticsearch 教學 - API 操作"},{"content":"最近有一些自建搜尋引擎的需要，所以找上了 Elasticsearch 這一套開源的分散式的 NoSQL Database，這一篇文章主要分享自己如何評估 Elasticsearch，著重在系統的架構與內部實作機制，教學預計會拉到另外一篇文章\n以下內容大多參考此本書：《Elasticsearch: The Definitive Guide: A Distributed Real-Time Search and Analytics Engine》，雖然是 2015 的書，但內容相當的豐富，還有豐富的內部實作機制/Cluster架構等等的補充，除了 api 有些參數要更新外，不會因為時間而抹滅他的價值，包含了有趣的浪漫故事\nElasticsearch 原作者其實是為了幫老婆寫一個食譜搜尋引擎而開始的，但過了好幾年 Elasticsearch 都變成公司了但老婆依然還沒拿到說好的食譜搜尋引擎\u0026hellip;. (截至 2015年)\nTerminology 與基礎介紹 Elasticsearch 是建構在 Lucene 這個 Java 開發的搜尋引擎上，增加了分散式與 API 呼叫的介面；\n資料層級分成 Index \u0026gt; Document，每個 Index 當第一筆 Document 建立後會有隱式的欄位型別，後續的 Document 都要遵守型別，其中 Index 這個詞蠻容易誤會，因為他除了代表最上層的資料集合外，也代表索引的同義詞\nElasticsearch 儲存時是用 JSON 格式，可以對每個欄位設定型別，同時可以指定是否支援全文搜尋\n為了支援全文搜尋，Elasticsearch 使用 Reverted index，也就是用一張表紀錄一個詞在哪些 Document 中的欄位出現過\nWarning: 在 6.x.x 版時 Elasticsearch 移除了 Type 層級，主要是因為當一個 Index 下有多個 Type，假設多個 Type 有相同名稱的欄位，對於 Lucence 來說會把兩個相同名稱欄位都當作同一個，所以如果想要兩個不同 Type 中相同名稱欄位設定不同型別是會拋出錯誤的 (ex. Index 下有 User / Twitter 兩個 Type，且兩個 Type 都有 user_name，則 user_name 必須是相同型別)，所以經過考量後就移除了\n參考文件 Removal of mapping types\n在建立索引時，Elasticsearch 會將欄位經過 Analyzer 處理，也就是 charaterizer -\u0026gt; tokenizer -\u0026gt; token filter\n所以能夠支援像過濾介系詞 / 將文字轉成小寫或是同義詞轉換 / 支援不同語言的斷詞等靈活的語言處理流程，所以才能夠提供別於一般資料庫死板的全文搜尋\n在資料更新與查詢時，Elasticsearch 提供近乎即時的操作，同時查詢支援依照條件篩選 (Filter) 或是依據關聯度排序 (Query)\n前者像是 給我 id 是 123 的資料這類 yes/no 問題；後者是回答 幫我找出跟 Mary 相關的文檔 這類模糊搜尋的結果\n架構介紹 Elasticsearch 支援分散式，可以在 Index 層級指定要 Sharding 配置方式，Shard 有兩種角色 Primary / Replica，前者負責讀寫而後者負責從 Primary 備份資料並分散讀的查詢，每一個 Shard 執行獨立的 Lucene，可視為獨立的 Worker 即使在單機上跑，Elasticsearch 也會按照配置，將資料拆分儲存\n如果 Cluster 此時增加機器，Elasticsearch 會自動將 Shard 分散到各個節點上，如果有 Replica 則盡可能分散風險讓每台機器都有備援資料 Cluster 之間會有一個 Master，Master 主要管理整個 Cluster / 加入、移除節點 / 建立 Index 等，例如查詢/寫入都是個節點能夠獨立完成，所以不需要擔心 Master 會成為系統的 Single Point of Failure\n節點發生問題，Cluster 會自動票選 Master 並重新做資料搬移的動作 實際上 Node 有分很多類型，像是 Master Node 負責管理 Cluster / Data Node 儲存資料 / Ingest Node 預處理流程等等\n服務評估 主要是針對各種維運上的點進行考量，看到 分散式資料庫都會有三個特點\n**Replica**: 資料如何備援 **Sharding**: 如何拆分資料集 **Cluster**: 如何水平擴展機器 一般來說分散式資料庫的 Replica 都是由 active slave 擔任，即時備份所有的更新，同時能夠支援讀取的分擔，各家提供的功能都差不多；\n至於 Cluster / Sharding 則各家資料庫的實作與考量各有不同，同時考量到 CAP 在可用性與資料一致性之間做取捨這是我覺得最有趣的地方\n實際評估上會去思考以下幾個案例，同時以我比較熟悉的 MongoDB / Redis 作為對比\n基礎資料庫\na. 寫資料時如何保證 Durability\nb. Concurrency 下的 Consistency 保證是什麼 Sharding\na. Sharding 的依據是什麼 / 是否能夠動態增減 shard\nb. Query 如何 routing 到正確的 shard 上面 Cluster\na. Cluster 基本架構要如何搭建\nb. Cluster 調整時需要做什麼改動\nc. Cluster 增減 node 時會有什麼變化\nd. Cluster 在什麼情況下會不可用 基礎資料庫 寫資料時如何保證 Durability Elasticsearch 為了平衡 Durability 與效能，所以當 Reverted Index 建立後就是 不可變 immutable的，這樣的好處是性能帶來很大的提升，資料載入 memory 後不用擔心會變動 / mulit thread 也不用擔心資料過期的問題等等\n但資料不可避免會遇到更新或刪除的需求，所以 Elasticsearch 採用 Segment 方式，每次建立 Index 後就是一個獨立的 Segment，累積一批修改後在建立一個新的 Segment\n查詢資料時就同時查找多個 Segment，如果同筆資料在多個 Segment 都有紀錄，則採取最新一筆的資料\n實際的Elasticsearch 內部儲存機制的方式參考下圖\n儲存機制如下\n新的修改產生，寫進 in-memory buffer 中，此時資料不能被搜尋 [Refresh] 固定時間 refresh 到 file cache (灰色的 Segment，代表還沒真正儲存到硬碟上)，此時資料可以被搜尋 (預設每 10秒) [Flush] 固定時間 file cache 透過 fsync 寫入硬碟，此時才是真正的持久化 (預設每 30分鐘或是 Translog過大時) 每次 Flush 會產生新的 Segment，但如果 Segment 太多會造成檔案過多性能反而下降，會固定時間合併多個 Segment\n但要小心 Segment 合併會吃掉大量的記憶體與 CPU，所以也不能夠太頻繁，可以指定 max_num_segments 參數 Translog 是為了確保 2~3 步驟如果機器發生問題，可以透過 Translog 回復資料\nTranslog 將每筆操作順序寫入硬碟，當第三步 flush 後一併清空\n但 Translog 本身也有 fsync 的頻率，預設是 5 秒鐘，所以有機會丟失 5秒鐘內的資料 Redis 在持久化保存有分 RDB 與 AOF，RDB 是透過保存整份 Snapshot / AOF 則是寫入每筆操作 log 預設每秒 fsync 到硬碟上\nConcurrency 下的 Consistency 保證是什麼 對 Elasticsearch 進行寫入操作時，針對 Request Level 可以做不同的保證性，預設是 sync，也就是 Primary + Replica 都寫入成功才返回，資料不用擔心會不見；\n可以調整成 async，只要 Primary 寫入成功就返回，但可能資料會不見； 但書中提到建議還是用 sync，因為如果 Elasticsearch 此時負載較高，可以讓 Request 較晚回覆產生 back preasure，否則可能會讓系統乘載過多的 Query\n在併發的狀況下，Elasticsearch 採用樂觀鎖，透過_version去比對更動是否為正確的版本，預設的_version 是遞增的數字，也可以用自定義的 _version\nSharding Sharding 的依據是什麼 / 是否能夠動態增減 shard Elasticsearch 在建立 Index 時，可以指定 sharding 的數量，設定後如果要更動，就必須要先關閉 Index 寫入並調整設定 至於資料如何分配到 shard，則是由內部的 hash function 對 id 做運算，Elasticsearch 會自動 balance 資料\nMongoDB 則是需要對 collection 指定 shard key，可以分成 range shard / hash shard，前者可自己指定 shard 要儲存的 key 範圍，後者同樣有內部 hash function 決定，MongoDB 會自行負責 balance；\nRedis cluster 預設有 16384 個坑，透過 hash 運算(CRC16 mod 16384)分散 key 到現有的 Node 上面，如果動態增減 cluster Redis 會自行處理資料搬移\nQuery 如何 routing 到正確的 shard 上面 Elasticsearch 每一個 Node 都能夠處理 Query，會在內部自己 routing 到資料所在的 Node 上，另外為了效能，如果是讀取且有 replica，Elasticsearch 會用 Round-Robin 去分散讀取的壓力\nMongoDB 則需要 Mongos 去處理 query，所以多一個服務要架設，也造成服務多一個中斷的可能；\nRedis Cluster 也是每一個 Node 都能處理 Query\nCluster Cluster 基本架構要如何搭建 Elasticsearch 在 Cluster 架構上非常彈性，要幾個 Node 都可以，同時可以針對不同的 Indices 去調整 sharding / replica 數量，Elasticsearch 會以容錯度最高的方式自動分配\n但建議還是 Cluster 還是 3 台以上，才可以讓可用性更高\n如果只有兩台，發生 Network partitioning，假設要保證 Consistency 要過多數的小 Cluster 能夠運作(避免 split brain)，則兩邊各一台就會服務中斷，在 Network partitioning 下沒有任何的容錯性\n所以 Cluster 基本都 3台起跳且選擇奇數\nMongoDB Cluster 只少要 3 個 Node (至少兩個可儲存的 Node)，如果是 Sharding Cluster 則需要 Config Server (3台) + Replica set (3台起跳) * n，外加 Mongos\nRedis Cluster 則也沒有限制，要加幾個 Node 都可以，如果要備援則設定 slave node\nCluster 調整時需要做什麼改動 Elasticsearch 再增加新的 Cluster Node 時不需要額外的設定，只要確保 Network 可以 multicast 且設定同一個 cluster name，就會自己加進去\nMongoDB 需要改 Config Server 設定檔 / Redis Cluster 也是\nCluster 增減 node 時會有什麼變化 Elasticsearch 再設定檔，可以指定 Primary 與 Replica 的數量，並平均分散到每一個 Node 上\n例如說 Primary 2 + replica 1，則會產生 P1 / P2 + R1 / R2，假設目前有兩個 Node 他會自動分配成 P1 + R2 / P2 + R1 分散風險，掛了一台資料也不會掉\nMongoDB 的 Cluster Node 主要有兩種 secondary / 僅投票用的 Arbiter (撇除 Primary)\nRedis Cluster 沒有特別區分 Node 性質\nCluster 在什麼情況下會不可用 Elasticsearch / MongoDB / Redis Cluster 這方面都是類似的，當發生 partitioning 時，只有超過半數的 cluster 可以運作\n","date":"2020-07-08T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-07-08-elasticsearch-%E7%B3%BB%E7%B5%B1%E4%BB%8B%E7%B4%B9%E8%88%87%E8%A9%95%E4%BC%B0/","title":"Elasticsearch 系統介紹與評估"},{"content":"一般的 API Endpoint 設計採用 RESTful API規範，不過 RESTful 比較指稱的是 Client/Server 的架構設計而不是侷限於 API Endpoint 的設計規範，更準確的說法是 HTTP + JSON 格式，佔據多數的 API 設計近一二十年 (SOAP/WSDL 因為工作中沒有使用過就不描述)\n但因應新的網路應用程式，不斷有新的設計嘗試去更進 API 設計，近年有 Facebook 提出了 GraphQL，將讀取資料的彈性交還給 Client，適應多屏幕多裝置 Client 的應用場景\n又或是今天要探討的 gRPC，由 Google 基於 http/2 提出且廣泛應用在微服務架構中，主要希望改善幾個問題\nAPI 文件化與 Client 實作繁雜\n一般的 API 設計是 Server 開好 HTTP endpoint 後，定義好參數並撰寫文件，接著 Client 再閱讀文件實作；\n好處是解耦的很徹底，不管有幾個 Client 或是使用什麼程式語言，按照 API 介面實作即可； 但同樣的要維護時相對成本也比較高，像是參數的名稱與型別，都必須雙方去維護與閱讀文件，再用程式碼實作，中間多了一層的轉換 gRPC 預設採用 protocol buffer 當作 IDL (介面定義語言)，將 Endpoint 提供的服務/參數/回傳值都定義好名稱與型別，當作 Server / Client 實作的 Interface，大幅降低雙方溝通的成本 增加傳輸效率\ngRPC 預設使用 protocol buffer 當作編碼/解碼傳輸內容，比用文字描述的 JSON 在檔案大小與網路傳輸效率上更具備優勢，專案中有提供 Android client benchmark，gRPC 的延遲遠比 JSON 要快上許多 應用彈性 傳統的 HTTP 就是 Request/Response 一來一往，在 grpc 中稱為 Unary Call，但是 gRPC 額外提供 streaming，可以分批在同一個 request response 中傳送多次 payload，設計更有彈性的溝通方式 HTTP verb 並無法完整描述資源的操作 之前設計 API 頭痛的是 HTTP 的動詞 (GET/POST) 等無法完整描述所有的操作，例如說批次刪除，後來參照 Google API 設計文件有找到解法，但還是有這麼一點彆扭\n採用 gRPC 就沒有這方面的規範(與困擾 ?! 目前 gRPC 由 Google 開源並主力維護，採用的大廠也有不少，也支援許多程式語言 Java/JS(Nodejs \u0026amp; browser)/Python/PHP 等等，Android/iOS App 也都有支援的 Library；\n觀念上要找到映射於 HTTP 還蠻容易的，像是 Http header 對應 gRPC metadata / Http Status Code 對應 gRPC 也有同樣的回傳格式\n以下內容包含\nprotocol buffer 簡介與編碼機制 gPRC 四種傳輸方式介紹與 server 端實作 錯誤處理與驗證機制 (*Warning 有坑) 但目前不包含 Client side 實作\nNodejs 實作 以下會實作一個簡單的 To-Do List，demo code 於此 grpc-demo\n定義 .proto 檔案 在專案路徑下，建立一個資料夾 ./protos 放\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import \u0026#34;google/protobuf/empty.proto\u0026#34;; syntax = \u0026#34;proto3\u0026#34;; package ToDoService; message ToDoItem { string title=1; string author=2; bool isDone=3; double createDate=4; } message ToDoList { repeated ToDoItem ToDoList = 1; } message GetQueryOptions { string author = 1; } service ToDoService { rpc CreateToDo (ToDoItem) returns (ToDoList); rpc createMultiToDo (stream ToDoItem) returns (google.protobuf.Empty); rpc GetToDoListByAuthor (GetQueryOptions) returns (stream ToDoItem); rpc GetToDoListByAuthorOnFly (stream GetQueryOptions) returns (stream ToDoItem); } import 可以從其他地方載入 proto 檔案使用裡面的宣告 syntax = \u0026quot;proto3\u0026quot;;\n標註使用 protobuffer version message 宣告型別名稱 { 型別 參數名 = 順序}\n型別部分可以參考官網，有提供 int / uint / float / string 等多樣基礎型別，也可以使用自定義型別；\n順序的部分，從 1 開始一路遞增，型別內部的參數順序不能重複，且越小的數字通常是越常使用到的參數，詳見後續補充 service 服務名稱 { rpc 函式名稱 (參數) returns (回傳值) }\n如果希望將 proto 用於 rpc，就需要宣告 service 類別，stream 表示參數或回傳值可能會是批次傳送 payload\n這邊定義了四個函式，建立 ToDoItem / streaming 批次建立 ToDoItem / 依照作者批次取得 ToDoItem / 動態改變搜尋作者並動態依照條件回傳 ToDoItem Server side 實作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 const path = require(\u0026#39;path\u0026#39;); const grpc = require(\u0026#39;grpc\u0026#39;); const protoLoader = require(\u0026#39;@grpc/proto-loader\u0026#39;); const toDoServiceImplementations = require(\u0026#39;./implementations/todoService\u0026#39;); async function main() { const server = new grpc.Server(); const PROTO_PATH = path.join(__dirname, \u0026#39;./protos/todo.proto\u0026#39;); const packageDefinition = protoLoader.loadSync( PROTO_PATH, { keepCase: true, longs: String, enums: String, defaults: true, oneofs: true }); const toDoProto = grpc.loadPackageDefinition(packageDefinition).ToDoService; server.addService(toDoProto.ToDoService.service, toDoServiceImplementations); server.bind(\u0026#39;0.0.0.0:50051\u0026#39;, grpc.ServerCredentials.createInsecure()); server.start(); } main(); 以上步驟大概是\n建立 grpc server instance 載入上一步定義的 proto 將 proto 與實作的 function 結合 server bind port 並開始運行 接著看 implementation 部分\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 const grpc = require(\u0026#39;grpc\u0026#39;); let todoList = [ { title: \u0026#39;Hello\u0026#39;, author: \u0026#39;Hello2\u0026#39;, isDone: true, createDate: 1.4 }, { title: \u0026#39;Hello2\u0026#39;, author: \u0026#39;Hello\u0026#39;, isDone: true, createDate: 1.4 }, { title: \u0026#39;Hello3\u0026#39;, author: \u0026#39;Hello\u0026#39;, isDone: true, createDate: 1.4 }, { title: \u0026#39;world\u0026#39;, author: \u0026#39;world\u0026#39;, isDone: true, createDate: 1.4 }, { title: \u0026#39;world2\u0026#39;, author: \u0026#39;Hello\u0026#39;, isDone: true, createDate: 1.4 }, { title: \u0026#39;world3\u0026#39;, author: \u0026#39;Hello2\u0026#39;, isDone: null, createDate: \u0026#34;1.4\u0026#34; } ]; function CreateToDo(call, callback) { const clientToken = call.metadata.get(\u0026#39;token\u0026#39;)?.[0]; if(clientToken !== \u0026#39;Secret\u0026#39;){ return callback({ error: grpc.status.PERMISSION_DENIED, message: \u0026#34;No token\u0026#34; }) } todoList.push(call.request); if(todoList.length \u0026gt; 5){ return callback({ error: grpc.status.OUT_OF_RANGE, message: \u0026#34;too many ToDoItem\u0026#34; }) } callback(null, { ToDoList: todoList }) } async function createMultiToDo(call, callback) { call.on(\u0026#39;data\u0026#39;, (data) =\u0026gt; { todoList.push(data); }) call.on(\u0026#39;end\u0026#39;, () =\u0026gt; { callback(null, {}); }) } function GetToDoListByAuthor(call) { const author = call.request.author; async function main() { let isAny = false; for (const todoItem of todoList) { if (author === todoItem.author) { isAny = true; call.write(todoItem); await wait(1); } } if (isAny === false) { return call.emit(\u0026#39;error\u0026#39;, grpc.status.PERMISSION_DENIED) } call.end() } main() } async function GetToDoListByAuthorOnFly(call) { let author = null; call.on(\u0026#39;data\u0026#39;, (data) =\u0026gt; { console.log(data) author = data.author; main(); }); call.on(\u0026#39;end\u0026#39;, () =\u0026gt; { call.end(); }); async function main() { for (const todoItem of todoList) { if (author === todoItem.author) { call.write(todoItem); } await wait(3); } } } async function wait(sec) { return new Promise((res) =\u0026gt; setTimeout(() =\u0026gt; res(), sec * 1000)); } module.exports = { CreateToDo, createMultiToDo, GetToDoListByAuthor, GetToDoListByAuthorOnFly, } 可以看到 handle function 共有四種\nhandleUnaryCall(call, callback) // CreateToDo handleClientStreamingCall(call, callback) // createMultiToDo handleServerStreamingCall(call) // GetToDoListByAuthor handleBidiStreamingCall(call) // GetToDoListByAuthorOnFly 拆成 client / server 兩部分\n如果 client 是送 unary data，則直接從 call.request 讀取傳送值 如果 client 是送 streaming data，則使用 call.on('data', (data)=\u0026gt;{}) / call.on('end', ()=\u0026gt;{}) 處理資料與傳送結束 如果 server 是送 unary data，則handle function 第二個參數為 callback，callback 常用前兩個參數，代表 error 跟 data，如果沒有錯誤則回傳 callback(null, myData) 如果 server 是送 streaming data，則用 call.write(myData)，結束傳送呼叫 call.end() 搭配的 GUI Client 工具可以參考 bloomrpc，載入 proto 檔案後就會自動跳出定義的 service 與預期的回傳結果，相當的方便，這也是使用 gRPC 的一大好處\n傳入多餘的參數或型別錯誤 在使用預先型別定義的設計時，不免腦中浮現如果我不按照定義的話會怎麼樣？\n如果是傳入多餘的型別，protobuffer 在 version 2 \u0026amp;\u0026amp; version 3.5 以上會保留，目前版本到 3.12 了\n如果是 型別不對，內部採用的 encode / decode 是基於 protobufjs，會使用 Number / Boolean 等 JS 類別來轉換型別，例如 number \u0026ldquo;123\u0026rdquo; 就會變成 123\n錯誤處理 根據前面所描述，Server 回應時會分成 Unary 跟 Streaming，兩種的錯誤回傳機制不同；\n這部分有點小坑，官方範例只有示範 Unary Response 時的錯誤處理，也就是呼叫 callback 的第一個參數\n1 2 3 4 callback({ error: grpc.status.OUT_OF_RANGE, message: \u0026#34;too many ToDoItem\u0026#34; }) 先前定義 service 時並沒有宣告錯誤回傳，這是因為 gRPC 內建錯誤訊息，格式如下\n1 2 3 4 { string error string message } error 對應於 HTTP status code，可以從 grpc 的靜態參數取得如 grpc.status.PERMISSION_DENIED 等同於 403，message 則是自定義的字串，需注意 Nodejs 不支援 rich format，以下截自官方文件\nThis richer error model is already supported in the C++, Go, Java, Python, and Ruby libraries, and at least the grpc-web and Node.js libraries have open issues requesting it.\n所以 gRPC client 收到的錯誤訊息會被轉成\n1 2 3 { \u0026#34;error\u0026#34;: \u0026#34;2 UNKNOWN: too many ToDoItem\u0026#34; } 另外 Streaming response 傳遞錯誤的方式是\n1 call.emit(\u0026#39;error\u0026#39;, grpc.status.PERMISSION_DENIED) emit error 後會自動 close，目前只支援 status，不能傳遞物件，否則 connection 無法被 close，呼叫 call.end() 也沒有用\n如果要有更豐富的錯誤格式，就需要自己定義了\n驗證機制 gRPC 內建兩種驗證相關的機制，一種是 SSL/TLS，提供通訊上的點到點加密，另一種是 Google 服務的 OAuth Token 驗證機制，後者僅限於與 Google 服務對接才有用；\n當然也可以用 middleware 方式自行實作驗證機制\n驗證機制有兩種 scope，一種是 Channel Level，也就是適用於 gRPC 連線，另一個是 Call Level，也就是每次呼叫，這部分是使用於 Client side\n1 2 var ssl_creds = grpc.credentials.createSsl(root_certs); var stub = new helloworld.Greeter(\u0026#39;myservice.example.com\u0026#39;, ssl_creds); 這邊為了實作方便性，將 token 放置於 metadata 之中，要從 server side 讀取使用 call.metadata.get('Key') 即可\nCache？ 另一個 HTTP (RESTful 架構下) 原有的設計是 Cache 機制，又分成 server / proxy / client 三者處理，gRPC 看起來有類似的規劃，但目前還在實驗階段 Provide support for caching GRPC method response #7945\nProtocol buffer Encoding 機制 參考官方文件 Protocol buffer Encoding，先前提到 Protocol buffer 會將訊息編碼成 binary 格式，而 JSON 則維持文字格式，這邊簡單介紹 Protocol buffer 編碼的過程\nVarints varints 是一種用多個 bytes 表達數字的方式，除了最後一個 byte 外，其餘 byte 的第一位元表示後續是否還有 byte，byte 的順序為最低有效位(越前面的 byte 是低位) least significant group first.，所以實際上每個 byte 是用 7 bits 表達數值\n例如說 1010 1100 0000 0010 代表 300，因為 1 010 1100 1 代表後面還有 byte 相連，0 000 0010 0 則表示他是最後一個 byte 了；\n因為最低有效位，所以重組成 000 0010 010 1100，也就是 300\nKey Value 其實 Protocol buffer 就是編碼一連串的 Key-Value，在編碼時會以 編碼號(5 bits) 類別(3 bits) 數值表示，例如說\n1 2 3 message Test2 { optional string b = 2; } 假設 b 儲存了 \u0026ldquo;testing\u0026rdquo;，那時記得編碼結果是 12 07 74 65 73 74 69 6e 67，拆解 12 成 0001 0010 =\u0026gt; 00010 010 對應到欄位編號 2 + 數值類別 2(代表是自訂長度的類別，如 string / object 等)；\n接著 07 表示接下來 7 個 bytes 是數值表示；\n後續的數值是 utf-8 編碼的顯示\n接著看\n1 2 3 4 5 6 7 message Test1 { optional int32 a = 1; } message Test3 { optional Test1 c = 3; } 假設 Test1 a 儲存 150，則編碼結果hex 表示為 08 96 01，也就是 00001 000 欄位編號 1 + 數值編號 0 也就是 varints；\n96 01 則是 1001 0110 + 0000 0001 並依照 varints 表示法轉乘 000 0001 001 0110 也就是 150\n接著 Test3 儲存 Test1，假設 Test1 的 a 等於 150；\nhex 表示法為 1a 03 08 96 01，也就是 00011 010，欄位 3的類別是 2，接下來 03 共 3 個bytes 為數值，也就是上一步的 08 96 01\n總結 gRPC 會全面取代 HTTP + JSON 嗎？ 這個問題或許有點像 Deno 會不會全面取代 Nodejs，現在談好像有點過早，畢竟 HTTP + JSON 行之有年，多方平台的支援度還是比較好，包含像 Proxy 等中介網路服務\n但是 gRPC 在某些用途上，基於開發效率 / 傳輸速率等，確實很值得投資與嘗試的技術\n","date":"2020-06-07T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-06-07-grpc-%E4%BB%8B%E7%B4%B9%E8%88%87-nodejs-%E5%AF%A6%E4%BD%9C%E5%88%86%E4%BA%AB/","title":"gRPC 介紹與 Nodejs 實作分享"},{"content":"之前在閱讀 ES6 相關教學時，有提及 Proxy / Reflect 這兩個新的內建物件型別，Proxy 主要是作為指定物件的代理，可以改寫、偵聽物件的存取與操作 / Reflect 則是用靜態方法操作物件，完善 Proxy handler 的實作；\n當初有看沒有懂，也想不到應用的場景，直到最近在開發應用程式時，遇到要包裝 API 自動 retry 機制\n針對不同的 API 錯誤集中化處理，可能是單純 retry 或是呼叫其他 API 換新的 token 之類的\n如果要每次 api call 時去 catch error 並處理是一件非常頭疼且難以管理的事情，臨機一動想到 Proxy 這個好幫手，目前用起來蠻順利的，以下分享 Proxy / Reflect 基本介紹，以及如何應用在 API retry 機制的實作\n文章內容大多參考自 javascript.info: Proxy and Reflect，個人覺得寫得比 MDN 詳盡且易懂\nProxy 當我們在調用 Proxy 時，會這樣宣告\n1 var p = new Proxy(target, handler); target\n要被 Proxy 代理的物件對象，只要是 Object 型態都可以，包含 Array / Function 等，如果不是 Object 宣告時會收到錯誤 1 Uncaught TypeError: Cannot create proxy with a non-object as target or handler handler\n選擇要指定觸發的時機，Proxy 會產生所謂的 trap，也就是攔截物件操作的方法，如果未定義則直接呼叫原 Target 在物件的操作上，都會有對應的內部呼叫方法(internal methods)，例如說 new A() 代表呼叫了 A 物件的 [[Contructor]] 方法，常用的 get / set 如 a.prop / a.prop = 'hello world' 則分別呼叫了 [[Get]] / [[Set]] 等方法，而 handler 則是對應這些方法產生攔截的定義\n另外有些物件會有內部的儲存資料格式，稱為 internal slot，例如 Map 的內部資料格式是 [[Mapdata]] 而不是透過 [[Get]]/[[Set]]，這類型就不能透過 Proxy 代理\n基本案例 - 改寫 get 設定預設值 我們希望在取得陣列時，遇到超出範圍則回傳預設值 0\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 let numbers = [0, 1, 2]; numbers = new Proxy(numbers, { get(target, prop, receiver) { if (prop in target) { return target[prop]; } else { return 0; // default value } } }); console.log(a[1]) // 1 console.log(a[-1]) // 0 在 handler 中定義 get 可以攔截 [[Get]] 呼叫，會收到三個參數 target / prop / receiver\ntarget 目標物件，也就是 number 本身 prop\n呼叫的屬性名稱 receiver:\n執行 target[prop] 時的 this 代表值，通常是 Proxy 本身，但如果是有繼承等實作會不太一樣，後續會補充 另外方法呼叫也會觸發 get喔，例如 a.method()\n第二個案例 - 改寫 set 統一驗證方式 在寫入表單時，可能會用一個物件暫存用戶的輸入，但此時都需要欄位的驗證，例如手機號碼 / 地址格式等等\n如果要將邏輯散落在每一個輸入後的 function 有點麻煩\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 let numbers = []; numbers = new Proxy(numbers, { // (*) set(target, prop, val, receiver) { // to intercept property writing if (typeof val == \u0026#39;number\u0026#39;) { target[prop] = val; return true; } else { return false; } } }); numbers.push(1); // added successfully numbers.push(2); // added successfully alert(\u0026#34;Length is: \u0026#34; + numbers.length); // 2 numbers.push(\u0026#34;test\u0026#34;); // TypeError (\u0026#39;set\u0026#39; on proxy returned false) set 會收到四個參數，並注意需要回傳 boolean 表示 set 是否成功\n其餘像是 construct / getPrototypeOf / ownKeys 等等的方法\n實際使用 - 正式環境複寫 console 行為 在開發時，為了 debug 方便回留下很多 console 呼叫的方法，但如果上到正式機忘記關閉就會很尷尬；\n同時像 error / warning 會需要用其他的方式送回 server 紀錄錯誤 log，避免正式機除錯不易，此時用 Proxy 去包 console 就是一個蠻方便的做法\n1 2 3 4 5 6 7 8 9 10 11 12 window.console = new Proxy(window.console, { get: function(target, prop, receive){ if(prop === \u0026#39;log\u0026#39; || prop === \u0026#39;debug\u0026#39;){ alert(\u0026#34;你看不見我\u0026#34;); return ()=\u0026gt;{}; } return target[prop] } }) console.log() // 彈出 alert console.error(\u0026#34;123\u0026#34;) // 照常顯示 當然也可以自定義 Log Class 達到一樣的效果，但會覺得用 console.log 是個很直覺的做法，如果能用 Proxy 去改寫達到一樣的效果比較方便 (懶\nreceiver 應用 剛才提到 get 第三個參數 receiver 指的是函式執行 this 所代表的物件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 let user = { _name: \u0026#34;Guest\u0026#34;, get name() { return this._name; } }; let userProxy = new Proxy(user, { get(target, prop, receiver) { if(prop === Symbol.iterator || prop === Symbol.toStringTag || prop === Symbol.for(\u0026#39;nodejs.util.inspect.custom\u0026#39;)){ return; } console.log({ target, prop, receiver }) return target[prop]; // (*) target = user } }); let admin = { __proto__: userProxy, _name: \u0026#34;Admin\u0026#34; }; console.log(userProxy.name) // outputs: Guest console.log(admin.name);// Expected: Admin but outputs: Guest (?!?) 宣告一個 user 物件，並用 userProxy 代理，最後 admin 用 __proto__ 方式繼承 userProxy，透過 userProxy get 可以看出 target 都指向 user，但是 receiver 就不一樣，兩者都指向呼叫的自身 (Proxy / Admin)\n但因為最後執行是透過 target[prop]，所以 this 指向的都是 user\n如果希望 admin.name 最後印出 \u0026ldquo;Admin\u0026rdquo;，也就是需要讓執行時 this 指向 admin，就需要 Reflect 協助\n記得要避免在 proxy handler get 中直接呼叫 receiver[prop]，因為會不斷透過 [[GET]] -\u0026gt; Proxy get -\u0026gt; [[GET]] -\u0026gt; Proxy get 輪迴\n這一段是用 node.js 執行，需加入 if condition 避免不斷的遞迴呼叫，因為在 console.log 時會主動去 iterate 物件並呼叫 toString，這些也會觸發 [[GET]]\nReflect Reflect 是 ES6 新增的類別，不能透過 new 建構新的 instance，只能呼叫靜態方法，主要是針對物件操作的方法，例如\n1 2 3 4 5 6 7 8 9 10 11 const a = {b: 123} a.b // 123 Reflect.get(a, \u0026#34;b\u0026#34;) // 123 const object1 = { property1: 42 }; delete object1.property1 Reflect.deleteProperty(object1, \u0026#39;property1\u0026#39;); 基本上都有一對一的方法可以調用\n剛才提到，getter 時第三個參數 receiver 可以變成 target 呼叫時的 this 指向，例如\n1 2 3 4 5 let a = {c: 123, get d(){ console.log(this); return this.c}} let b = {c: 456} Reflect.get(a, \u0026#34;d\u0026#34;) // 123 Reflect.get(a, \u0026#34;d\u0026#34;, b) // 456 所以當我們希望指定 getter 實際操作的物件，可以用 Reflect.get 去取代 target[propd]，這是一種最安全的做法，結合 function call 用以下方式最為保險\n1 2 3 4 5 6 new Proxy(user, { get(target, prop, receiver) { let value = Reflect.get(...arguments); return typeof value == \u0026#39;function\u0026#39; ? value.bind(target) : value; } }); 在某些時候，用 Reflect.get 可以避免不預期錯誤，例如說 Map，Map 在讀寫參數時是透過 this.[[MapData]] 而不是 this.[[Get]]/this.[[Set]]，所以如果沒有指定 receiver 則預設 this 指向 Proxy 就會拋出錯誤，要改用 Reflect.get 將 this 替換成 Map 本身才不會有問題\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 let map = new Map(); let proxy = new Proxy(map, {}); proxy.get(\u0026#39;test\u0026#39;); // 錯誤: Uncaught TypeError: Method Map.prototype.get called on incompatible receiver /// 正確方法 let map = new Map(); let proxy = new Proxy(map, { get(target, prop, receiver) { let value = Reflect.get(...arguments); return typeof value == \u0026#39;function\u0026#39; ? value.bind(target) : value; } }); proxy.get(\u0026#39;test\u0026#39;) API retry 機制 個人還是蠻喜歡用 axios 的而不是用原生的 fetch，可能是因為 axios 更像一個物件，可以透過 create 創建 instance 蠻方便的\n接著用 Proxy 代理 function 的呼叫，並回傳一個 async function，在裡頭就能自定義錯誤處理機制，例如說收到 403 就去換新的 token 之類的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 const APIInstace = axios.create({ baseURL: \u0026#39;https://httpstat.us\u0026#39; }) const APIProxy = new Proxy(APIInstace, { get(target, prop, receiver){ let fn = Reflect.get(...arguments); return async function(){ try{ const result = await fn(...arguments); return result; }catch(error){ if(error?.response?.status === 403){ const result = await APIInstace.get(\u0026#34;https://www.mocky.io/v2/5ed11b963500005b00ffa29a\u0026#34;); return result; } throw \u0026#34;OhNo\u0026#34; } } } }) console.log(await APIProxy.get(\u0026#34;/403\u0026#34;)); // 沒有錯誤 await APIProxy.get(\u0026#34;/404\u0026#34;); // 拋出 OhNo 錯誤 ","date":"2020-05-27T08:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-05-27-js-proxy-/-reflect-%E5%AF%A6%E6%88%B0-%E5%AF%A6%E4%BD%9C-api-%E8%87%AA%E5%8B%95-retry-%E6%A9%9F%E5%88%B6/","title":"JS Proxy / Reflect 實戰 - 實作 API 自動 retry 機制"},{"content":" 內容摘錄自影片介紹，要討論 Vault 的功能之前，先退一步回來看最一般的機敏資料的定義與管理方式\n機敏資料主要分成 驗證(Authentication) 與 授權(Authorization)，一者表明身份一者決定操作的權限，可能是 DB 的權限 / 第三方服務的 API Token / 用來加解密的對稱金鑰等等，這些資料可能被放在原始碼當中 / config 文件 / 環境變數 / 版本控制等等四散各地，之前 Github 有提出許多開發者都誤上傳機敏資料；\n另一方面時間一久沒有人知道到底發出去哪些 key 又或是誰有在用，糟糕的是可能一組 key 有多個人使用\nVault 提出三個層級的解決方案\n1. 集中化管理 搭建 Vault Server 集中管理所有的機敏資料，在 Vault Server 中確保所有的機敏資料都是被加密儲存，同時 Client 來跟 Server 要機敏資料時傳輸過程也是加密的，安全性大幅提升；\n且有 Vault 管理，可以定期 Rotate，並隨時查看目前的機敏資料使用狀況\n2.動態生成 透過動態生成方式，每一個 Client 來要即使權限需求相同，也動態生成不同的 key，如果有異常操作，就能很清楚是哪一個 Client 有問題，接著直接 revoke 掉就行了；\n如果大家都共用一把 key，就查不出是誰也不能直接 revoke，造成安全上的漏洞\n3.Encrpyt as Service\nAPI Server 設計時，我們常需要加解密資料，所以會需要金鑰與實作加解密演算法，但是一方面金鑰外流到 API Server 上多一層風險，再者如果加解密演算法有漏洞又是另一個風險；\n所以 Vault Server 本身也提供加解密 API，讓金鑰與演算法的保障都留在 Vault Server 上，大幅降低風險。\n架構上，Vault 拆成幾個模組增加相容性 Authentication\n基於人員的驗證，可以透過第三方的架構如 AWS/LDAP，或是K8S Audit 將所有的操作都記錄下來，可以存放在 system logs 或是其他儲存方式 Secret\n機敏資料的型態，可能是 Key-Value，也可能是第三方服務如 Database / AWS 等 Storage\n機敏資料的儲放位置 以下教學將初步探索 Vault 功能為主，只適用於學習與測試環境使用!\n下一篇實作篇，以 AWS 為主，動態 Launch API Server 時主動索取機敏資料，被配與動態 MongoDB 金鑰與 AWS Role\nVault 初探 Vault 官方教學 是一個互動式的教學，蠻簡單明瞭的，以下是整理的筆記，僅適用於學習，完全不建議用在正式環境!\n先到 官方載點下載並安裝，設定好路徑之後可以安裝 auto complete $vault -autocomplete-install，vault 會偵測 shell 安裝對應的插件\n啟動測試環境的 Vault server $ vault server -dev 接著會在 terminal 打印出 Root Token: 的字樣，此時 vault server 會跑在前景；\n開啟另一個 terminal tab，將其輸出至環境變數等等會使用上 export VAULT_DEV_ROOT_TOKEN_ID=\u0026quot;{替換 Root Token}\u0026quot;，接著指定 vault server url $export VAULT_ADDR='http://127.0.0.1:8200'；\n透過 $ vault status 檢查是否指定正確\n取得/新增/更新/刪除 secret 先前提到 Secret 有很多種，從最簡單的 key value 開始，Vault 通過以下指令設定\nvault kv put secret/{path} {key}={vaule} {key2}={value2} vault kv get -format=json secret/{path}\nformat 是選擇性回指定回傳格式 vault kv delete secret/{path} 可以把 Vault Server 儲存密碼的方式想成 RESTful API Server，指定資源的路徑，採用最長匹配方式，secret 是預設的前綴，後面的path 用 / 分隔，接著的 kv 可以設定多組，put 同時代表新增與更新；\n啟用 Secret 並設定 path 預設用 dev 啟動後 secret/ 路徑會對應到 KV 儲存方式，可以透過 $vault secrets list 查看，接著如果我們想要自訂其他的 Secret，或是指定新的路徑儲放 KV，可以透過 $vault secrets enable -path={path} {secret engine} 例如 $vault secrets enable -path=kv kv 就能指定 kv/ 為新的 key-value 儲存路徑\n接著下 $vault kv put kv/hello foo=bar 可以得到相同的結果，透過 $vault secrets list 可以看出來多了一組設定\n如果不要用了，可以透過 $vault secrets disable {path}/ 如 $vault secrets disable kv/ 刪除，注意底下儲存的密碼也都會一並消失\n啟動 AWS Secret Engine 取得動態 AccessKey / AccessSecret 先前提到 Vault 一大特色是支援多種 Secret，而且 Secret 可以是動態生成的，每一個 Client 來索取都能要到獨立的 Secret\n透過 aws secret engine，可以指定 Role 與權限並給予動態 secret，首先讓我們啟動 secret $vault secrets enable -path=aws aws，接著設定 aws 帳號\n1 2 3 4 $ vault write aws/config/root \\ access_key=${AWS Access Key} \\ secret_key=${AWS Secret key} \\ region=${AWS Region} 絕對不建議在生產環境如此使用，因為 shell command 會被記錄在 log 中\n後續的操作，Vault 就會用這組帳密去管理 IAM\n接著創建角色\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 $ vault write aws/roles/my-role \\ credential_type=iam_user \\ policy_document=-\u0026lt;\u0026lt;EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1426528957000\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:*\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] } ] } EOF 這裡給予了 ec2 全部的權限\n接著要產生 Secret 通過 $vault read aws/creds/my-role，此時就會回傳一組新的 role，每次呼叫都會回傳獨立的 Secret (為什麼用 read 代表創建動作就不確定設計的原理了 :thinking_face)，從 IAM Console 可以看到被創建的用戶 如果要 revoke 的話，通過 $vault lease revoke aws/creds/my-role/{lease id} 即可\n剛剛想要查找有沒有指令可以條列出所有的 credentials，但看來是沒有這項指令，如果當初沒有記得 lease id 的話，就要去 IAM Console 手動刪除了\n所有的指令可以從這邊看到 AWS Secret 文件，或是用 $vault path-help aws 查看，path-help 可以指定不同的路徑層次，例如 $vault path-help aws/config/lease\nAuthentication \u0026amp; Authorization 現在我們知道要如何建立與管理金鑰，但重點回到誰能來要金鑰以及權限劃分，Authentication 部分 Vault 可以用 token / AppRule / Github / AWS IAM / LDAP 等等，Authorization 部分可以制定 Policy，限縮用戶能夠操作的 Secret 權限\nToken Token 驗證機制是 Vault 核心、不能被關閉的驗證功能，像我們一開始啟動 Vault Server 後會產生一個 root token，接著會用 root token 去制定 Policy，並產生多組 Token，文件提到如果設定完就應該 revoke root token，最好是有 root token 時大家一起盯著螢幕操作後就 revoke，後續有需要可以在動態生成\n$vault operator init $vault operator generate-root 用 root token 生成其他 root token 在 Vault 中，Token 管理是階層化的，用母 Token 去生成的 Token 會自動變成子 Token，為了管理方便，如果母 Token 被 Revoke 那麼以下所有的子 Token 會全數被 Revoke，如果希望 Token 不受母 Token 影響，可以用設定為 Orphan Token，跳脫階層獨立管理\n除了 Root Token，其餘的 Token 都有 TTL，TTL 設定有幾個地方\nVault Server 啟動時的 Config 透過 mount tuning 去更改 根據母 Token 的Policy 在有些長駐的程式中，需要長時間持有 Token 可以設定為 Periodic，指定更新週期只要在這期間內去 renew 就能繼續使用 Token，同時可以指定 TTL 或是設為永不失效，等 TTL 到了會自動 revoke\nToken 建立後會回傳 id / accessor / policy 等資訊，accessor 可以用來操作 token 相關的指令例如 renew / revoke / 反查 token id 等等，不太確定為什麼不直接用 token id 而是要多產生另一組 accessor 代號，但目前看到要條列出所有 token 只能透過列出 accessors 再去反查 token\n以上的 Token 是指 Service Token，也就是預設最常使用的 Token 形式；另外還有 Batch Token 用在 Vault 操作上，暫時沒有看到用處就先略過\n接著實際操作看看\n1 2 3 4 5 6 7 8 9 10 11 12 13 // 透過 root 生成的 token，ttl 預設為無過期 $ vault token create ----- 開啟另一個 shell，試著用剛剛的 token 登入 $ export VAULT_ADDR=\u0026#39;http://127.0.0.1:8200\u0026#39; $ vault login {token} ------ // 刪除剛才的 token $ vault token revoke {token} // 新增一個 ttl 為 1h 的 token $ vault token create -ttl=\u0026#34;1h\u0026#34; 其他的部分，覺得透過 api 下指令比較方便，東西也比較全面，可以參考文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // 查看所有的 accessors $ curl \\ --header \u0026#34;X-Vault-Token: {token}\u0026#34; \\ --request LIST \\ http://127.0.0.1:8200/v1/auth/token/accessors // 查看各別 accessor $ curl \\ --header \u0026#34;X-Vault-Token: {token}\u0026#34; \\ --request POST \\ --data \u0026#39;{\u0026#34;accessor\u0026#34;: \u0026#34;{accessor id}\u0026#34;}\u0026#39; \\ http://127.0.0.1:8200/v1/auth/token/lookup-self // 透過 accessor revoke token $ curl \\ --header \u0026#34;X-Vault-Token: {token}\u0026#34; \\ --request POST \\ --data \u0026#39;{\u0026#34;accessor\u0026#34;: \u0026#34;{accessor token}\u0026#34;}\u0026#39;\\ http://127.0.0.1:8200/v1/auth/token/revoke-accessor 設定 Policy 接著就是設定 Policy 部分，限制不同 Token 能夠操作 Secret 的權限，Vault 撰寫 policy 的格式是採用 hcl，也就是 HashiCorp 自己內部的 Config 描述語言，在路徑下建立檔案 my-policy.hcl 並貼上以下內容\n1 2 3 4 5 6 7 path \u0026#34;kv/foo\u0026#34; { capabilities = [ \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34; ] } path \u0026#34;secret/data/foo\u0026#34; { capabilities = [ \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34; ] } 上述文件的意思是這個 Policy 只能操作 secret/foo 的新\u0026amp;更新，其他的操作都一率被禁止，/data/ 指的是預設 Key Value 的 Secret 路徑，如果是另外啟動的如 $vault secrets enable -path=kv kv，則可以不用加 /data 變成 kv/foo 即可\n接著實際操作\n1 2 $ vault policy write my-policy my-policy.hcl $ vault token create -policy=my-policy 可以看到 token 的 policy 窩了自定義的 my-policy\n接著開另一個 shell 登入\n1 2 3 4 5 6 $ vault login {token} $ vault kv put secret/foo bar=baz // permission denied $ vault kv read secret/foo $ vault kv put secret/foo/bar bar=baz 後來發現 $vault login 會影響所有的 shell，這點在測試要稍加留意\nVault Policy 設定可以很彈性，主要有幾項關鍵字\n1. path 路徑選擇，可以用 * 代表以下所有路徑 / + 代表此路徑匹配任何字元等方式，如果有多組 path，Vault 採取最長匹配的原則\n2. capabilities 權限選擇，有 [read, create, update, delete, list, sudo, deny]，文件提到 Vault 沒有很仔細區分 create/update 的使用場景，所以要給就一起給；\nsudo 指的是受 root 保護的權限 / deny 是禁止任何操作包含 sudo 也不行\n3. allowed_parameters \u0026amp; denied_parameters \u0026amp; required_parameters 針對參數的設定，可以條列允許/不允許/必備的參數，如果沒有特別設定，則代表不受任何限制\n4. min_wrapping_ttl \u0026amp; max_wrapping_ttl 產生子 Token 的最長與最短 TTL\n以下是綜合範例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 path \u0026#34;secret/foo\u0026#34; { capabilities = [\u0026#34;create\u0026#34;] allowed_parameters = { \u0026#34;foo\u0026#34; = [] \u0026#34;bar\u0026#34; = [\u0026#34;zip\u0026#34;, \u0026#34;zap\u0026#34;] } min_wrapping_ttl = \u0026#34;1s\u0026#34; max_wrapping_ttl = \u0026#34;90s\u0026#34; } path \u0026#34;secret/bar/*\u0026#34; { capabilities = [\u0026#34;create\u0026#34;] required_parameters = [\u0026#34;bar\u0026#34;, \u0026#34;baz\u0026#34;] } path \u0026#34;secret/baz/+/data-*\u0026#34; { capabilities = [\u0026#34;create\u0026#34;] denied_parameters = { \u0026#34;deny-*\u0026#34; = [] } } 針對 secret/foo，在創建時只允許參數 foo / bar，其中 foo 的 value 不受限，但是 bar 只能是 zip 或是 zap\n第二條是指 secret/bar 底下的任意路徑適用，包含 secret/bar/baz 和 secret/bar/baz/foo\u0026hellip;. 等等，並且必須包含 bar, baz 這兩個參數\n第三條是路徑會匹配如 secret/baz/foo/data-bar，且不允許 deny- 開頭的 key\n多重 Policy 與衝突 在 Token 創建時可以同時綁定多組 Policy，不免讓人好奇如果兩組 Policy 對同一路徑產生衝突時的狀況，實測發現是 capacities 相同路徑下會給予最綜合的權限，而不是按照順序覆蓋之類的\n可以用 $vault token capabilities {token} {path} 查詢\n改變 Policy 當改變 Policy 時，原本已經建立好的 Token 不會動態套用，需要刪除重建，而新增/更新 Policy 的方式用 $ vault policy write {policy_name} my-policy.hcl即可\n統整 僅僅介紹基本功能就花了不少篇幅，涵蓋了基本的 Secrets / Auth 的功能介紹與操作\n啟動 dev server，關掉資料則全部消失 1 $ vault server --dev 登入與設定路徑 1 2 $ vault login {token} $ export VAULT_ADDR=\u0026#39;http://127.0.0.1:8200\u0026#39; Secret Engine 是與路徑匹配，secret/data 是預設 Key Store Secret Engine，可以另外開啟新的路徑 1 2 3 4 $ vault secrets enable -path=kv kv // 條列 $ vault secrets list -detailed $ vault secrets disable {path} 新增\u0026amp;更新/取得／刪除 Token 1 2 3 $ vault kv put kv/foo foo=\u0026#34;123\u0026#34; $ vault kv get kv/foo $ vault kv delete kv/foo Authentication 1 2 $ vault token create -ttl=\u0026#34;1h\u0026#34; -policy=\u0026#34;my_policy\u0026#34; $ vault token revoke {token id} Authorization\n另外用 hcl 檔儲存 Policy，並匯入設定中 1 2 3 4 5 $ vault policy write my-policy my-policy.hcl $ vault token capabilities {token} {path} $ vault policy read my-policy $ vault policy list $ vault policy delete my-policy 想不到初步介紹就花這麼大的篇幅，下一篇要實作結合 AWS 與搭配 API Server 的流程\n","date":"2020-04-23T20:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-04-23-vault-%E6%95%99%E5%AD%B8-%E9%9B%86%E4%B8%AD%E5%8C%96%E7%AE%A1%E7%90%86%E6%A9%9F%E6%95%8F%E8%B3%87%E6%96%99%E4%B8%8A/","title":"Vault 教學-集中化管理機敏資料(上)"},{"content":"當我們在利用 Vagrant 建立開發環境，或是雲端上準備部署時，都需要用 VM 執行指定的 Image，準備好環境後開始執行應用程式，但是管理 Image 是一件有點麻煩的事，尤其是要記錄每個步驟到底做過了什麼，又或是要整合入持續部署 CD 的環節都不這麼友善\nPacker 用 json 檔指定基礎 Image / Provision 步驟 / 指定平台打造對應的 Image，例如可以同時針對 AWS / DigitalOcean / vmware 等不同平台建立，透過 command line 就可以建立 Image\n以下教學專注在 AWS 的 Image 建立上，並分享實際使用經驗\nPacker 在建立 AWS Image 時會需要開機器，自動上傳的 AMI 並關閉機器，Packer 開立的機器型別時 t2.micro 有免費的額度，但如果超過可能會有額外的一點點費用，取決於 build image 的次數與時間；\n另外 AWS AMI 儲存於 S3，也會有額外的成本，Packer 只負責建立 Image，後續的管理要自己處理\n之前在公司建立 Image 也是要先開新機器，動手處理完壓成 Image，缺點是整個操作步驟是不透明，雖然可以翻 bash_history 但還是不這麼容易，如果文件漏寫後人維護就會很痛苦；\n用 Packer 就解決了 Image 建立過程不透明、每次都要開關機器的困擾\n安裝 Packer 下載連結\n指定基礎 Image 可以在 AWS Marketplace 上找想要採用的 Image，或直接指定 ami-id，例如我在 us-east-1 使用 Ubuntu 18.04 LTS - Bionic 的 ami-id 是 ami-0d03e44a2333dea65，要找到 ami-id 其實有點小麻煩，可以參考下列步驟\n先找到 Image，點進去後按 \u0026ldquo;Continue to Subscribe\u0026rdquo; -\u0026gt; \u0026ldquo;Continue to Configuration\u0026rdquo; (沒截圖)，接著選定區域就能看到 ami-id 了\nPacker 設定檔 Packer 的設定檔是 json 格式，內容相當簡單明瞭，主要四塊\nvariables\n文件有說為了變數管理方便，後續設定檔的參數全部都要定義在這裡 builders\n指定的平台與對應的建立方式，如 AWS 要指定 credential / region / vpc 等 provisioners\nImage 要執行的設定，可以用 shell 執行 / file 上傳檔案，或是其他的 provisioning 工具如 Ansible 等 post-processors\n產生 Image 的後處理，可以壓縮上傳到指定位置等，這邊先略過 以下檔案是建立 API server image，基於 ubuntu 安裝 Nodejs 並將 Server 檔案上傳至指定路徑執行，所以的設定檔在github 上 packer_get_started\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 { \u0026#34;variables\u0026#34;: { \u0026#34;aws_access_key\u0026#34;: \u0026#34;{{env `AWS_ACCESS_KEY_ID`}}\u0026#34;, \u0026#34;aws_secret_key\u0026#34;: \u0026#34;{{env `AWS_SECRET_ACCESS_KEY`}}\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34; }, \u0026#34;builders\u0026#34;: [{ \u0026#34;type\u0026#34;: \u0026#34;amazon-ebs\u0026#34;, \u0026#34;access_key\u0026#34;: \u0026#34;{{user `aws_access_key`}}\u0026#34;, \u0026#34;secret_key\u0026#34;: \u0026#34;{{user `aws_secret_key`}}\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;source_ami\u0026#34;: \u0026#34;ami-0d03e44a2333dea65\u0026#34;, \u0026#34;instance_type\u0026#34;: \u0026#34;t2.micro\u0026#34;, \u0026#34;ssh_username\u0026#34;: \u0026#34;ubuntu\u0026#34;, \u0026#34;ami_name\u0026#34;: \u0026#34;packer-example {{timestamp}}\u0026#34; }], \u0026#34;provisioners\u0026#34;: [{ \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;script\u0026#34;: \u0026#34;init.sh\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;inline\u0026#34;: [ \u0026#34;ls\u0026#34;, \u0026#34;pwd\u0026#34; ] }, { \u0026#34;type\u0026#34;: \u0026#34;file\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;server\u0026#34;, \u0026#34;destination\u0026#34;: \u0026#34;~/server\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;script\u0026#34;: \u0026#34;start.sh\u0026#34; }] } 在 json 檔中，變數取用都是以 {{}} 雙括號括住，只有在 variables 中可以用 env 指定使用環境變數，也可以留空在 command line 執行時指定，如 $ packer build -var 'aws_access_key=YOUR ACCESS KEY' ... 來源還可以從 Consul / Vault 等地方，可以參考文件\n後續階段要讀取就要用\n1 {{user `aws_access_key/`}} 的形式讀取 variables 中的參數\nbuilders 可以看到 builders 是以陣列形式，這邊可以指定多個建立的目標，例如增加 vmware / virtualbox 等等，Packer 會併發建立 Image\nprovisioner 常用搭配 shell / file 執行 shell script 或是上傳本地端資料到 server上，可以指定在某些目標下才複寫，如\n1 2 3 4 5 6 7 8 9 { \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;script\u0026#34;: \u0026#34;script.sh\u0026#34;, \u0026#34;override\u0026#34;: { \u0026#34;vmware-iso\u0026#34;: { \u0026#34;execute_command\u0026#34;: \u0026#34;echo \u0026#39;password\u0026#39; | sudo -S bash {{.Path}}\u0026#34; } } } Packer 大概就是這麼簡單! Do one thing and do it well. 執行的時候照樣 hashicorp 工具的操作方式，$packer validate 先檢查語法正確性，接著 $packer build 就完成囉\n最後看一下 Packer 每次 Build 所產生的 instance 使用經驗談 以下分享一些實際遇到的問題與解決辦法\n使用 AWS build Image 偶爾遇到 package 安裝失敗的問題 症狀大概是 Packer 在執行 $sudo apt-get install 時偶發說 package not found，但偶爾可以，且進去機器安裝網路狀況都是沒問題的\n問題主要出自於開啟 AWS 機器時有可能網路還沒有好，所以才會是偶發性，解決辦法就是確認網路好才開始執行 shell script，詳細請參考 Packer Github Issue Option for builder to wait on cloud-init to complete\n在 provisions 第一步加入以下 script (答案摘錄自上方 issue 回覆)\n1 2 3 4 5 6 provisions:[{ \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34; \u0026#34;inline\u0026#34;: \u0026#34;/usr/bin/cloud-init status --wait\u0026#34; }, .... ] 希望在 launch 機器時就啟動服務 這比較偏原有的系統 service 設定，使用 ubuntu 的話可以設定 systemd service，並指定 target 就可以在 launch 機器時啟動，詳細參考 How To Use Systemctl to Manage Systemd Services and Units 與\nUnderstanding Systemd Units and Unit Files 兩篇來自 DigitalOcean 的好文，預計這篇會在產出一篇文章 (挖坑)\n希望在 launch 機器後取得機器資訊與 IP 這應該是蠻實用的需求，像是綁定一些網路服務，會需要在 config 檔指定機器的 IP 等，須注意這邊要等到 launch 後才綁定，而不是在 Packer Build 的機器產生，所以會結合上一點，指定 script 在機器啟動後才執行\n取得 AWS 機器資訊 取得 IPv6 修改文件 ","date":"2020-04-15T02:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-04-15-packer%E6%95%99%E5%AD%B8-%E6%89%93%E9%80%A0-image%E8%88%87%E5%AF%A6%E9%9A%9B%E4%BD%BF%E7%94%A8%E7%B6%93%E9%A9%97/","title":"Packer教學-打造 Image與實際使用經驗"},{"content":"大綱 Infrusture as Code 挑戰賽 - Hashicorp 工具鏈全教學\nVagrant 主要是建立與管理 VM 的工具，主要希望在工作流程中提供一致的環境，例如說有新人加入開發，需要在本地端設定 Runtime 環境 / 資料庫等，這時候如果用 Vagrant 使用 $vagrant up 一鍵啟動所有需要的環境，就非常的方便，也可以避免「明明在我的電腦就沒問題」的尷尬\nVagrant 主要是 VM-based 的工具，預設使用 Virtualbox，也可以用 VMWare / AWS 等虛擬化平台\n本次教學目標為部署一個 nodejs api server + mongodb\ngithub repo 在此 vagrant-getting-started\nInstall 下載 Vagrant\n下載 Virtualbox\nTerminology 在 Vagrant 有幾個名詞先介紹\nprovider:\n提供 Vagrant 虛擬化的環境，預設是 virtualbox，也可以是其他的第三方 provider box:\n對比是 Docker 的 Image，也就是 Vagrant 虛擬化啟動的 Image，透過這個基礎再去客製化，以下用 ubuntu 示範 provision:\n客製化環境的每一個步驟，可以用 shell執行 shell script、file 上傳檔案，或是搭配 chef/ansible 等 provisioning 工具 Get Started! 首先建立一個檔案夾 vagrant-demo，先拉下等等要用的 box，\n1 2 [Host] $ vagrant box add ubuntu/trusty64 可以在這邊找到你想要的 box，Discover Vagrant Boxes\n接著建立 Vargrantfile 文件，裡頭寫入\n1 2 3 Vagrant.configure(\u0026#34;2\u0026#34;) do |config| config.vm.box = \u0026#34;ubuntu/trusty64\u0026#34; end 首先看一下語法，Vagrant 設定檔是用 Ruby 語法，當然不會 Ruby 也可以使用，Vagrant.configure(\u0026quot;2\u0026quot;) 定義 Vagrant 的語法版本，接著 do |config| ... end 定義 vm 的設定，config 是我們在這個 block 中的 vm 名稱；\nconfig.vm.box 則是指定採用的 box，config.vm 底下有很多參數可以指定，請參考文件\n接著執行\n1 2 [Host] $ vagrant up 如果安裝正確，vagrant 會直接啟動新的 vm，如果直接開 virtuabox 會看到一個新的 vm instance $vagrant up 表示依序路徑讀取 Vagrantfile(當前路徑 \u0026ndash;沒有找到再往\u0026ndash;\u0026gt; ../當前路徑 -\u0026gt; \u0026hellip;.)，並啟動新的 vm instance，啟動後會發現路徑下多了 ./.vagrant 的資料夾，這主要是記錄 vagrant 執行狀態\n接著讓我們 ssh 進入 vm 看一下\n1 2 [Host] $ vagrant ssh ssh 用的 key 等等 vagrant 都處理好了，ssh 進入後，預設操作的 user 是 vagrant，先看一下 /vagrant\n1 2 [VM] $ ls /vagrant 你會意外發現 /vagrant 裡頭竟然有 Vagrantfile，主要是 vagrant 預設會把本地端 Vagrantfile 同在的資料夾一並同步到 vm 底下的 /vagrant 當中\n依照共享的不同，可能是 rsync 一次性複製，也可以透過 SMB 雙向同步共享資料夾\nprovisioning 接著我們安裝上 nodejs，讓我們透過 shell script 安裝 nvm，並建立 server 資料夾放到 vm 中，接著透過 pm2 啟動\n將 Vagrantfile 改成\n1 2 3 4 5 6 7 8 9 10 11 12 Vagrant.configure(\u0026#34;2\u0026#34;) do |config| config.vm.box = \u0026#34;ubuntu/trusty64\u0026#34; config.vm.provision :shell, path: \u0026#34;init.sh\u0026#34;, privileged: false config.vm.provision \u0026#34;file\u0026#34;, source: \u0026#34;server\u0026#34;, destination: \u0026#34;/home/vagrant/\u0026#34; config.vm.provision :shell, path: \u0026#34;start.sh\u0026#34;, privileged: false config.vm.synced_folder \u0026#34;.\u0026#34;, \u0026#34;/vagrant\u0026#34;, id: \u0026#34;sync\u0026#34;, type: \u0026#34;rsync\u0026#34;, rsync__exclude: [\u0026#34;.git/\u0026#34;, \u0026#34;server/\u0026#34;, \u0026#34;start.sh\u0026#34;] end 每一個 config.vm.provision 代表一個步驟，我們指定了\n執行 init.sh：主要是安裝 nvm 與 pm2 file 是用來複製檔案，將 server 路徑複製到 /home/vagrant/ 底下 start.sh 主要是啟動 nodejs server config.vm.synced_folder 則是顯示指定我們要同步到 /vagrant 底下的資料，也可以指定到其他資料夾下，不過要小心處理檔案路徑\n接著執行\n1 2 [Host] $ vagrant provision 注意如果這時候跑 $ vagrant up 是沒有用的，因為 vm 已經啟動了，如果是 Vagrantfile 增加 provision，記得用 $ vagrant provision或 $ vagrant reload --provision，如果是 Vagrantfile 其他設定檔有更動，請用 $ vagrant reload\n此時 ssh 登入後可以看到 nodejs server 已經在執行了\n這時候好好說一下 shell 需要注意的地方，\n1 2 3 4 .... source ~/.nvm/nvm.sh nvm install 12 npm install -g pm2 可以看到我在 init.sh / start.sh 要呼叫 nvm/pm2 之前都要 $source ~/.nvm/nvm.sh 而非 $source ~/.bash.sh，主要是因為預設 vagrant 的 .bashrc 有這一段\n1 2 3 4 5 # If not running interactively, don\u0026#39;t do anything case $- in *i*) ;; *) return;; esac 而我們又將 nvm 的啟動 script 放在這一段之後才會導致 shell 找不到 nvm command，所以要改成$source ~/.nvm/nvm.sh，這部分是參考 stackoverflow Why is nvm command installed as root and also not found during vagrant bootstrap.sh?\n其他的 provisioning 工具搭配請參考文件\ndestroy 如果開發過程中有誤，建議 Vagrantfile 改完砍到 vm 重來，使用 $ vagrant destroy，主要是每次執行 $ vagrant provision 會不斷在原本的 vm instance 操作，這樣會導致每次疊加結果而違反 immutable\nnetwork 在 Vagrant 中，網路大致有三種設定方式\nPort Forward\n讓 Host 環境可以用指定 port 對應 VM 中的 port Private Network\n指定 private ip 讓內網的機器都能透過 ip 溝通 Public Network\nVagrant 預設整合 Ngrok，可以用公開連結連線 此時 nodejs server 只能在 vm 內部使用，我們用 Port Forward 方式讓 host 也可以呼叫 修改 Vagrantfile\n1 2 3 4 Vagrant.configure(\u0026#34;2\u0026#34;) do |config| ... config.vm.network \u0026#34;forwarded_port\u0026#34;, guest: 80, host: 8080 end 接著用 $ vagrant reload 這時候在 Host 就可以發請求到 VM 上囉 $ curl localhost:8080\n小結 在 Provisioning 部分，個人覺得用 shell script 有點不太方便，例如說切換 OS 時 script 就需要修改，而且很指令式而非宣告式，如果能用其他的 provisioning tool 去管理，又或是使用 image / container 去隔離對底層 OS 的相依，這勢必會方便很多\nVagrant 有提供 AWS provider，但因為是社群開發套件就暫且不試，專注在搭建本地端的開發環境\n目前是只有一台 api server，接著要配置 DB 並放在同一個 Vagrantfile 中整理\nMulti machile 如果要在 Vagrantfile 中定義多個 vm instance，可以透過 config.vm.define 區隔，預設會繼承全域的所有 provision，但是個別定義中可以複寫或是客製化，如以下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Vagrant.configure(\u0026#34;2\u0026#34;) do |config| config.vm.box = \u0026#34;ubuntu/trusty64\u0026#34; config.vm.define \u0026#34;api\u0026#34; do |api| api.vm.provision :shell, path: \u0026#34;init.sh\u0026#34;, privileged: false api.vm.provision \u0026#34;file\u0026#34;, source: \u0026#34;server\u0026#34;, destination: \u0026#34;/home/vagrant/\u0026#34; api.vm.provision :shell, path: \u0026#34;start.sh\u0026#34;, privileged: false api.vm.synced_folder \u0026#34;.\u0026#34;, \u0026#34;/vagrant\u0026#34;, id: \u0026#34;sync\u0026#34;, type: \u0026#34;rsync\u0026#34;, rsync__exclude: [\u0026#34;.git/\u0026#34;, \u0026#34;server/\u0026#34;, \u0026#34;start.sh\u0026#34;] api.vm.network \u0026#34;forwarded_port\u0026#34;, guest: 3000, host: 8080 end config.vm.define \u0026#34;db\u0026#34; do |db| db.vm.provision :shell, path: \u0026#34;./mongodb/install.sh\u0026#34;, privileged: false end end 如果想要指定其中一台下指令，針對名稱就好如 $vagrant ssh db\n結語 個人覺得開發不推薦用 Vagrant (那怎麼花了一個假日\u0026hellip;)，主要是社群看起來沒有很活躍，例如最多人下載的 box ubuntu/trusty64 還是在 14.04 的版本，想要直接找 Mongodb 的 box 也沒有(大多的 db 都沒有)，且整個配置上沒有很方便，例如 shell script 切換 os 就要重寫，還不如用 docker + docker compose 來得快速\n但如果你是一定要用 vm 那或許 Vagrant 還是個不錯的選擇\n如果你有其他建議，又或是有覺得 Vagrant 有厲害獨特的應用場景再麻煩指教～\n","date":"2020-04-12T11:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-04-12-vagrant-%E6%95%99%E5%AD%B8-%E5%BE%9E%E6%9C%AC%E5%9C%B0%E7%AB%AF%E9%96%8B%E7%99%BC%E5%88%B0-aws-%E9%83%A8%E7%BD%B2/","title":"Vagrant 教學- 從本地端開發到 AWS 部署"},{"content":"最近開始接觸 DevOps 與 Infrasture as Code 的概念，深深被用程式碼管理部署流程與架構設計感到著迷，對工程師來說最棒的文件莫過於結構清晰、沒有重複冗余的乾淨程式碼，尤其是把過往透過人工操作的不穩定性一切攤開在陽光下用 Code Review 方式檢視每一個環節，讓程式碼從出生到部署都可以完整地被檢視；\n再者在 Cloud-Native 時代，能夠跨區域部署、甚至混合雲部署能夠讓公司的性能與成本上獲得更大的彈性\n只是在學習過程中，被一堆技術工具轟炸，每個工具之間又有些重複又不怎麼相同的功能，又或是抽象畫層級不同 (Container vs VM)，像是 Chef / Ansible / Terraform / Kubernetes / Docker 等等，每次東學一些西學一些總是一個頭兩個大，最近看了來自 Hashicorp 的介紹影片以及一篇文章整理，覺得在觀念上更加的融會貫通，至少更清楚知道 部署的流程 與對應的工具應用\nWhy we use Terraform and not Chef, Puppet, Ansible, SaltStack, or CloudFormation\n簡單總結一下上面兩個參考資料的想法\nApplication Delivery with HashiCorp 當在開發應用程式時，要正確地交付到用戶手中有一段路要走，主要分成幾個階段\n1. Write 本地開發 在本地端開發時，開發者會需要有跟正式環境類似的配置，例如資料庫等，這時候可以用 Vagrant 建制開發環境\n2. Test 測試 進行測試時也需要一個乾淨、獨立、與正式環境接近的配置，同樣能用 Vagrant\n3. Package 打包應用程式 當程式碼測試完，要準備部署時，還需要把設定檔、環境變數等一併打包變成一個可部署的最小單元，此時能用 Packer 依據不同環境(AWS/GCP\u0026hellip;)打包出 Image\n4. Provision (Day1/Day2+) 機器的環境設定 應用程式需要完整的架構設計，例如說 CDN / DNS / Networking / Firewall / Storage System 等等，這時候可以用 Terraform 配置 dev/stage/prod 環境\n5. Deploy 部署方式 (Orchestration) 部署方式常見有 Canary / Blue-Green 等，又或是遇到流量起伏時的 Auto-Scaling，可以用 Nomand，將 Dev 跟 Ops 獨立拆分，Dev 只需要專注在需要的運算資源、部署流程的掌握，Ops 則負責底層的機器配置、數量管控、機器的安全性補丁等\n6. Monitor 監控 Hashicorp 目前沒有直接相關的產品，但有提供 Consul，如果是採用 Microservice / SOA 架構，內部服務間如何發現彼此需要內部的 DNS處理 / 機器要怎麼管理 config 都是個問題，Consul 透過 Key/Store 儲存解決這類的問題\n7. Security! 另一個不再流程中但開發者需時時牢記在心 Security，一般來說 key 會在打包階段被一並放進，但是這樣相對不太安全，Vault 提供key 自動 rotate / 中心化管理 credential / 中心化處理加解密過程，降低機器被攻陷後的影響與更快速彈性的替換 key，增加安全性保證\n以上工具都能夠整合 VM / Container based 的環境，也有許多不同的替代方案；\n例如 Docker 可以單獨吃掉 Write/Test/Package 的功能，K8S 則負責 Deploy 與 Monitoring，如果是託管於 Cloud 則由 Cloud 負責 部分Provision的功能(如 Load Balancer，但不會自動設定 Public DNS 或 S3 這類架構)\n其中 Nomand 跟 K8S 比較是互補的工具，Hashicorp 提到 K8s 上手學習曲線太高，很多時候我們不一定要這麼複雜但強大的工具，如果原本是 VM based 架構那用 Nomand 管理部署會輕鬆、直觀很多\nWhy we use Terraform and not Chef, Puppet, Ansible, SaltStack, or CloudFormation 這篇的作者非常厲害，之前拜讀了全部的 Terraform 教學個人覺得比官網更實在，這一篇文章含金量一樣超高，從更高層級的角度看這類型的工具，Chef, Puppet, Ansible, SaltStack 主要是做 Configuration management，也就是一台乾淨的機器啟動後要做什麼配置，例如 API Server 需要安裝 program runtime 並部署程式碼 / DB server 要安裝 DB 並搭配安全性配置等等\n而 Terraform / CloudFormation 是 Provision，是配置整個架構，所以兩者的討論面向不同，雖然像 Ansible 也能做一些 Provision 工作，但相較就不適合\n接著是 Mutable / Immutable 的問題，Chef/Ansible 等工具在產生配置變化時會在同一台機器發生，也就是 Mutable，當累積的變化變多時可能會有配置衝突的問題；\n如果是用 Terrform 搭配 Packer/Docker 等，每次都會產生新的機器，所以是 Immutable，行為相對比較單純且可預測\n接著作者還從語法上、管理上、社群大小做了比對。\n技術層面上可以做不同混搭，例如 Terraform + Ansible，配置好架構後用 Ansible 管理每一台機器的設定； 又或是 Terraform + Packer + Docker + Kubernetes，一樣架構配置好且機器的 Image 預設都裝有 Docker 與 K8s agent，後續用 K8s 管理部署的流程\n實作 看完對於整個部署流程與工具更加的理解了，此時當然要挑戰自己動手搭建整個環境，以 API Server + DB 的架構去挑戰 Hashicorp 全套工具鍊，希望用一個月左右時間完成，以下是完整的教學記錄，希望能夠幫助到大家\nVagrant 教學-一鍵啟動配置開發環境\nPacker 教學- 打造 Image Vault 教學-集中化管理機敏資料 (上) \u0026hellip;待續\n","date":"2020-04-12T02:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-04-12-infrusture-as-code-%E6%8C%91%E6%88%B0%E8%B3%BD-hashicorp-%E5%B7%A5%E5%85%B7%E9%8F%88%E5%85%A8%E6%95%99%E5%AD%B8/","title":"Infrusture as Code 挑戰賽 - Hashicorp 工具鏈全教學"},{"content":"公司今年度開始全面導入數據驅動設計，每個部門的所有專案都必須有明確的數據評估成效，身為一個工程師，看到公司往更有邏輯的方式組織專案，用更多的理性數據當作依據來安排做事的優先順序感到開心，這讓我們擺脫窮忙的局面，專注在執得投資的項目上；\n但另一方面也感到憂慮，是不是任何現階段無法使用數據衡量的事物就完全不重要？例如用戶體驗、工程品質等等，是不是我們走向數據的同時就成了完全百分百的賺錢機器？\n基於這樣的考量，同事推薦了這本書《Designing with Data 善用數據幫你打造好設計》\n\u0008這本書以設計師為出發，闡述數據是設計師的好朋友，而不是扼殺創意的牢籠，同樣的這句話我覺得也能套用到工程師身上(工程師也是有對品質要求的浪漫)\n如果你跟我有一樣的擔憂，又或是想知道良好的數據驅動設計是怎麼正向的推動，這本書無疑是個很棒的入門書 (我們公司的資深 UX 設計師 / PM 都推薦閱讀)\n數據思維 數據的本身只是個單純的計數，他可以是網頁上按鈕點擊的次數、用戶停留的時間長，又或是 app store 給予的評價星等，數據是個理性且沒有意義的存在，只有當人們解讀時才賦予了含義\n當我們開始解讀數據，可以分成三個層次的思維\n數據驅動\n數據的表現很明確，可以直接回答團隊的問題，並驅動結論的產生 例如說希望透過改善付費頁面增加轉換率，此時透過實驗觀察到轉換率的高或低，就能有明確的結論知道改善是否有效\n數據啟示\n當面對一個模糊不清的領域，團隊需要花一些時間研究與探索才能明定目標，此時數據的用途不是給予一翻兩瞪眼的結論，而是從中的差異去解讀可能性 另如說之前公司為了瞭解用戶對於登入方式的想法，增設多個第三方登入的方式，我們不期待這次實驗後就拍板要不要導入第三方登入，而是單純探索用戶的反應，並分析增加登入通過率的可行方案 往往是先有了數據啟示，經過幾次的探索有了更明確的假設與目標，接著就拆分成多個數據驅動的實驗\n數據意識\n當我們習慣數據思考後，開始打造完整的數據收集系統，讓整個產品的流程都能用數據去解釋與呈現 例如說公司之前想要衡量新用戶的體驗，重新定義數據收集的內容與格式，用數據紀錄抵達第一個「a ha moment」的用戶體驗里程，這成了後續評估專案成效的重要指標\n為什麼需要數據 當我們在打造一項產品，我們都希望有很多的用戶能夠使用而受惠，進而付費讓整個商業模式可以運轉，持續打造能夠解決用戶痛點的好產品\n但是我們要如何知道「用戶是不是真的滿意我們的產品？」「推出的新功能是不是用戶真的要的？」「新的 UI 介面成效有比舊的 UI 更受歡迎嗎？」「我們該怎麼做才能增加營收？」等問題\n我們該如何知道用戶的使用情況與反饋？\n對於網路產業來說，推出產品與收到反饋的時間都非常快，在更近一步討論數據之前，必須先回歸原點，公司再不同的階段會有不同的戰略目標，可能是營收的成長率或是用戶的成長數等\n[案例一] Netflix 的商業模式是訂閱制，在訂閱方案的金額固定下，用戶的訂閱數就是他們很重要的關鍵指標，關鍵指標通常是直接跟營收做掛鉤；接著透過數據觀察到用戶的訂閱意願與觀看時間成正比，所以觀看時間就成了次要指標 [案例二] Coursera 的商業模式是透過販售課程完成的結業證書，他們的次要指標是用戶是否通過第一次考試、用戶在一週內是否有反覆回來造訪網頁 在資源有限的情況下，我們必須持續且快速的嘗試，關鍵指標反應可能會比較慢，所以會需要有其他的次要指標去觀察嘗試的結果，用每一次的學習去往次要指標與關鍵指標邁進，因此快速且正確的評估用戶的反應，就是件非常重要的事情，常見的手段分成量化研究與質化研究\n量化研究 主要是透過客觀的觀察大量用戶的行為，透過統計分析探討「用戶是如何操作產品 / 用戶怎麼操作這項功能」\n質化研究 透過用戶訪談或使用性測試，與少量的用戶進行深度的訪談，藉此了解用戶的真實情緒反應，更注重於「為什麼用戶會怎樣思考」等問題\n量化研究與質化研究相輔相成，例如說公司推出了新的付費方案，透過量化研究發現整體用戶的續訂率下降了，這時候就透過訪談方式去了解到用戶看到多個方案反而不清楚其中的差異，導致考慮的時間拉長等當初設計方案時欠缺考慮的方向\n這本書反覆強調，是數據優先而非單純數據，意即量化研究或許是主要手段，但也不能完全捨棄質化研究\nA/B 測試 當我們執行專案後，要怎麼清楚的知道這樣的結果是基於我們做出的改變，而不是外在因素干擾或是瞎貓碰到死耗子，簡言之要釐清高相關性與因果性，在此推薦一篇 whoscall 團隊非常棒的文章 數據分析的力量\n為了找出因果性，可以套用科學實驗的方法，\n拆分同質的對照組與實驗組，控制一次改變一個自變數的情況下，觀察應變數的變化關係\n也稱作為 A/B 測試，拆分成 A,B 兩組，一組維持原本的設計，另一組依照我們的假設套用新的設計，觀察兩組的指標變化，推論假設是否成立\n基於我們的商業目標與現有的數據，我們提出多組假設，並依據假設設計多組實驗組，實際的情況大概會長這樣\n以下拆分成三個階段 定義 -\u0026gt; 執行 -\u0026gt; 評估\n定義 有幾個問題可以幫助思考目標\n你想把時間跟精力放在哪邊產生影響力 你相信什麼對使用者是好的 怎樣的使用者體驗或是商業關鍵議題可以被視為機會點 什麼是改進使用者體驗的好機會 定義的目標可以是量化指標，也可以是質化指標，例如書中以辦營隊為例，「增加營隊報名人數」可以是個目標，「讓營隊變得更好玩」也可以是個目標；\n但不論目標是量化還是質化，都必須找到量測的指標，確認自己有在往目標邁進，質化目標也可以通過問卷回收、田野調查等方式\n指標的評估 先前提到關鍵指標與次要指標，設定次驗指標的目的在於關鍵指標的反應週期可能很長，另如月訂閱制要等到一個月後才知道結果太慢了；\n又或是關鍵指標太不敏感，像是app store 評價，如果今天做的是局部+評估類型的改動，像是調整按鈕大小與顏色，估計是不會很快反映在評價上，所以需要額外的指標衡量\n指標之間的衝突 目標的制定是科學也是藝術，例如 eBay 的交易媒合的方式是賣家上架商品 + 買家下標兩者動作結合，他們透過數據發現在某些情況下買家棄標的比例很高，經過研究發現是目前的上架流程某些資料不透明，例如買家下標才發現商品在海外需要額外的運費跟稅收，所以他們希望研究增加商品資訊量是否能夠減少棄標量，他們發現增加商品資訊量確實減少棄標量，但是也因為增加購買的阻力導致下標數量下滑\n這時候減少棄標量增加交易品質跟下標數量下滑這兩個指標孰輕孰重，就考驗團隊的價值觀，此時 eBay 選擇前者，因為與其增加短期的下標量，他們更重視用戶的中、長期關係\n同樣的 Airbnb 也面臨過相同的抉擇，他們發現某些屋主的物件條件明明不錯卻出租日很少，後來發現是照片拍得不夠好、資訊不夠充分，當時內部一個團隊目標是「增加屋主的上傳物件數」另一個團隊的目標是「增加屋主出租的機會」，兩者同時希望更改註冊流程，一個希望簡化另一個希望屋主更慎重註冊，實驗的方向剛好衝突，最終他們選擇融合兩者目標去平衡，達到最佳的成效\n這是一個不斷反覆思索的過程，從目標到指標的設定需要來回的檢視，並與各個團隊溝通以免實驗互相衝突\n假說 有了目標後要開始發想假說，我們必須決定專案的規模以及預計學習的事物，可以用兩個維度四個象限拆分：局部/全面、探索/評估\n改動範圍如果是局部，那代表我們可以用較激進的方式去量測，因為潛在的風險比較小/\n專案的目的如果是評估，那代表結果必須很明確的回答某些問題，在設計數據收集方面就要往這方面邁進\n例如說「改動付費頁面希望提升轉換率」就是個局部+評估類型的需求，只改動一個頁面不會影響到其他關鍵體驗，同時專案的結果會直接決定要不要套用新的設計；\n而「修改視覺系統希望更清楚表達產品價值與品牌意識」就是個全面+探索的需求，改動上線後無法直接回答轉換率是否直接因此提升，需要從其他面向與增加量測方式才能得知專案的成效\n制定假說 接著用以下的句子問自己\n對於 [使用者類型]，如果 [改變]，就會 [效應]，這是由於 [理由]，並會影響 [量測值]\n使用者類型 可以是不同的子集，例如說新用戶跟舊用戶就是很大的差異，又或是不同國家、不同性別等，這依據產品調性與實驗的性質不同而區隔 以下問題可以幫助思考\n你對於他們的人口結構有什麼了解？他們有什麼習慣？ 他們和公司有什麼關係？ 這是現有的使用者？新用戶？專業用戶？ 改變 是指希望用來影響使用者的事物，一個假說可以多種不同的改變設計\n你是要增加新的設計？ 還是要移除舊的設計 效應 是預期帶來的改變\n你發現的問題是什麼？什麼樣的用戶可以解決或是減少這個問題？ 你發現的機會領域是什麼？ 理由 則是支持假說的證據\n你的理由是來自消費者動機？還是由某種戰術與機制來達成改變 有哪筆資料支持這項假說？ 量測 是想要影響的終極指標，用一個客觀的方式最大化學習\n為了瞭解你正在創造正面且夠大的影響，需要衡量什麼指標？ 你會量測用戶的情緒嗎？ 以照片分享平台為例，假說可以是「我們預測藉由增加濾鏡與照片特效，會有更多人使用我們的產品，因為這會讓他們的照片更好看、更有趣，若我們發現使用者投入層度增加就知道假設是真的」\n樣本與信心程度 在定義階段，我們需要明確測試的族群是哪些，並且決定測試的時間週期與樣本數，例如說假日上線的用戶跟平日上線的用戶可能不同，如果測試的週期不夠全面，容易得到錯誤的結論\n為了避免假陽性(實驗判斷有效但實際無效)、假陰性(實驗判斷無效但實際有效)等實驗不準確的狀況，我們需要對測試結果有一定程度的信心，例如說有人通知巷口失火了，如果只是一個人你可能覺得在開玩笑，但有一百人都這樣說勢必就會開始起疑心，在統計上稱為信賴區間，從母體採集某個數量的子樣本，套用數據後得到一定的信心水準\n面對不同等級的修改會需要不同的信心水準，同時也會決定結果放量的過程，例如說跟營收相關的改動可能要比較高的信心水準，以及較謹慎緩慢的放量過程\n快速驗證 再進行產品做實體測試前，可以推出低擬真度的 mockup 進行使用性研究，例如 spotify 希望統一視覺系統，但不確定要採用深色還是淺色的方案，此時他們將設計原型做成問卷調查先快速驗證，最終採用了深色的方案，用此減少不必要的嘗試\n在定義的階段，試著回答以下問題\n你想要為公司達到的目標有哪些？ 在你的試驗中，最重要學習到的會是什麼？ 在生成假說時，運用了哪些數據？ 在挑選假說時，是否真的生成了所有的假說？ 執行 這一個階段是如何透過設計呈現假說，例如 Netflix 想要測試 「在首頁增加電影的選擇數目是否會增加銷量」，但是增加選擇數目有很多種方式，設計團隊提出[增加電影類別數]或[類別中電影的數量] (廣度與深度)，分別涉及成三個實驗\n25 x 100 50 x 75 50 x 100 最終發現中間的實驗效果最好 在 A/B Testing 五大必殺招數，讓你轉換率立馬提升 200% - Day 12 / 200, #EverythingAboutGrowth 文章中分享一段很精闢的見解\n好的實驗假設，都是奠基於使用者行為與心理脈絡發展而成\n1 2 3 4 5 6 7 【功能性假設】 - 按鈕從藍色變紅色，會提升轉換率 - 把圖片由小放大，會提升轉換率 【行為心理脈絡假設】 - 旅宿頁面加強急迫感，會提醒使用者有訂不到房的可能性與壓力，進而提升轉換率 - 搜尋列表頁讓更多商品能一次映入眼簾，能幫助使用者容易比較多間民宿並找到喜歡民宿，進而提升轉換率 切記每個專案的重點要放在學習與檢討，而不是做完沒有成效就算了，擺正心態才能不斷的調整與改善\n好的實驗並需明定目標，並平衡學習的細度以及測試的項目，在過程中可能會同時有多個假說與實驗再進行，透過不斷地檢視與學習，可以捨棄或是增加測試項目\n假門測試 (Fake door) 可以再投入完全工程開發資源前，先做假的介面可以互動但背後的工程邏輯先忽略，測試用戶是否真的有需求，假門測試相比問卷可以真實反應用戶的需求，但要小心有可能惹惱用戶，所以在投放的過程應該要保守\n例如我們公司為了測試用戶是否需要額外的登入功能，就先放假的登入按鈕收集用戶需求，點擊後彈窗顯示功能還在開發中，執行假們測試僅套用到極少數的用戶上，取得足夠樣本後就終止了\n試驗零 再開始聚焦細節之前，可以先退一步思考「如果功能移除會有怎樣的影響」來避免過度聚焦的副作用，Skyscanner 發現移除頁面上的「最便宜機票」按鈕不影響關鍵數據，他們就省下精力在優化這個項目上\n這個階段可以試著問以下問題\n你如何打造最符合假說的體驗與設計？ 在這個階段什麼元素是關鍵設計？什麼是其他可以之後注意的？ 跟其他測試項目相比，這個項目能從中學到的獨特之處在哪？ 如果需要學習到事務，最多需要幾個測試項目？ 你能否說明每個測試項目的差異？ 分析 發行 AB 測試之前 當要發行測試時，可以透過一些使用者研究去精練測試項目，確保實際發出的 AB 測試項目是最有價值的，向 spotify 每兩到三週會找真實的用戶到辦公室接受聲音測試，各團隊依據需求提出申請，在大規模 AB 測試之前調整文案與設計\n接著要確保 最小可檢測效應，意即足以宣告成功的最小改變量，要記得做任何的實驗都有代價的，不論是投入的資源，還是用戶的學習成本都是，要事先明定指標的變動量大於某個數值才能帶來真正的商業價值，反之則不值得發行\n最後要執行前，確認樣本的檢樣方式是否正確 / 確認發布後要持續多久 / 考量落實的細節等，像是 Facebook 在推出新功能前都會在紐西蘭先測試，因為 Facebook 測試會需要有實際的社交關係，同時又要與其他國家有點隔閡才不會用戶體驗互相衝突等\n這時候可以問自己幾個問題\n我在嘗試學習的東西是什麼？是否我還相信我的設計可以傳達想要學習的東西？ 如果我的試驗成功或失敗，我要做什麼？ 我的測試樣本是否足夠大？ 是否了解測試中的所有指標？ 是否有良好的次要指標？ 評估結果 如果結果是正面的，代表假說是有價值的，但此時要仔細評估背後的學習，而不是貿然的推出新功能\n例如說 Esty 網站推出新的後台系統流程，透過測試結果良好，但貿然推出新流程增加舊用戶的學習曲線，後來他們在不會推出與逐步釋出之間做抉擇\n如果結果是負面的，要反過來思考\n用戶是否以你所想像的方式使用？ 用戶是否關心你所沒有考慮的事物？ 這功能是否只是群體的細分族群使用而非大眾需求？ 此外有些決策是為了更大的商業考量，即使目前的測試導致些許的負面結果，但權衡之後還是值得推出，像是先前 ebay 的案例\n如果對照組與實驗組的結果差不多，這是常見的事情，不必太氣餒，可以反過頭來檢視打造測試的過程是否有所遺漏\n樣本的選擇是否正確 是否需要更多的用戶才能量測 是否有外部因子干擾 挖掘其他次要指標與關鍵指標 接著可以決定是否進行下一輪測試，又或是將成果逐步放量到全部用戶上 結語 最後兩章講得是如何在公司內導入，以及招聘合適的人選，這部分就暫略，整份讀書心得其實有點生硬，畢竟是不常接觸的領域，從門外漢與公司運作角度一瞥數據驅動與AB測試的美妙，整套設計非常的理性科學，卻也在某些設計環節依舊保留創造與彈性，結合兩者才能持續打造用戶真正需要的產品\n","date":"2020-04-05T11:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-04-05-%E9%96%B1%E8%AE%80%E5%BF%83%E5%BE%97designing-with-data-%E5%96%84%E7%94%A8%E6%95%B8%E6%93%9A%E5%B9%AB%E4%BD%A0%E6%89%93%E9%80%A0%E5%A5%BD%E8%A8%AD%E8%A8%88/","title":"閱讀心得《Designing with Data 善用數據幫你打造好設計》"},{"content":"會看到 DNS over HTTPs(DoH) 是因為閱讀到 Firefox 在 2/26 於美國用戶推出預設採用 DoH 的文章，早在 Firefox@62 時就已經內置這項設定，其餘地區用戶可以透過設定開啟；\n目前 Chrome 於 78 之後預設開啟，Windows 10 也宣布即將整合 DoH，公開 DNS 解析服務商也越來越多支援 DoH，早先 Firefox 與 Cloudflare 合作，後續 Google Public DNS / AdGuard 等也加入支援\nDNS 是歷史悠久的網路通訊協定，基於 UDP/TCP 查詢域名對應的紀錄，但這一切在網路傳輸都是明文，沒有任何的隱密性與安全性，後來有了 DNSSEC 可以確保 DNS 紀錄沒有被竄改，但可惜在推廣上需要改 DNS Server，受到實作限制並沒有全面實施\nMozilla 與 Google 於前陣子提出 DNS over HTTPs，將 DNS 查詢放在 HTTPs 當中，如此一來除了可以防止被惡意竄改等中間人攻擊外，更可以保護用戶隱私，ISP 等中間的網路設施提供商就比較難追蹤用戶的網路瀏覽\n這一篇文章會先大略技術介紹，接著整理網路上對於 DNS over HTTPs 的正反兩方討論，先說結論是\n沒有所謂的 100% 安全，但我們能盡力做到最好\n技術實作 DoH 其實就是透過 HTTPs 查詢 DNS 紀錄，一般 DNS 在查詢記錄時會需要反覆多次，從頂級域名開始一路查到子域名，在這過程中全部的服務商與網路中介設備都知道你在查詢哪個網站\nDoH 希望改透過 Trusted Recursive Resolver(TRR) 去查詢，交由 TRR 做到解析域名這件事，這樣就無法回溯到使用者本身的 IP，達到保密性的效果；\n同時由 DNS 域名商提供的 TRR 服務時，會參考用戶的 IP 位置，避免一些透過 Geo DNS 的服務造成影響\nSpec 概覽：RFC-8484 RFC8484，簡單抓出幾個有趣的地方\nMIME Type DoH Client 可以用 application/dns-message，表明是要用 dns 格式查詢，也可以用 json 等方式，看 DoH Server 提供，如 Cloudflare 有提供 json 格式\nHTTP Method 支援 GET / POST 兩種方式查詢\n例如說要查詢 www.example.com 長這樣\n1 2 3 4 5 :method = GET :scheme = https :authority = dnsserver.example.net :path = /dns-query?dns=AAABAAABAAAAAAAAA3d3dwdleGFtcGxlA2NvbQAAAQAB accept = application/dns-message 建議 HTTP2 為最低版本 對比 DNS 採用的 UDP/TCP 來說，HTTP 開銷大很多，採用 HTTP2 有一些性能上優化的部分，例如重用連結、重整封包順序、Header 壓縮等，相比更舊的 HTTP 版本已儘可能減輕通訊上的負擔\n實際使用與測試 要實際使用的方式有幾種，一種是在自己電腦裝上 DoH Client，接著將 DNS 查詢指向 DoH Client 經由他代理查詢，好處是透過代理不用擔心應用程式的支援問題，這部分可以安裝 cloudflare 提供的 cloudflared\n但這次實驗只是要快速測試功能，與利用 Wireshark 實地看封包重送的過程，所以安裝另一個 command line 工具 kdig，kdig 可以直接指定 DoH Client 查詢 dns 紀錄，比 dig 功能再多一些，安裝完成後比較兩個指令\n$ kdig -d @1.1.1.1 +tls-ca +tls-host=cloudflare-dns.com yuanchieh.page vs kdig -d @1.1.1.1 yuanchieh.page\n透過 Wireshark 擷取封包結果 前者走一般 HTTPS 流程，後者走 DNS 流程，可以看到傳輸量與傳輸速度的差異\n爭議之處 這篇新聞整理了許多專家對於 DoH 的反對意見：DNS-over-HTTPS causes more problems than it solves, experts say，其中幾點蠻值得更深入探討\nDoH 不能防止 ISP 業者窺視用戶的瀏覽紀錄(可以解決) DoH 確實能夠防止 ISP 業者看到 DNS 查詢記錄，但如果用戶是用 HTTP，那走不走 DoH 他的網頁瀏覽紀錄還是會被發現；\n即使用戶走 HTTPS，基於現有的 HTTPS 缺失，不是每個 request 都是加密的，例如 SNI 跟 OSCP\nSNI 是 Client 先跟 Server 説他要連線的 hostname 是哪個，因為同個 IP 上可能有多個 Server (Virtual host) 需要回傳對應的憑證；\nOSCP 是 Client 收到 Server 回傳的憑證，並需再去 CA 驗證憑證的有效性；\n這兩個 Request 都是明文，所以都會被記錄到。\n再者，ISP 業者還是會知道 Client 要送封包到哪個 IP，IP 位置不是很時常更動，透過反查就知道 Client 是在看什麼網站\n上述條件無關乎 DoH 的問題，但確實讓 DoH 毫無用武之地\n公司管理困擾 如果公司防火牆透過在 DNS 階段過濾，碰上 DoH 就會很頭疼，造成管理上的漏洞\n過度集中於 DNS 解析商 再發起 DoH 時，會需要指定 HTTPs 聯繫的對象，這通常是由 DNS 解析商所提供，這導致權利集中於幾間大公司手裡，例如 Firefox 瀏覽器內預設使用 Cloudflare / NextDNS，雖然也可以加入自定義，但如果一般的用戶不慎熟悉，這會導致 DNS 解析商反而有很大的影響\n專家建議是基於 DNS 本身機制外加加密方式，而不是把作法綁到幾間解析商手上。\n內文有提到「某些公司為了讓自己看起來很在意用戶隱私，才推出這種半殘的協定，是非常不負責任的」\n結論\u0026hellip; not the end! DoH 看似解決了部分問題，又有 Firefox / Chrome / Windows / Google DNS / Cloudflare 等多家世界級產品與 DNS 服務商的推廣與背書，但確實反對者提出的質疑也很有說服力，只能說如果在意 DNS 有沒有被竄改，使用 DoH 或許可以，但不能期望 DoH 完全抹去個人的網頁瀏覽紀錄\nTLS 1.3 TLS 1.3 改變了加密流程，由最一開始的 Client 發出 ClientHello 就加密了，此時加密的方式是從 DNS 紀錄拿到 Server 的 Public Key，並採用 Diffie-Hellman 交換金鑰方式，用「非對稱加密的公鑰」加密「對稱加密的金鑰」，用「對稱加密的金鑰」加密指定的 Server name\n目前僅剩唯一個問題 IP 位置，但如果 Server 是在 CDN 後面，讓 ISP 只能追蹤到 CDN，後面的就無法追蹤到\n此外 TLS 1.3 將握手機制減少一個 RTT 變成只要一個 RTT 就能完成交握，效率大幅提升\n結論 TLS 1.3 需要 DNS 支援，就可以從交握開始就全程加密；\n而為了避免 DNS 被攻擊或是竄改紀錄，記得要使用 DNSSEC；\n最後加上 DoH，讓 DNS 紀錄查詢這一段也不會被 ISP 追蹤\n整段補上後，就能增加用戶的隱私與安全性，可惜 AWS 的 Route53 跟 Cloudfront 不支援\u0026hellip; :/\n看了一些文章，目前覺得 Cloudflare 在安全性這部分著墨很多，支援度也都很夠，或許該考慮看看\n參考資料 A cartoon intro to DNS over HTTPS: 非常仔細講解過程 DNS over TLS / DNS over HTTPS - Is it the privacy magic bullet?\n","date":"2020-02-29T07:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-02-29-dns-over-https-%E5%88%86%E4%BA%AB/","title":"DNS over HTTPs 分享"},{"content":"在使用 express.js 當作 Nodejs server 框架時，時常會需要寫一些 Middleware 處理 Token 驗證、用戶權限檢查等等，也會套用很多第三方的模組去建構程式\n但突然某天在思考如何自己寫一個紀錄response time的 Middleware，發現自己沒辦法用一個 Middleware 註冊就完成這件事，因為 express.js 不像是 Koa 的 middleware 是用 promise based 實作，所以當某個環節是非同步，執行的順序就會錯亂\n後來查看了 morgan 被大量使用的 express log middleware，才發現其中設計的小巧思，以下是整理的內容\nExpressjs Middleware 設計 先來看最基本的 Middleware 設計\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 const express = require(\u0026#34;express\u0026#34;); const app = express(); app.use(function(req, res, next){ console.log(\u0026#34;middleware 1 start\u0026#34;); next(); console.log(\u0026#34;middleware 1 end\u0026#34;); }); app.use(function(req, res, next){ console.log(\u0026#34;middleware 2 start\u0026#34;); next(); console.log(\u0026#34;middleware 2 end\u0026#34;); }); app.get(\u0026#34;/\u0026#34;, async function(req, res){ console.log(\u0026#34;request started\u0026#34;) await delay(); console.log(\u0026#34;request finished\u0026#34;) res.send(); }); async function delay(){ return new Promise((res)=\u0026gt;{ setTimeout(()=\u0026gt;{ res() }, 1000) }) } app.listen(3000); 目前的 log 會變成\n1 2 3 4 5 6 middleware 1 start middleware 2 start request started middleware 2 end middleware 1 end request finished 如果我們希望在 Request 進來先紀錄開始時間，接著在 Response 結束時紀錄結束時間，就必須仰賴其他的實作方式\non-headers / on-finished 爬過 morgan 程式碼後，發現是透過這兩個模組去實作功能的\non-headers：註冊事件，當 header 被寫入時會觸發\non-finished：註冊事件，當 request/response 結束時觸發\n這兩個模組可以針對 Nodejs 原生的 http server 搭配使用，express.js 也是繼承原生的 http server\n在 morgan module 中，在 middleware 進入一開始標記 request 開始時間，在 on-headers 時紀錄 request 結束時間，在 on-finished 將訊息印出，pseudo code 大致如下\n1 2 3 4 5 6 7 8 9 10 11 12 app.use(function (req, res, next) { res._startTime = new Date().getTime(); onHeader(res, function () { res._endTime = new Date().getTime(); }) onFinished(res, function () { console.log(`req process time: ${res._endTime - res._startTime} ms`) }) next() }) log 結果是\n1 2 3 request started request finished req process time: 1010 ms on-headers 實作 將原本的 response 中的 writeHead 複寫，只是多包一層觸發事件的機制\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 function createWriteHead (prevWriteHead, listener) { var fired = false // return function with core name and argument list return function writeHead (statusCode) { // set headers from arguments var args = setWriteHeadHeaders.apply(this, arguments) // fire listener if (!fired) { fired = true listener.call(this) .... } return prevWriteHead.apply(this, args) } } function onHeaders (res, listener) { if (!res) { throw new TypeError(\u0026#39;argument res is required\u0026#39;) } if (typeof listener !== \u0026#39;function\u0026#39;) { throw new TypeError(\u0026#39;argument listener must be a function\u0026#39;) } res.writeHead = createWriteHead(res.writeHead, listener) } on-finished 實作 這邊的實作就比較有趣，如何正確的判讀 http request/response 結束了呢？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 function isFinished (msg) { var socket = msg.socket if (typeof msg.finished === \u0026#39;boolean\u0026#39;) { // OutgoingMessage return Boolean(msg.finished || (socket \u0026amp;\u0026amp; !socket.writable)) } if (typeof msg.complete === \u0026#39;boolean\u0026#39;) { // IncomingMessage return Boolean(msg.upgrade || !socket || !socket.readable || (msg.complete \u0026amp;\u0026amp; !msg.readable)) } // don\u0026#39;t know return undefined } 如果是套用在 response，需注意根據官方文件 response.finished 是指說 res.end() 被呼叫後設定為 true，不代表 response 中的資料完全傳輸到網路上\n這些討論可以看 PR response is only finished if socket is detached #31，提交者修改成\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 if (stream \u0026amp;\u0026amp; typeof stream.closed === \u0026#39;boolean\u0026#39;) { // Http2ServerRequest // Http2ServerResponse return stream.closed } if (typeof msg.finished === \u0026#39;boolean\u0026#39;) { // OutgoingMessage return ( msg.finished \u0026amp;\u0026amp; msg.outputSize === 0 \u0026amp;\u0026amp; (!socket || socket.writableLength === 0) ) || (socket \u0026amp;\u0026amp; !socket.writable) } 增加 http2 的檢查，以及確保 outputSize === 0 所有 queued 住的資料都確實送出 socket\n知道如何判斷 response 是否結束，最後看事件的註冊\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 function attachFinishedListener (msg, callback) { var eeMsg var eeSocket var finished = false function onFinish (error) { eeMsg.cancel() eeSocket.cancel() finished = true callback(error) } // finished on first message event eeMsg = eeSocket = first([[msg, \u0026#39;end\u0026#39;, \u0026#39;finish\u0026#39;]], onFinish) function onSocket (socket) { // remove listener msg.removeListener(\u0026#39;socket\u0026#39;, onSocket) .... eeSocket = first([[socket, \u0026#39;error\u0026#39;, \u0026#39;close\u0026#39;]], onFinish) } if (msg.socket) { // socket already assigned onSocket(msg.socket) return } // wait for socket to be assigned msg.on(\u0026#39;socket\u0026#39;, onSocket) .... } 在 response 與 response.socket 分別註冊事件，當結束或錯誤事件觸發後，檢查 response 是否真的結束，最後觸發用戶註冊的事件\n","date":"2020-02-06T22:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-02-06-expressjs-middleware-%E5%A6%82%E4%BD%95%E5%9C%A8-response-%E7%B5%90%E6%9D%9F%E8%A7%B8%E7%99%BC/","title":"Expressjs Middleware 如何在 Response 結束觸發"},{"content":"本篇分享 MongoDB 在批次查訊大量數據時的小技巧\n在查詢小筆數據時，往往我們就是用 find().toArray() 直接回傳結果，需要簡單的分頁就搭配 limit() / skip() 即可完成\n但如果需要分析數據跑過整個 collection，不可能用 find() 一次拉回所有的資料，透過 skip() 批次處理，如果 collection 資料不多還好，如果幾十萬筆、幾百萬筆資料效能回非常的低落，因為 skip 的話每次查詢 DB 都必須要重頭開始跳過指定筆數資料，才能回傳，所以查詢時間會隨著筆數增加而趨近於指數倍增長\n目前已知有兩種做法，可以有效輪詢整個 collection 而不拖垮 DB 效能\n一種是 Cursor，但是在實務上我個人並沒有採用，原因是 Cursor 每次只能透過 .next() 取得一筆資料，如果希望一次拉個上千筆資料處理就無法做到；\n另一種是本次要分享的方式，透過遞迴查詢，保持性能情況下跑完整個 Collection，實務上每天應用在有千萬筆資料的 Collection 而沒有太大的問題\n遞回查詢 說穿了其實很簡單，透過有建立 index 的索引，按照遞增或遞減排序，每批次取出部分數據後，下次查詢的條件改為取出數據的最後一位，一路到 Collection 結束\n以下是程式碼部分\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 const MongoClient = require(\u0026#39;mongodb\u0026#39;).MongoClient; const dbUrl = \u0026#34;mongodb://127.0.0.1:27017\u0026#34;; MongoClient.connect(dbUrl, async function (err, client) { console.log(err) const testDB = client.db(\u0026#34;test\u0026#34;); const userCollection = testDB.collection(\u0026#34;users\u0026#34;); const result = await iterateCollection({ sourceCollection: userCollection, query: { age: 20 }, batchSize: 10, order: \u0026#34;asc\u0026#34; }); console.log(result); client.close(); }); async function iterateCollection({ sourceCollection, query, batchSize, order }) { let result = []; let sort = { _id: -1 } let _query = { ...query }; if (order === \u0026#34;asc\u0026#34;) { sort = { _id: 1 } } while (true) { const queryResults = await sourceCollection.find(_query).limit(batchSize).sort(sort).toArray(); if(queryResults.length === 0){ break; } _query._id = { $lt: queryResults[queryResults.length - 1]._id } if (order === \u0026#34;asc\u0026#34;) { _query._id = { $gt: queryResults[queryResults.length - 1]._id } } // process data and push to result // result.push(queryResults.map(result =\u0026gt; result._id)); } return result; } ","date":"2020-02-06T22:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-02-06-mongodb-%E6%89%B9%E6%AC%A1%E8%99%95%E7%90%86%E5%A4%A7%E9%87%8F%E6%95%B8%E6%93%9A/","title":"MongoDB 批次處理大量數據"},{"content":"先前公司遇到 Client 在不明狀況下連續呼叫註冊用戶的 API 兩次，導致用戶被重複建立，導致後續的 API 操作與資料分析異常，所以決定加上 Lock 機制避免重複建立的問題\n剛好在 Redis 官網看到 Redlock，一種 Redis 作者 antirez 基於 Redis 設計的分散式 lock 機制，並且已經有了 Nodejs 版本的實作，所以就決定採用這套方法，也確實解決了問題\n本次部落格會摘錄官方說明 Distributed locks with Redis，並整理 Martin Kleppmann 提出質疑 How to do distributed locking與作者再次回復 Is Redlock safe?\n題外話介紹 Martin Kleppmann，他就是《Designing Data-Intensive Applications》一書的作者，目前仍是我最推薦的工程書籍，可以參考之前的筆記\n技術筆記 Designing Data-Intensive Applications 上\n技術筆記 Designing Data-Intensive Applications 下\nRedlock 簡介 當我們在設計分散式 Lock 機制時，有三點原則必須考量到\nSafety 當 Lock 被取走後，在釋放之前不能有另一個 Client 取得 Lock，也就是 mutual exclusive DeadLock Free\nLock 必須在一段時間後(TTL) 自動釋放，避免握住 Lock 的 Client 跨掉而 Lock 從此不能被釋放 Fault Tolerance\n整體系統不能有單一節點失敗的可能，必須考量系統容錯性 後續解說演算法實作時，會不斷去檢視這三點原則是否被滿足\n需注意到容錯機制可能會聯想到 Master / Slave 架構，但是在 Redis 中 Slave 資料備份是非同步的，所以當 Master 掛掉到 Slave 接手，中間的時間差 Client 有機會取得多個相同 Lock，這會違反第一點原則，Cluster 架構同理\n所以這裡作者提議的容錯機制主要基於 Multi-Master 機制，後續會有更深入的解釋\n單機原則 再考量分散式設計之前，讓我們先思考單一台 Redis 如何實作 Lock 機制\n取得 Lock 當要取得 Lock，可以用以下的指令\n1 SET resource_name my_random_value NX PX 30000 NX 表示只有當 resource_name 不存在才創建這個 Key，避免重複建立，符合第一點原則 Px 30000 表示 Key 的 TTL 是 30秒，符合第二點原則\n釋放 Lock 1 2 3 4 5 if redis.call(\u0026#34;get\u0026#34;,KEYS[1]) == ARGV[1] then return redis.call(\u0026#34;del\u0026#34;,KEYS[1]) else return 0 end 在釋放 Key 時，必須先檢查 Key 對應的 Value是不是我們一開始塞進去的值，也就是上個步驟的 my_random_value，這是要確保移除的 Lock 是當初我們取得的 Lock，試想一下情況\n1 2 3 4 1. Client A 取得 Lock 2. Client A 時間超過 TTL，Redis 移除 Lock 3. Client B 取得相同 Lock，因為 Client A 超時所以 Client B 可以取得 Lock 4. Client A 此時要釋放 Lock 步驟四如果沒有檢查，Client A 不小心移除了 Client B 的 Lock，此時就會破壞第一點原則\nRandom String Random String 的產生機制，可以取 /dev/urandom 的 20 bytes，/dev/urandom 是*unix 系統產生偽亂數的方式，依據設定的不同可能是從環境噪音、網路數據封包等外在物理現象，這樣的做法可以保證較好的隨機性，也可以用其他簡單的方式，例如取系統時間加上 client_id 等等，取決於實作者的設計\n套用至分散式系統 取得 Lock 假設目前有 N 台 Redis master，這 N 台都跑在獨立的環境上而非使用 Cluster 架構，假設 N=5，以下是實作步驟\n取得當前時間 T1 用相同的 Key / Value 依序取得 N 台的 Lock，在取得 Lock 時要設定連線Timeout，此 Timeout(ex. 5~50ms) 應該遠小於 Lock 的 TTL (ex. 10s)，避免 Client 浪費太多時間在等死掉的 Redis Server，Client 因儘速取得 Lock 當取得 Lock 後，假設此時時間為 T2，Client 檢查 T2-T1 是否大於 Lock 有效時間 TTL，只有當時間有效且大多數的 Redis Servre(過半數，也就是 \u0026gt;=3) 才算是有效取得 Lock 此時 Lock 僅剩有效時間是 T2 - T1 如果 Client 取得 Lock 失敗 (例如有效時間是負數、無法取得過半 Redis Master Lock)，Client 必須對每一台 Redis Master 發送釋放 Lock 指令，即使該台 Redis Master 沒有給他 Lock 分散式代表各個程序沒有同步的時間上，而且每台機器因為計時器的物理性質，會有時間偏差的問題(Clock Drift 問題) 時間計算會影響第三步驟的有效時間，所以需要減去一點偏差當作補償，但現實世界的時間計算頂多就幾個毫秒的誤差\n所以實際 Lock 的有效時間會是 TTL - (T2-T1) - CLOCK_DRIFT\nRetry 如果 Client 取得 Lock 失敗，應該在一定秒數後隨機 delay 一段時間，再次重新嘗試，隨機 delay 是為了錯開同時多個 Client，讓較快者可以先取得 Lock，如果 Client 沒有取得 Lock，應該儘速釋放 Lock\n釋放 Lock 同時向所有的 Redis Master 釋放 Lock\n檢驗演算法 因為再依序取得 Lock 會有時間差，假設 Client 從第一個 Redis Master 取得 Lock 時間為 T1，最後一個 Master 回傳時間為 T2，那麼第一個 Lock 僅剩的生命時間是 TTL - (T2 - T1) - Clock Drift，這也就是最小有效時間 MIN_VALIDITY；Clock Drift 是上述的時間誤差；(T2-T1) 則是取 Lock 的等待時間\n假設 Client 取 Lock 時間 (T2-T1) \u0026gt; TTL，也就是 Client 取到最後一個 Lock 時第一個 Lock 已經失效了，那此時就會全部釋放，不會有錯誤產生\n假設 Client 成功取得 Lock，那在先前的條件保證，沒有其他用戶可以在 MIN_VALIDITY 內取得大多數 Master 的 Lock，也就不會破壞 Lock 的原則性\n要確保 MIN_VALIDITY 的時間內關鍵資源能夠運作完成，不然 TTL 過後 Lock 被其他人取走，Lock 就失去互斥原則\n性能、故障復原 Redis 常用於高性能需求的場景，我們會希望 Lock 取得/釋放可以越快，增加 throughput 與降低延遲，在過程最好的方式是 Client 同時向多台 Master 取 Lock\n另外考量到故障復原的部分，假設今天取得 Lock 後 Master 故障了，假使沒有開 AOF 儲存機制，那可能 Lock 沒有保存到磁碟上，復原時遺失就有機會違反第一點原則\n假使有開 AOF，也記得要調整 fsync 頻率，最保險是設成 always，但這會影響性能\n但除了即時資料備份到磁碟上外，還可以考慮另一種做法，當 Master 故障復原後，延遲重啟的時間大於 TTL，也就是說讓原先 Master 上的 Lock 都釋放或自動失效，之後再重新加入就能避免違反安全原則，不過要小心如果超過多數的 Server 故障，需要等相對長的時間才能重新運作，此階段 Lock 都無法取得\n最後如果有餘裕可以設計延長 Lock，讓握有 Lock 的 Client 可以延長手中的 Lock\n以上是摘錄自官網文件的整理\nNodejs Package - Redlock 實作 看完演算法，來看一下 Nodejs 版本的實作 mike-marcacci/node-redlock\n他在 Redlock.prototype._lock 部分實現 Lock 機制\n截幾個關鍵片段，輪詢所有的 server\n1 2 3 return self.servers.forEach(function(server){ return request(server, loop); }); request 主要封裝 lock 的 script，支援如果同時多個 resource 要鎖可以一起鎖\n1 2 3 4 5 6 7 8 9 10 11 12 request = function(server, loop){ return server.eval( [ self.lockScript, resource.length, ...resource, value, ttl ], loop ); }; 這一段考量到 drift 問題，檢查 Lock 的有效時間，並且只有在取得多數 Redis Server 同意才往下繼續\n1 2 3 4 5 6 7 8 // Add 2 milliseconds to the drift to account for Redis expires precision, which is 1 ms, // plus the configured allowable drift factor const drift = Math.round(self.driftFactor * ttl) + 2; const lock = new Lock(self, resource, value, start + ttl - drift, attempts); // SUCCESS: there is concensus and the lock is not expired if(votes \u0026gt;= quorum \u0026amp;\u0026amp; lock.expiration \u0026gt; Date.now()) return resolve(lock); 主要關鍵是這幾個部分，其餘的就是封裝\nMartin Kleppmann 的質疑 中斷導致 Lock 有效判斷錯誤 在分散式系統設計中，時間是一個非常難以掌握的因素，程序可能因為各種狀況而導致時間序錯亂，例如說系統時間不準、網路延遲、程式運作遇到垃圾回收、作業系統切換 Process 導致中斷等，所以各種檢查機制都有可能因而出現錯誤，例如以下的例子 Client1 取得 Lock 後 結果遇到 GC 暫停運作 Lock 超過 TTL 自動釋放，此時 Client2 成功取得 Lock，並更新 DB 紀錄 接著 Client 從 GC 中恢復，他以為自己手上有 Lock 可以去更新 DB 這樣就違反了 Lock 的安全性原則\n直覺解法是在更新 DB 之前 Client 再去檢查 Lock 的時效性，但 GC 可能卡在檢查完之後的那一個點，所以設定再多檢查都是沒用的\n實際解法也蠻直覺的，系統全域有個不斷遞增的發號機制，每取一次 Lock 就配一個數字，在 DB 更新的時候，檢查對應的數字是不是有大於上次更新的數字，就可以避免掉上述提到的問題，就是 Fencing 機制\n太快樂觀預估時間的複雜性 TTL 的「時間」計算有實作問題，Redis 目前使用 gettimeofday 而非 monotonic clock 的時間\n前者是系統時間，這是可以被調動，例如 NTP Server 同步 / Admin 手動調整等，所以時間可能大幅度前後跳動\n而後者是每當 timmer 發出 interrupt 就 持續遞增永不回頭，所以在判斷兩個點的絕對時間差用後者會比較精準\n試想如果 Client1 取得 Lock 後 Redis 時間跳轉立刻 Lock 失效，結果 Client2 又可以拿到 Lock，就會違反安全性原則\n此外 Martin Kleppmann 認為 Redlock 預設太多時間因素是可預期的，像是網路延遲、時鐘偏移等問題，但這不是分散式演算法正確的設計方式，所以 Redlock 的安全性是建立在半同步系統當中(意即各種時間因素是有上限且可以被假設的)，而不是真正的分散式設計\nantirez 再次回復 以上兩點主要是被質疑的部分，接著看 antirez 回覆\n當自動釋放機制導致 Lock 的 Mutual Exlusive 機制失效時 在 Martin 第一點質疑中，他覺得要加上遞增的 Token 當作保護機制，避免 Client 手上 Token 過期還去更新 Resource，達到強保證性\n讓我們先回過頭來想為什麼我們會需要分散式 Lock，正是因為我們的關鍵資源本身沒辦法一次只服務一個請求 (linearizable)，如我自身案例是 API Server 同時有多台\n而 Martin 提出的系統全域有個遞增 Token，關鍵資源在操作時會先去檢查 Token，這本身就是個 linearizable store，讓所以的操作不再併發而變成線性逐一處理，這本身就與分散式的現況矛盾\n再者如果有個遞增 Token 系統，那 Redlock 在產生 Lock 的 Value 時，用這個遞增 Token 取代原本的隨機字串也有一樣的效果\n又或是根本不需要遞增函數，只要把隨機字串也記錄到關鍵資源上，在操作時去比對先後兩者是否相同，同樣有 Fencing 的效果\n基於時間的過多理想化假設 作者承認 Redis 應該改用 monotonic time，但是看來還沒有個結論是否要修正 Redis Repo - (#416) Use monotonic clock when available，主要問題在於並非所有系統都支援 monotonic time API\n關於時間跳動的問題，作者並沒有給出很明確的答案，但看來只能盡量避免(例如 Admin 不要改動系統時間)而不是從演算法的部分改進，因為目前 Redis 還不是用 monotonic time API\n接著考量一個情況\n取得當前時間 取得 Lock 計算當前時間 檢查 Lock 是否還在有效期間 如果有 Lock，則繼續處理 在步驟一到三，不論是網路延遲、程序中斷等時間問題，都沒有關係，因為步驟四會再次檢查 Lock\n但如果是在第四步到第五步之間，這時候沒有任何有自動釋放機制的分散式 Lock 可以保證 Mutual Exclusive，只能靠關鍵資源本身的機制，這就回到上一步說的 Fencing 機制，例如說 DB 就寫入遞增 Token 或隨機字串，讓後者不能更新\n所以 Redlock 安全嗎？\n答案取決於對於安全的要求有多高，\n配置多台 Redis Server 並開啟 fsync = always 的 Redlock 機制 在系統時間沒有大幅度跳動的情況下 Lock TTL 保證大於關鍵資源的運行時間或是在關鍵資源處有 fencing 機制 符合這三點 Redlock 就是安全的\n正因為我們沒有其他方法避免 Race Condition，才會採用 Redlock 或是其他樂觀鎖的處理機制\n","date":"2020-01-14T05:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-01-14_redis-lock-redlock-%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90%E8%88%87%E5%AF%A6%E4%BD%9C/","title":"Redis Lock (Redlock) 分散式 lock 原理分析與實作"},{"content":"近日因為公司專案，要把之前寫好處理圖片的 C++ code 搬移至網頁上，趁機會探索 Web Assembly，未來可以持續移植現有的 C/C++ Library，增加程式的復用性與前端的開發自由度\nWebassembly 其實也不是什麼新技術了，在 2017 年已經正式推出，並在四大瀏覽器都能夠使用，Nodejs 也支援，但網路上相對的中文較少，例如記憶體操作、pass by reference 等等較少提及，這也是讓我頭疼許久的地方，花了兩天不斷試錯，趁跨年假期整理並分享\n以下兩天是主要參考的文章\nCreating a WebAssembly module instance with JavaScript\nEmscripting a C library to Wasm Passing and returning WebAssembly array parameters Webassembly(Wasm) 主要目的是將其他語言透過編譯方式輸出瀏覽器可以運作的 bytecode，目前除了 C/C++ 外，Rust 也是個熱門的 Wasm 開發語言，周圍的生態系與工具鏈都相對完善；\n以下的教學主要專注於使用 Emscripten，Emscripten 功用是將 C/C++ 編譯成 Wasm，除此之外提供 JS 嫁接到 Wasm 這端的處理(膠水程式)，例如說 malloc / free / printf / cout 等等 C/C++ 的標準函式庫支援的函式，目前 Wasm 不能直接 Access，只能透過 JS 去操作 WebAPI，這些都必須在編譯時被納入實作，此外 Wasm 目前還不能像一般的 JS Library 直接 include 就能使用，而是要處理 Memory Mapping 等，這些 Emscripten 都會處理好\n主要教學項目有\n使用 Emscripten 產生範例 code 移植乘法運算 C++ Code 記憶體操作，關於 Pointer \u0026amp; Array Wasm 總結 使用 Emscripten 產生範例 code 安裝 Emscripten Emscripten 官方安裝步驟，按照步驟安裝最新版的 Emscripten，確認安裝完成\nemcc \u0026ndash;version\n官方基礎教學 Hello World 以下參考 官方基礎教學 Hello World，並翻譯(解釋)每個步驟\n產生 hello_world.c 1 2 3 4 5 6 7 8 9 10 11 12 13 /* * Copyright 2011 The Emscripten Authors. All rights reserved. * Emscripten is available under two separate licenses, the MIT license and the * University of Illinois/NCSA Open Source License. Both these licenses can be * found in the LICENSE file. */ #include \u0026lt;stdio.h\u0026gt; int main() { printf(\u0026#34;hello, world!\\n\u0026#34;); return 0; } 執行\n$ emcc build/hello_world.c -o hello_world.html\n此時會輸出三個檔案，hello_world.html、hello_world.out.js、hello_world.wasm\n-o 指定輸出的檔案與檔名，如果沒有指定會輸出 a.out.js、a.wasm；\n.html 檔是 Emscripten 方便開發者除錯用的網頁； .wasm 檔即是 binary 格式的 assembly code，人類無法閱讀； .js 檔是後續與 JS 整合會需要用到的檔案，也可以直接用 NodeJS 執行 $ node hell_world.out.js\n-o 如果有指定 {file_name}.html，則會生成配合的前端頁面，顯示 main function 的執行結果\n有了 html 檔，可以使用 http server 用瀏覽器開啟網頁，例如 npm 套件 http-server，在本地端開啟頁面查看結果\nC++ 的 code 雷同\n1 2 3 4 5 6 #include \u0026lt;iostream\u0026gt; int main() { std::cout \u0026lt;\u0026lt; \u0026#34;hello, world!\u0026#34; \u0026lt;\u0026lt; std::endl; return 0; } 乘法運算並移植到網頁上 上述的 hello_world 主要是檢測環境與工具鍊是否正常，接著開始暖身，用 C++ 寫一個簡單的整數乘法運算，輸入兩個整數，回傳兩整數相乘的結果，著重於如何將 Wasm 整合進前端中\n產生 multiply.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #include \u0026lt;iostream\u0026gt; #include \u0026lt;emscripten/emscripten.h\u0026gt; extern \u0026#34;C\u0026#34; { EMSCRIPTEN_KEEPALIVE int multiply(int num1, int num2) { return num1 * num2; } int main() { int result = multiply(2, 5); std::cout \u0026lt;\u0026lt; result \u0026lt;\u0026lt; std::endl; return 0; } } 預設 Emscripten 產生的 .js 只會執行 main function，如果想要呼叫其他韓式必須在欲輸出 function 前加上 EMSCRIPTEN_KEEPALIVE，在 Comile 時指定參數 -s NO_EXIT_RUNTIME=1 避免 wasm 執行 main function 後直接退出\n另外如果是使用 C++ 而不是 C，建議在要輸出的 function 前加上 extern \u0026quot;C\u0026quot;，主要是指定這一段程式碼用 C 的方式編譯，這樣輸出的 function 名稱會保持原狀，可以試著拿掉看看\n$ emcc build/multiply.cpp -s NO_EXIT_RUNTIME=1 -o multiply.js\n此時會輸出 multiply.js \u0026amp; multiply.wasm\n如果不確定 compiled 出來的檔案能不能運行，建議先 -o {filename}.html 確認可以運作，接著再考慮移植\n在網頁使用 multiply.out.js 獨立產生 index.html\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34; /\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;ie=edge\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Document\u0026lt;/title\u0026gt; \u0026lt;script src=\u0026#34;./multiply.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; Module.onRuntimeInitialized = function() { console.log(Module); console.log(Module._main()); console.log(Module._multiply(2, 7)); }; \u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; 請打開 Console \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 當我們打開 Console，可以看到 10，以及 Module 這個由 multiply.js export 的 Object，當我們想要使用 Module 當中的參數，需要包在 onRuntimeInitialized listener 當中，等 Module 初始化完成才能調用，Module 裡頭包含非常多的參數與 function，後續會再介紹\n而我們輸出的 function 會主動被加上 _ 前綴，如果要傳入 int 直接用 JS 的 number 就可以了\n甚至如果用 Module._multiply(2, \u0026quot;10\u0026quot;) 都會成功輸出 20，傳入參數時會自動做型別轉換，如果輸入純字串則會回傳 0\n記憶體操作，關於 Pointer \u0026amp; Array 在 C/C++ 中，pointer 很常被直接當作參數傳遞，讓 sub function 直接操作 pointer 指向的記憶體位置，function return 後原 function 可以直接取值出來用\n目標是實作一個 filter function biggerThan，只有大於 target 的 element 會被塞進 array_pointer 指向的記憶體位置，size 指向最後的 array length\n1 2 // 目標 biggerThan([elements], elements.length, target, \u0026amp;array_pointer, \u0026amp;size) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #include \u0026lt;iostream\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;emscripten/emscripten.h\u0026gt; extern \u0026#34;C\u0026#34; { EMSCRIPTEN_KEEPALIVE void biggerThan(int *elementList, int elementListLength, int target, int **result, int *size) { *result = (int *)malloc(sizeof(target) * elementListLength); std::cout \u0026lt;\u0026lt; *result \u0026lt;\u0026lt; std::endl; for (int i = 0; i \u0026lt; elementListLength; i++) { if (elementList[i] \u0026gt; target) { (*result)[*size] = elementList[i]; *size = *size + 1; } } std::cout \u0026lt;\u0026lt; \u0026#34;size mem position:\u0026#34; \u0026lt;\u0026lt; *size \u0026lt;\u0026lt; \u0026#34;\\nresult mem position:\u0026#34; \u0026lt;\u0026lt; result[0] \u0026lt;\u0026lt; std::endl; } } $ emcc build/bigger_than.cpp -O1 -s NO_EXIT_RUNTIME=1 -o bigger_than.js\n-O1 是指名要 compiler optimize 輸出結果，-O1 是初步優化，-O2 / -O3 是更進階耗時的優化，但要小心優化可能會移除需要的功能\n接著是 index.html\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34; /\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;ie=edge\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Document\u0026lt;/title\u0026gt; \u0026lt;script src=\u0026#34;./bigger_than.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; Module.onRuntimeInitialized = function() { const elementList = new Int32Array([1, 2, 3, 4, 5, 6]); const elementListBuffer = Module._malloc( elementList.length * elementList.BYTES_PER_ELEMENT ); Module.HEAP32.set(elementList, elementListBuffer \u0026gt;\u0026gt; 2); const target = 2; const result = new Int32Array(1); const resultBuffer = Module._malloc( result.length * result.BYTES_PER_ELEMENT ); Module.HEAP32.set(result, resultBuffer / result.BYTES_PER_ELEMENT); const size = new Int32Array(1); const sizeBuffer = Module._malloc(size.length * size.BYTES_PER_ELEMENT); Module.HEAP32.set(size, sizeBuffer / size.BYTES_PER_ELEMENT); console.log( `mem position:\\nsizeBuffer: ${sizeBuffer} resultBuffer: ${resultBuffer}` ); Module._biggerThan( elementListBuffer, elementList.length, target, resultBuffer, sizeBuffer ); const sizeInMem = Module.HEAP32[sizeBuffer / Int32Array.BYTES_PER_ELEMENT]; const resultRef = Module.HEAP32[resultBuffer / Int32Array.BYTES_PER_ELEMENT]; const resultInMem = new Int32Array(sizeInMem); console.log( resultBuffer, resultRef, Module.HEAP32[resultRef / Int32Array.BYTES_PER_ELEMENT + 1] ); for (let i = 0; i \u0026lt; sizeInMem; i++) { resultInMem[i] = Module.HEAP32[resultRef / Int32Array.BYTES_PER_ELEMENT + i]; } console.log(sizeInMem, resultInMem); Module._free(resultBuffer); Module._free(sizeBuffer); Module._free(elementListBuffer); }; \u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; 請打開 Console \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ArrayBuffer \u0026amp; TypedArray 在開始之前，必須先了解 JS 如何處理 binary data，在 JS 中 binary data 是以 ArrayBuffer 表示，ArrayBuffer 只能讀不能做其他的操作，只能透過 TypedArray 與 Dataview轉換，而 Wasm 中會用到的是 TypedArray\nTypedArray 有許多不同長度的類型，如 Int8Array / Int16Array，數字代表每個 element 的 bit 長度\n1 2 3 4 5 6 const buffer = new ArrayBuffer(2); const i8 = new Int8Array(buffer); i8[0] = 100; i8[1] = 20; const i16 = new Int16Array(buffer); console.log(i16[0]); // 5220，因為是 0x14 0x64 如果兩個 Type Array 從同一個 ArrayBuffer 生成，則兩者改動都會互相影響，如果遇到不同類型互轉，則高位在後低位在前，所以 i16[0] = i8[1] * 2^8 + i8[0] 在 C/C++中，有多種不同長度的型別，例如 char / int / float / double 加上 signed / unsigned 等，就會一一對照到 JS 的 Typed Array Pass Array by pointer 當我們要讓 C++ 讀取陣列，我們不能直接傳遞陣列，而是先在 JS 中把陣列放進 Memory \u0026ndash;\u0026gt; 接著傳遞 Memory 中的位址 \u0026ndash;\u0026gt; 從 Memory 位址讀取陣列的元素\n在 Wasm 中，一開始初始化會需要 Memory Object，表明整個 Wasm 能夠使用多大的記憶體，接著把資料放進記憶體當中，並取得存放的位址，將位址從 JS 傳遞給 C++，C++ 去相對應的記憶體空間將值取出\nEmscripten 簡化這個過程，改用 _malloc 去取得記憶體空間，並由對應類別大小的 HEAP 塞入空間，此時會拿到記憶體位址\n1 2 3 4 5 const elementList = new Int32Array([1, 2, 3, 4, 5, 6]); const elementListBuffer = Module._malloc( elementList.length * elementList.BYTES_PER_ELEMENT ); Module.HEAP32.set(elementList, elementListBuffer \u0026gt;\u0026gt; 2); 這段話的翻譯是\n產生 [1, 2, 3, 4, 5, 6] 陣列，每個元素是 32 bits (4 bytes) 大，剛好對應 C++ 的 int 大小 索取記憶體空間，_malloc 需指定要多大的 bytes 空間，此例需要 6 * 4 = 24 bytes，elementListBuffer 此時代表這塊記憶體的起始位置，每間隔 4 個 bytes 就是陣列的下一個元素 因為每個元素是 32 bits，所以用 HEAP32 塞資料，這邊 elementListBuffer \u0026raquo; 2 是因為每個儲存單位是 4 bytes， \u0026raquo; 2 代表 / 4\n可以想像是大小抽屜，JS 中操作最小單位是單一個 byte，如果是 Int8Array 則是一個抽屜對應一個單位，但如果是 Int32Array，就是一個抽屜對應四個單位，所以編號(位址)也會比小抽屜少四分之一 在 C++ 當中，要輪詢 elementList 陣列的值，就只要\n1 2 3 4 for (int i = 0; i \u0026lt; elementListLength; i++) { elementList[i].... } Pointer 在 Wasm 中，pointer 是 32 bits，所以針對 result / size 都是用 Int32Array，即使 size 是整數而非\u0026quot;陣列\u0026quot;，但是一樣用 Int32Array 宣告\n先看 size，宣告方式相同，最後要取值時，同樣是去 Memory 中的位置找，記得一樣要做位址座標的切換\n1 2 3 4 5 6 const size = new Int32Array(1); const sizeBuffer = Module._malloc(size.length * size.BYTES_PER_ELEMENT); Module.HEAP32.set(size, sizeBuffer / size.BYTES_PER_ELEMENT); // 取值 const sizeInMem = Module.HEAP32[sizeBuffer / Int32Array.BYTES_PER_ELEMENT]; Pointer of pointer 再來是比較特別的 result，這其實是一個 pointer of pointer，先看 C++ 實作\n1 *result = (int *)malloc(sizeof(target) * elementListLength); 在 JS 層我並沒有先建立整個陣列，而是到了 C++ 才用 malloc 方式去索取陣列的記憶體空間，此時新增加的記憶體空間 JS 並不知道在哪裡，所以我必須想辦法回傳，此時可以透過 return value，我選擇直接修改 result 的值，暫存記憶體位址，再用這個位址去找真正的陣列所在處\n1 2 3 4 5 6 const resultRef = Module.HEAP32[resultBuffer / Int32Array.BYTES_PER_ELEMENT]; const resultInMem = new Int32Array(sizeInMem); for (let i = 0; i \u0026lt; sizeInMem; i++) { resultInMem[i] = Module.HEAP32[resultRef / Int32Array.BYTES_PER_ELEMENT + i]; } 唸起來很擾口，但也就是多一次的記憶體位址的轉換\nfree 最後別忘了要釋放索取的記憶體，避免 Memory leak\n1 Module._free(resultBuffer); 總結 WebAssembly 讓網頁開發的「部分功能」可以外包給給其他語言，讓網頁開發的疆域與技術更加的彈性與兼容，甚至未來可以有更多的跨語言協作的可能，十分令人期待\n這一篇教學介紹了 Emscripten 工具，與 C/C++ 編譯出的 Wasm 如何跟 JS 互動，包含基本的整數運算、陣列操作、Pointer 與記憶體存取\n下一篇預計介紹不使用 Emscripten，直接用 Clang 編譯 Wasm，還原到最簡單原始的狀態去認識 Wasm\n","date":"2020-01-01T05:21:40.869Z","permalink":"https://yuanchieh.page/posts/2020/2020-01-01-webassembly-%E6%95%99%E5%AD%B8-%E5%9F%BA%E6%9C%AC%E9%81%8B%E7%AE%97%E9%99%A3%E5%88%97%E8%99%95%E7%90%86%E8%88%87%E6%8C%87%E9%87%9D/","title":"Webassembly 教學 - 基本運算、陣列處理與指針"},{"content":"\n博客來連結\n前言 《原子習慣》是本實證的書籍，看完覺得很受用也感觸良多，所以會用較多的自身經驗去應證與實踐書中內容，打完心得後發現篇幅遠比自己想像中長，也比較多個人的歷程 (防雷宣告)\n這兩個月生活有些改變，跑完人生第一場全馬馬拉松(也是人生第一場馬拉松)、部落格從 Medium 移出至 Hugo 自己 host、租屋從永和搬至三重，這三件事情算是下半年的人生里程碑，在生活習慣與環境轉變的過程，察覺自己在心理狀態上有些微妙的模式，後來看到這本書《原子習慣》，不禁想說「對啊! 就是這樣」\n自己一直非常著迷於行為科學的書籍，我認為天生的基因會影響在不同領域投注的效率以及成就天花板，但往往常人很難去達到基因的極限，我們應該更專注於我們所能控制的 - 打造正確的學習態度與積極正向的心態\n這些年陸續看了《大腦喜歡這樣學》、《刻意練習》到現在推薦的這本《原子習慣》，從最基層大腦的生理運作方式、各種行為科學實驗到各領域的頂尖人才研究，讓我更相信改變沒有想像中的困難，只要找對學習的方法\n在自己生活中也試著落實書中的理念，去調整自己的學習方式與心態，雖然成就方面遠不足以說嘴，但跟兩年前的自己相比，卻也成長許多\n最近開始察覺到自己在某些方面有了退步，例如部落格更新的頻率下降非常多，從以前一個月四篇到現在一個月一篇，一方面是有意希望下降自己發文的頻率，希望讓自己有更長的學習週期去鑽研更難的問題而非為了發文而發文，但卻也慢慢開始出現了怠惰，看到《原子習慣》才驚覺”習慣“的樣貌，發文頻率下降的原因是可以被拆解，也才有可以被改善的可能\n我們都在不斷的設定目標，小至讀書計畫大至財富自由，希望用自制力去說服(強迫)自己培養出正確的習慣，但往往堅持不了多久就放棄了，這是因為我們忽略探討「習慣」的本質，所以才無法「養成好習慣與杜絕壞習慣」\n如果你覺得「會放棄的人就是自己不夠努力、自制力不夠好」，又或是「元旦制定了新年新計畫卻每次撐不到農曆年就放棄的人」這本書可以帶給你不同的想法，並提供非常實用的技巧\n核心理念 複利的力量比原子彈還可怕 每天進步 1%累積一年之後就是 37倍的成長，堅持每個微小的改善，累積起來就是驚人的成長，這也是\u0026quot;原子 atomic\u0026quot; 在書名的含義，在 2003 年英國自行車協會雇用新的教練 戴夫・布萊爾斯福德，在此前英國自行車隊在過往一百多年只是一隻平庸的車隊，布萊爾斯福德決定在每個面向都嘗試做出微幅改善，例如重新設計坐墊讓選手更好做、設法增加輪胎摩擦力、調整每位選手的訓練模式等，累積這些正面的改變後，英國車隊在短短數年內斬獲了九項奧運紀錄與多項世界紀錄\n已經有很多成功學都在探討複利的威力，但這邊有個重點是\n微小\n專注於系統而非目標 往往我們習慣制定很多的目標，例如說全馬要四小時內跑完(破四)，設立這樣的目標會帶來一種心理暗示「如果我破四了才會快樂，反之則不快樂」的單選題，如果沒有達成目標就會很沮喪，反之即使達成目標了，也就會因此停下腳步；\n作者提議我們應該專注於系統而非目標\n目標是想要達到的成果、系統是讓你達到成果的過程 例如說你是教練，你的目標是拿下冠軍，你的系統是招聘球員、帶隊訓練、制定比賽策略等\n這不是說目標不重要，有了具體的目標才有後續打造系統的可能，但只聚焦於目標而非系統容易過於短視，而習慣就是專注於打造良好的系統，每日投入正確的事情，累積起來就不太需要擔心成果會太差 (此心法或許僅限於個人而非公司使用)\n以我自身的經驗一開始馬拉松練習確實也想要拼破四，網路上找了菜單開始按表操課，每日的練習確實帶來進步，越接近比賽跑得也越快，但心情也越來越低落，甚至不太想跑步了；\n因為一方面目標設定的有點高，沒有參加過長跑第一次就想要挑戰全馬遠比想像中困難，另一方面一直聚焦於目標與現實的落差實在讓人沮喪，導致後面反而倦怠成績又更差強人意\n自我身份認同 - 你想要成為怎樣的人 當我們做一件事一個決定，可以再往回追溯「我們要如何做」以及「為什麼我們要這樣做」，也就是 Golden Circle 所描述的 What \u0026lt;-- How \u0026lt;-- Why\n在行為上也是，成果 \u0026lt;-- 系統(改變過程) \u0026lt;-- 身份認同，習慣的養成讓我們自然而然去做某件事，持續的做，在執行的過程不斷的加強「我是誰」的意象，這又回應到為什麼專注於系統有時比專注於目標更重要，因為這關乎到我們最後會成為怎樣的人；\n而我們自身的身份認同，也決定了我們決定去打造對應的習慣，兩者是互相影響的\n作者舉一個有趣的例子，有個癮君子決定要戒菸了，當一個人遞香菸給他時，如果他回答「對不起我不抽煙」跟「對不起我正在戒菸」，前者戒菸的成功機率會高出許多，因為他的身份認同已經是不抽菸的人，所以不抽菸的舉動對他來說是很正常的回應(或蓄意塑造自己)；\n而後者還是以抽菸者自居，每次不抽菸都是種掙扎，容易又受到誘惑而菸癮復發\n習慣形成 習慣的養成主要是因為大腦的心力是有限的，每次也只能專注在一件事情上，但生活中大大小小有這麼多的決定要做，小至每天吃什麼穿什麼到工作上的商業決策，所以大腦為了節省資源，漸漸的一些熟悉的事情就會用固有的模式處理，這些模式就成了我們的習慣\n但習慣的養成是有一個迴路\n提示 -\u0026gt; 渴望 -\u0026gt; 回應 -\u0026gt; 獎勵\n心理學家做了一項實驗，將貓放在迷龍的設計中，設計多個出口，必須拉下出口旁的拉桿門才會開，成功找到出口後會有食物的獎勵，心理學家觀察到貓咪隨著練習的次數，開始知道要拉拉桿，認得正確出口的速度也越來越快\n所以將習慣的迴路拆解套用在人的生活上\n提示：手機響了有新訊息\n渴望：想要知道訊息內容\n回應：拿起手機\n獎勵：滿足了知道訊息的渴望\n當我們要建立好習慣跟戒除壞習慣，就要從這四個步驟下手\n提示 人類受到環境的影響遠比想像中的大，例如說商店貨架上，跟視線等高的商品區銷售額更好；又或是在零食區顯眼處擺放健康食品，不做其他改善人們也會傾向拿取健康食品\n我們在做決定時很多時候都處於淺層意識的狀態，這時候環境的提示就會自然而然引導我們，所以想要建立好習慣就讓他更顯而易見 / 戒除壞習慣就連看都不要看到\n像是現代人容易受到 3C 產品的誘惑，即使想要集中精神還是會不斷的被干擾，最好的方式是連看到都不要看到，舉例來說在書房看書就把手機遺留在客廳，連看都看不到； 如果說想要培養運動習慣，就在每天睡覺前把運動服整理好，放在一起床就會看見的地方，隔天醒來下意識就會被提示「該運動了」\n環境 環境對人也有提示的功能，例如說習慣在沙發上滑手機吃零食，只要一坐到沙發上就會不自覺想要去拿零食，作者建議一個地方就只做一件事，書桌就只用來辦公跟讀書、沙發就用來看電視放鬆等等，一個位子一件事，當你到了那個環境，自然就會做原本熟悉的事情\n如果想要徹底改頭換面，換個新的環境或許是個不錯的選擇\n渴望 當人類收到獎勵時，腦中會分泌「多巴胺」，也就是讓人類感到放鬆快樂的激素，但根據研究證實，當渴望獎勵時而尚未收到獎勵時，也會分泌多巴胺，而渴望也會比實際收到獎勵時的快樂更強烈\n綑綁獎勵 所以我們可以透過綑綁獎勵，將一個不怎麼有吸引力的好習慣捆綁在一個喜歡的習慣上，例如說有個工程師希望培養騎腳踏車的運動習慣，同時他也很愛看 Netflix，所以他決定結合兩者，當腳踏車機踩到一定時速之後，Netflix 才會開始播放，反之就會暫停播放\n公式是\n做完 [目前習慣後]，我將執行 [我需要的習慣]\n做完 [我需要的習慣] 後，我會執行 [我想要的習慣]\n例如午休回來後打電會給三個客戶，打完後就可以滑手機這樣，將需要被培養的習慣安插在中間\n尋求團體 人們都有尋求 歸屬 的心理需求，我們總希望找到一個認同自我的團體，同時我們為了加強認同也不知不覺開始模仿團體中大多數人的習慣，希望藉此獲得掌聲，有研究發現如果身邊有朋友變胖了，那自己變胖的機率會提高 1/3\n同樣的我們可以透過加入團體來培養自己新的習慣，但需要注意\n你希望的習慣是常態 你跟這個團體本身有共通點 有個共通點我覺得蠻重要的，如果沒有一開始很難融入，後續要培養習慣就會很困難\n我自己是加入了龍舟隊之後，發現大家都在跑馬拉松，不知不覺也興起培養跑步興趣的念頭，雖然不是跟龍舟隊一起練跑，但在社群媒體上分享互相加油，也是我一直堅持下去的動力之一\n回應 當你在房間排除雜念，把前置作業都準備好了，最終阻擋你完成任務的原因是什麼？\n某大學的教授做了一項實驗，想要探討兩種模式對於拍照技巧的磨練\nA 組是量組，以量的做成績的計算方式，上交 100 張拿到 A，90 張拿到 B 等等；\nB 組是質組，學期末上交一張自認為最完美的照片，以這張作品評分\n最後品質比較好的是哪一組呢？\n教授發現是 A組，因為被鼓勵大量拍攝，一開始可能拍出不好的照片，但不斷的試錯過程也提升了拍照技巧；\nB 組沒有規定不能大量拍攝，但因為被暗示要「追求完美」，所以反而會想太多，反而練習的次數下降，最終成果反而不這麼完美\n當然練習也不是無腦的一直反覆做(詳見《刻意練習》一書)，但是人們會有個天性是追求完美，否則寧願不做的心態，書中引用伏爾泰的一句話 至善者，善之敵，寧可開始行動，慢慢修正，也不要一直埋著頭苦幹，就如同精實創業，你應該先從最小可行性產品開始，從市場得到反饋，反之人的習慣養成也是\n兩分鐘法則 作者提出 兩分鐘法則，不要怕不完美，假設你想培養做伏立挺身的習慣，不需要一開始就從 3,50 下開始，從 5 下開始就好，甚至 2 下也沒有關係，重點是 開始行動；\n想要培養跑步不如從提醒自己要到戶外走走，等習慣後再慢慢增加強度，不要太好高騖遠，從兩分鐘開始就好\n調整執行習慣的成本 如果希望建立習慣，我們應該要降低執行的成本，上面的兩分鐘法則是為了降低心理成本，在執行面上可以將東西歸位好，當要執行時就可以立馬行動不用在找東找西；\n又或是反過來，讓壞習慣的執行成本很高，作者分享到為了戒除不斷滑社交媒體，她與另一個好友互相改對方密碼，等到週五才可以跟對方要新的密碼登入，他發現到這樣的改變就大大降低他對社交媒體的依賴\n獎勵 有些時候好習慣的獎勵是有延遲性，例如說健身要過個數週才有成效，這也是為什麼好習慣不容易持續的關係，但我們還是能透過不同的方式去改變\n迴紋針策略 有個成功的業務分享到，每當他打完一通電話給客戶，就把迴紋針從 A 桶搬移到 B 桶，透過投入迴紋針，他不知道要打多少通電話才會成交一通，但透過投擲迴紋針，他知道自己每次的電話都是有成果的，所以將目標可視化是個很好追蹤的方式\n小時候每次拿到好寶寶標章也是相同的道理，現在有許多的 App 可以幫忙追蹤，例如跑步也有跑步的里程，每次看到自己的累積，看到自己的成績穩定進步，確實有很大的滿足\n如果可以的話讓目標追蹤很顯眼，這就會再轉變成下一個迴圈的提示\n問責夥伴 也可以找個夥伴，告訴對方自己的目標後，由雙方互相提醒與鼓勵，透過人際關係給自己壓力與獎賞，是個不錯的作法\n結語 1 2 3 4 讓提示顯而易見 讓習慣變得有吸引力 讓行動輕而易舉 讓獎賞令人滿足 最終人生就是由一連串微小的行為所串連，如果我們可以更有意識地去打造自己的潛意識，那或許這樣能更貼近我們想要的生活，看完《原子習慣》讓我重新思考自己的生活，剛好適逢搬家，也是徹底打造新習慣的開端\n","date":"2019-12-14T05:21:40.869Z","permalink":"https://yuanchieh.page/posts/2019/2019-12-14-%E9%96%B1%E8%AE%80%E5%BF%83%E5%BE%97-%E5%8E%9F%E5%AD%90%E7%BF%92%E6%85%A3%E7%B4%B0%E5%BE%AE%E6%94%B9%E8%AE%8A%E5%B8%B6%E4%BE%86%E5%B7%A8%E5%A4%A7%E6%88%90%E5%B0%B1%E7%9A%84%E5%AF%A6%E8%AD%89%E6%B3%95%E5%89%87/","title":"[閱讀心得] 《原子習慣》細微改變帶來巨大成就的實證法則"},{"content":"介紹 上一篇完成了用 Terraform 實作單一區域定時執行 Lambda 的部署，這一篇將轉成 module，並使用 for loop / if condition，一次部署到多個區域，同時探索 Terraform 本次教學沒有用到卻也值得留意的功能\nModule - 模組化 先前提到，只要在專案根目錄下，任何的 *.tf 檔案都是 root module，在 $terraform apply 時都會被執行；\n如果要獨立出個別的模組，可以放在不同專案下，透過放在 github、s3 等 remote 方式載入，又或是單純獨立出一個資料夾放置，用路徑的方式載入，先重整原本的專案資料夾架構\n1 2 3 4 5 6 7 8 9 10 11 12 --- |--- main.tf // 進入點 |--- input.tf // 定義參數 |--- output.tf // 定義輸出 |--- modules // 存放所有的路徑 |--- lambda-api-test |--- main.tf // module 的進入點 |--- input.tf // module input |--- lambda-function.zip ... 上一篇所有的資料 |--- global |--- global.tf 重新思考之後要部署的架構，global 用來存放全域的資源，例如說 IAM Role / DNS 等資源，這部分會需要先被創立，方便後續的資源綁定；\n接著把各區域相同的架構包成 module 放置在 modules 底下，程式碼移除 IAM 資源，其餘大多雷同，只是要注意如果有用到 file 相關的參數，要改變路徑位置為 ${path.module}，否則會找不到資源；\n1 2 3 4 5 6 7 resource \u0026#34;aws_lambda_layer_version\u0026#34; \u0026#34;lambda-layer_fetch\u0026#34; { filename = \u0026#34;${path.module}/lambda_layer_payload.zip\u0026#34; layer_name = \u0026#34;lambda_layer_name\u0026#34; source_code_hash = \u0026#34;${filebase64sha256(\u0026#34;${path.module}/lambda_layer_payload.zip\u0026#34;)}\u0026#34; compatible_runtimes = [\u0026#34;nodejs10.x\u0026#34;] } 如果要引用 module，也就是 ./main.tf 的內容為\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 locals { region = \u0026#34;us-east-1\u0026#34; } provider \u0026#34;aws\u0026#34; { profile = \u0026#34;default\u0026#34; region = \u0026#34;${locals.region}\u0026#34; } module \u0026#34;lambda-api-test_us-west-2\u0026#34; { source = \u0026#34;./modules/lambda-api-test\u0026#34; # depends_on = [\u0026#34;aws_iam_role.iam_for_lambda\u0026#34;] region = \u0026#34;${local.region}\u0026#34; lambda_variables-SLACK_URL = \u0026#34;slack url\u0026#34; iam_role_name = var.lambda_role_name } 宣告 module 並透過相對路徑指定 source，其餘的參數對應 module 的 input；\n要注意目前 v0.12 module 不支援 depends_on，這也是為什麼 IAM Role 創立要獨立到 global.tf 先執行，不然目前無法先建立 IAM Role 再建立 module\nlocals locals 用來宣告區域變數，就像是寫程式中僅用於限定範圍內的變數，後續透過 local.{var} 取用\ndata source 如果有需要跨檔案路徑存取資源，又或是讀取某些資料例如 AWS 所有的可部署區域列表，又或是執行某些指令如呼叫 lambda，可以宣告 data source；\ndata source 跟 resource 最大差別是 data source 是唯讀，並大多數執行於 apply 階段之前，後續的資源建立都可以使用；\n而 resource 則是會被 $terraform 指令影響而增加、刪除、修改資源。\n看到 data source 讓我十分的興奮! 因為這代表我們有更好的方式與既有的架構共存\n如果你擔心導入 Terraform 會不小心破壞現有的架構，可以透過 data source 去擷取重要的資料同時保證 Terraform 不會修改或是刪除，例如說 DNS設定、IAM Role 等等\ndata source 個別取用方式可以查文件，最基本就是用 name 當作搜尋依據，例如說我在 ./module/lambda-api-test/main.tf 希望存取 ./global/global.tf 或是不存在於 terraform 專案下的 IAM Role，可以用\n1 2 3 4 5 6 7 8 9 10 11 # data \u0026#34;資源類型\u0026#34; \u0026#34;資源名稱\u0026#34; # { 搜尋條件與參數 } data \u0026#34;aws_iam_role\u0026#34; \u0026#34;iam_for_lambda\u0026#34;{ name = \u0026#34;iam_role_name\u0026#34; } # 存取示範 resource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;lambda_main\u0026#34; { role = \u0026#34;${data.aws_iam_role.iam_for_lambda.arn}\u0026#34; .... } data source 也是用來跨 module 間傳遞資源的方法，但要自己釐清 module 的先後順序\n條件式 - for / if HCL 是個宣告式語言，讓我們可以用 high level 方式宣告我們的意圖，至於如何實作就不用我們操心；\n但跟程序式語言比起來，條件判斷與迴圈等邏輯判斷舊沒有如此的方便，不過 HCL 還是支援基本的條件判斷語法，雖然沒這麼直觀，但還是有辦法滿足大多數的應用場景\ncount count 是最早支援的語法，主要是重複創建資源，透過 count.index 取得當下 interation 的 index\n1 2 3 4 resource \u0026#34;aws_iam_user\u0026#34; \u0026#34;example\u0026#34; { count = 3 name = \u0026#34;neo.${count.index}\u0026#34; } 也可以搭配 list，動態調整變數\n1 2 3 4 5 6 7 8 9 variable \u0026#34;user_names\u0026#34; { description = \u0026#34;Create IAM users with these names\u0026#34; type = list(string) default = [\u0026#34;neo\u0026#34;, \u0026#34;trinity\u0026#34;, \u0026#34;morpheus\u0026#34;] } resource \u0026#34;aws_iam_user\u0026#34; \u0026#34;example\u0026#34; { count = length(var.user_names) name = var.user_names[count.index] } 如果想要 access 資源的輸出，可以透過 [index/*] 方式取得\n1 2 3 4 output \u0026#34;all_arns\u0026#34; { value = aws_iam_user.example[*].arn description = \u0026#34;The ARNs for all users\u0026#34; } count 搭配三元運算式，就變成了現成的 if/else\n1 2 3 4 resource \u0026#34;aws_iam_user_policy_attachment\u0026#34; \u0026#34;neo_cloudwatch_full\u0026#34; { count = var.give_neo_cloudwatch_full_access ? 1 : 0 .... } count 限制 不能用於 inline block\n有些 resource 有 inline block，例如 auto scaling group 可以指定 tag，此時的 tag 不能使用 count 1 2 3 4 5 6 resource \u0026#34;aws_autoscaling_group\u0026#34; \u0026#34;example\u0026#34; { .... tag { count = 3 (無法使用) } } 採用 list 時的元素增減\n如果創建資源時是用 list 搭配 count，必須注意 Terraform 在後續更新資源時是認定 list index 而非元素本身 例如原本是 [\u0026rsquo;ele1\u0026rsquo;, \u0026rsquo;ele2\u0026rsquo;, \u0026rsquo;ele3\u0026rsquo;]，此時希望刪除 ele2，變成 [\u0026rsquo;ele1\u0026rsquo;, \u0026rsquo;ele3\u0026rsquo;]\n但是 Terraform 會解讀成 刪除 ele3，並更新 ele2 成 ele3，這一點必須特別注意，不然就要使用其他的迴圈方式\nfor_each for_each 是在 0.12 加入，可以輪詢指定的 collection，並支援 inline block!\n如果 collection 為空值則效果等同於 count = 0\n如果多個 resource 本身近乎一致可以用 count，但大多數情況請用 for_each\n配合 dynamic 就可以用於 inline block，以下是建立 security group 時指定 ingress 多組 port\n1 2 3 4 5 6 7 8 9 10 11 12 resource \u0026#34;aws_security_group\u0026#34; \u0026#34;example\u0026#34; { name = \u0026#34;example\u0026#34; dynamic \u0026#34;ingress\u0026#34; { for_each = var.service_ports content { from_port = ingress.value to_port = ingress.value protocol = \u0026#34;tcp\u0026#34; } } } for_each 也可以搭配 map 使用\n1 2 3 4 5 6 7 8 resource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;rg\u0026#34; { for_each = { a_group = \u0026#34;eastus\u0026#34; another_group = \u0026#34;westus2\u0026#34; } name = each.key location = each.value } 透過 each 加上 key, value 取得需要的值\nWarning! 目前 Terraform 還有幾個不支援的功能，例如說 provider 不支援變數、module 不支援 depends_on、module 不支援 for loop\n這三個功能不支援讓 multiple region 部署時相當不方便，module 支援 for loop 有在接下來的 Terraform roadmap 中，但還不確定何時會支援\n另在 Refactor 時務必注意，例如說 resource name 更新，Terraform 大多數會刪除舊資料並重建新資料，即使改個名稱而已，所以務必要仔細看 $ terraform plan 的結果，避免造成不必要的 downtime\n或是有幾個方式可以避免 downtime\n修改 lifecycle 為 create_before_destroy 每個 resource 可以指定 lifecycle，create_before_destroy 會先創建新資源再刪除舊資源，避免 downtime 1 2 3 4 5 6 7 resource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;example\u0026#34; { # ... lifecycle { create_before_destroy = true } } 其他 lifecycle 還有 prevent_destroy 不會Terraform 被刪除 以及 ignore_changes 指定某些屬性更新不觸發 Terraform 更新資源 2. 使用 Terraform CLI 改變 state 像是要修改 resource 名稱，可以透過修改 state 而非資源本身即可 $ terraform state mv，盡量透過指令去修改 state，而不是手動直接改 tfstate\n結語 完整程式碼，後來決定將上一篇的內容整理成 module，接著 iam role 部分獨立出來創立，接著用 data source 方式引入；\n多區域部署套用 module 獨立宣告，可惜 for_each 尚未支援 module\n以下是 slack 的 log 畫面\n對於導入 Terraform 評估蠻正面的，一來有 data source 或 import 與現有架構整合，又可以不擔心搞爛整個架構；\n二來語法都慢慢完整，可以應付大多數的場景，確實省下很多的管理上的心力，期待之後可以用 Terraform 整合 Kubernetes，並整合 CI/CD，讓開發、整合、部署、維運可以更順暢\n接下來要繼續熟練 Terraform，希望挑戰整合 Docker 的跨區域跨 Provider 部署\n","date":"2019-11-04T00:21:40.869Z","permalink":"https://yuanchieh.page/posts/2019/2019-11-04-%E5%88%9D%E8%A9%A6-terraform-%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%B4%B9%E8%88%87%E7%94%A8%E7%A8%8B%E5%BC%8F%E7%A2%BC%E9%83%A8%E7%BD%B2-lambda-%E4%B8%8B/","title":"初試 Terraform - 基本介紹與用程式碼部署 Lambda (下)"},{"content":"介紹 Terraform 在 Github 上有一萬九千多顆星星(截自發文日)的開源專案，由 HashiCorp 這間專注於 DevOps 工具開發的公司所維護，主要透過 DSL 編寫定義檔，管理跨雲端架構，讓架構也可以程式碼化，近一步更好的協作、版本控制等好處，達到 Infrastructure as code 的目標。\nTerraform 可以達到以下幾件事：\n架構代碼化\nTerraform 採用宣告式程式語言(declarative language)的 DSL，但同樣提供基礎的程式語言該有的功能，例如變數、輸入輸出、模組化等，其中模組化也支援加載外部模組，不用擔心違反 DRY，內建一些函示也都很好用； 當你有多個環境要部署時，可以用同樣的架構但是不同的機器規格與參數，管理上很方便 跨平台服務\nAzure / GCP / AWS / Heroku 都可以，其他的工具包山包海，可以參考文件 Providers 自動管理架構升級\n架構異動時，Terraform 會自動更新或替換正確的資源，同時也可以一鍵刪除 團隊協作\n提供多樣的解決方案，可以用官方的 Terraform Cloud 或 AWS S3 等，在團隊內共同管理 與現有架構整合\nTerraform 提供兩種方式與既有架構整合，一是維持唯讀型態只存取資源(例如讀 AWS arn 綁定到 Lambda 上)、二是 Import 資源一並由 Terraform 管理(增加、刪除、修改) DX 很好\nDeveloper Experience 還不賴，官方的文件、教學，以及整體的設計上都很友善，錯誤也會很直接顯示哪一行的哪一部分語法錯誤，學習上 Debug 上都很容易，HashiCorp 員工有分享這是他們在 0.12 很大的修正，讓用戶更快找出錯誤是他們重視的一環 這次目標跟上次的 CDK 研究一樣，部署一個每五分鐘執行的 Lambda，並分佈到多個區域，CDK 教學連結 AWS-CDK教學 — Infrastructore As Code 用程式碼管理架構\n事前準備 請先安裝 Terraform，並設定好 AWS configuration，也可以先玩過官方教學 Terraform getting started； 另一個很棒的參考資料 An Introduction to Terraform，系列文超仔細也超實用，比官方文件還推薦，作者也有出書，有機會應該會入手\n部署單區域的 Lambda 與 IAM Role 創建一個檔案，先命名為 main.tf ，在 Terraform 中檔案分成 root module 與 module，沒有特別宣告是 module 則為 root module，目錄下可以有多個 root module，檔案名稱沒有進入點問題，只要結尾是 .tf 即可\n首先第一步，先部署單一區域的 Lambda，與建立對應需要的 IAM Role，以下程式碼主要做幾件事\n宣告 aws 部署的區域 建立新的 IAM role 命名為 iam_for_lambda，並給予調用 lambda 的權限 建立 IAM Policy 命名為 lambda_logging，給予 Cloudwatch log 權限 將 IAM Policy 賦予 IAM role，lambda_logging 給 iam_for_lambda 等等 Lambda 會用到一些 node_modules，建立 Lambda layer 命名為 lambda-layer_fetch 建立 Lambda function aws_lambda_function，綁定 lambda-layer_fetch 與執行角色 iam_for_lambda 寫完介紹，剛好一步對照一塊程式碼，如果對 AWS 有點熟悉的人應該可以很快理解語法，尤其是變數命名跟後台設定很雷同，所以上手相當輕鬆\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 # 指定後續資源的提供者是哪個平台的哪個區域 # provider 沒有指定 alias 代表為預設 provider provider \u0026#34;aws\u0026#34; { profile = \u0026#34;default\u0026#34; region = \u0026#34;us-east-1\u0026#34; } # 建立 IAM role，取名為 iam_for_lambda resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;iam_for_lambda\u0026#34; { name = \u0026#34;iam_for_lambda\u0026#34; assume_role_policy = \u0026lt;\u0026lt;EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34; } ] } EOF } # 建立 IAM Policy，主要給 Cloudwatch Log 權限 resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;lambda_logging\u0026#34; { name = \u0026#34;lambda_logging\u0026#34; path = \u0026#34;/\u0026#34; description = \u0026#34;IAM policy for logging from a lambda\u0026#34; policy = \u0026lt;\u0026lt;EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; } ] } EOF } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;lambda_logs\u0026#34; { role = \u0026#34;${aws_iam_role.iam_for_lambda.name}\u0026#34; policy_arn = \u0026#34;${aws_iam_policy.lambda_logging.arn}\u0026#34; } resource \u0026#34;aws_lambda_layer_version\u0026#34; \u0026#34;lambda-layer_fetch\u0026#34; { filename = \u0026#34;./lambda_layer_payload.zip\u0026#34; layer_name = \u0026#34;lambda_layer_name\u0026#34; source_code_hash = \u0026#34;${filebase64sha256(\u0026#34;lambda_layer_payload.zip\u0026#34;)}\u0026#34; compatible_runtimes = [\u0026#34;nodejs10.x\u0026#34;] } resource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;lambda_main\u0026#34; { function_name = \u0026#34;global-api-lantency-test\u0026#34; filename = \u0026#34;./lambda_payload.zip\u0026#34; handler = \u0026#34;index.handler\u0026#34; runtime = \u0026#34;nodejs10.x\u0026#34; role = \u0026#34;${aws_iam_role.iam_for_lambda.arn}\u0026#34; source_code_hash = \u0026#34;${filebase64sha256(\u0026#34;lambda_payload.zip\u0026#34;)}\u0026#34; layers = [ \u0026#34;${aws_lambda_layer_version.lambda-layer_fetch.arn}\u0026#34; ] publish = true environment { variables = { REGION = \u0026#34;us-east-1\u0026#34; SLACK_URL = \u0026#34;https://hooks.slack.com/services/.....\u0026#34; } } } Provider 指定資源是套用在哪個平台，目前是指定在 us-east-1 AWS 上，如果沒有指定 alias 則代表是預設的 provider\nResource 命名的方式是\n1 2 3 resource 資源類型 資源名稱 { 資源參數 } 資源參數對應資源類型，可以從文件中找範例與定義的方式，資源的定義依賴於 Provider 平台的不同，可以指定 provider，不指定則用預設\n除了個別的資源定義外，像有些資源會相依，例如 Lambda 要綁定特定的 IAM Role，注意到這邊的寫法是\n1 2 3 4 resource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;lambda_main\u0026#34; { role = \u0026#34;${aws_iam_role.iam_for_lambda.arn}\u0026#34; .... } 要引用其他資源的參數，需要用\u0026quot;${資源類型.資源名稱.arn}\u0026quot;，arn 是 AWS 用來識別資源的全域 ID，透過這樣的方式就能夠綁定資源，這邊我指定要用 Lambda 的 Execution Role 是 iam_for_lambda 這個 Role。\n另外為了效率，定義的資源會並行建立，但有些資源有相依性，Terraform 會自動處理相依性，所以上述的資源理論上要 Policy \u0026gt; Role \u0026gt; Lambda Layer \u0026gt; Lambda，但我們不用宣告 Terraform 會自行處理； 但有時候相依性不明顯或是有特殊需求，可以顯示宣告 depends_on。\nLambda Function 建立完成後，如果只是要單純更改 Lambda 內容而不調整架構，可以宣告\n1 2 resource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;lambda_main\u0026#34; { source_code_hash = \u0026#34;${filebase64sha256(\u0026#34;lambda_payload.zip\u0026#34;)}\u0026#34; source_code_hash 是指說如果 hash 值改變就更新 Lambda 內容，而 filebase64sha256() 是 Terraform 的內建函示，自動用 sha 256 算出檔案 hash 值並用 base64 編碼\n題外話，Lambda 的 zip file 記得解壓縮後不要有額外的資料夾，不然會失敗，正確應該要是\n1 2 --- lambda.zip |--- index.js 部署架構 編寫好架構，此時要調用 Terraform CLI 來部署架構 首先初始化環境與載入需要的執行資源\n$ terraform init\n一開始 Terraform 並不知道建立的 Provider 是誰，直到初始化才會下載對應的 Library，放在專案路徑底下的 .terraform 資料夾下\n成功後，就可以部署架構了\n$ terraform apply\n此時 Terraform 會列出更動的資源，+ 代表需要新建的資源、-代表會被刪除的資源、~代表會被更新的資源，注意資源更新可是刪除舊的資源部署新的資源，依照各家 provider 的 API 而有所不同，需要特別留意 如果確認就輸入 \u0026ldquo;yes\u0026rdquo;，等 Terraform 幫忙部署\n這樣就完成了，後續有什麼調整就重複 $ terraform apply 步驟，可以到 AWS 後台確認資源的建立狀況\n加上 Cloudwatch 這一段雷同，補上 cloudwatch event rule / cloudwatch event targe，最後別忘了要加綁定 lambda permission 不然觸發 Lambda 會失敗\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # add cloudwatch event resource \u0026#34;aws_cloudwatch_event_rule\u0026#34; \u0026#34;every_five_minutes\u0026#34; { name = \u0026#34;routine-api-request\u0026#34; description = \u0026#34;Routinely call global api lantency test\u0026#34; schedule_expression = \u0026#34;rate(5 minutes)\u0026#34; } resource \u0026#34;aws_cloudwatch_event_target\u0026#34; \u0026#34;api_request\u0026#34; { rule = \u0026#34;${aws_cloudwatch_event_rule.every_five_minutes.name}\u0026#34; target_id = \u0026#34;CallApiRequest\u0026#34; arn = \u0026#34;${aws_lambda_function.lambda_main.arn}\u0026#34; } # resource \u0026#34;aws_lambda_permission\u0026#34; \u0026#34;allow_cloudwatch_to_call_check_api\u0026#34; { statement_id = \u0026#34;AllowExecutionFromCloudWatch\u0026#34; action = \u0026#34;lambda:InvokeFunction\u0026#34; function_name = \u0026#34;${aws_lambda_function.lambda_main.function_name}\u0026#34; principal = \u0026#34;events.amazonaws.com\u0026#34; source_arn = \u0026#34;${aws_cloudwatch_event_rule.every_five_minutes.arn}\u0026#34; } Import 現有的 IAM Role 回過頭說一下之前採用 AWS CDK 的最大問題，當初研究時沒有看到 AWS CDK 與現有架構的整合，這導致公司要採用需要很大的決心，或是只能用在測試或新的環境建設，沒有辦法 graceful 轉移，這點我覺得對於要導入新技術來說，有點麻煩，尤其是架構這麼重要的地方；\n另一點是角色的權限管理，公司都會有針對不同的職位給予不同的權限，AWS CDK 預設就要 CreateRole 等權限，基本上很難直接要到這麼高的權限，也是當初要在公司專案嘗試 AWS CDK 最大失敗的原因\nTerraform 現行支援 import 既有的資源，但是資源內容要自己填寫，未來宣稱會支援自動載入內容；\nexisted_role 是我預先在 AWS 創建的 IAM Role，權限跟上面的 iam_for_lambda 一樣，記得要先在設定檔宣告，接著執行指令就完成了，之後 terraform destroy 也會一並刪除 (需要留意)\n$ terraform import aws_iam_role.existed_role existed_role\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # existed iam role resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;existed_role\u0026#34; { name = \u0026#34;existed_role\u0026#34; assume_role_policy = \u0026lt;\u0026lt;EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34; } ] } EOF } Variables - 抽離參數 目前的參數都是寫死的，例如說部署的區域、Lambda 名稱等，Terraform 支援 variable 定義，可以有以下幾種類型\nstring boolean number set map object (等同於 map，但會蓋過 map) tuple 如果要在參數使用變數的話，必須要先在資源檔 .tf 宣告\n1 2 3 4 5 variable \u0026#34;image_id\u0026#34; { type = string default = \u0026#34;default 值\u0026#34; description = \u0026#34;The id of the machine image (AMI) to use for the server.\u0026#34; } type 必填，但是 default 跟 description 不用，當沒有 default 且後續變數沒有賦值的話，在 \u0026gt;$terraform apply 時會中斷要求輸入\n在資源定義檔上，可以採用 var.變數名稱\n1 2 3 4 provider \u0026#34;aws\u0026#34; { profile = \u0026#34;default\u0026#34; region = var.image_id } 接著，透過以下幾種方式賦值給 variable\n環境變數 1 2 $export TF_VAR_image_id=ami-abc123 $export TF_VAR_availability_zone_names=\u0026#39;[\u0026#34;us-west-1b\u0026#34;,\u0026#34;us-west-1d\u0026#34;]\u0026#39; terraform.tfvars 檔案中 1 2 image_id = \u0026#34;ami-abc123\u0026#34; availability_zone_names=[\u0026#34;us-west-1b\u0026#34;,\u0026#34;us-west-1d\u0026#34;] terraform.tfvars.json 檔案中 1 2 3 4 { \u0026#34;image_id\u0026#34;: \u0026#34;ami-abc123\u0026#34;, \u0026#34;availability_zone_names\u0026#34;: [\u0026#34;us-west-1a\u0026#34;, \u0026#34;us-west-1c\u0026#34;] } *.auto.tfvars 或是 *.auto.tfvars.json，順序按照檔名 在 CLI 執行時指定 -var -var-file $terraform apply -var=\u0026ldquo;image_id=ami-abc123\u0026rdquo;\n如果有同樣的變數名稱，按照上面的規則順序後者蓋過前者，例如 -var 會蓋過其他檔案的宣告\nOutput - 輸出參數 對於 root module 來說，設定 output 會在 \u0026gt; $terraform apply 時打印，例如說 EC2 Instance 的 public DNS等； 對 module 來說，Output 等同於 function 的 return value，決定哪些資源讓外部讀取\n在範例程式的目錄下有獨立的 output.tf\n1 2 3 4 5 6 7 output \u0026#34;lambda-arn\u0026#34; { value = aws_lambda_function.lambda_main.arn } output 輸出參數名餐 { value = 資源類別.資源命名.資源屬性 } 會在 apply 成功後打印出來\n1 2 3 4 5 Apply complete! Resources: 2 added, 0 changed, 0 destroyed. Outputs: lambda-arn = arn:aws:lambda:us-east-1:..... State - Terraform 如何掌管架構的更動 當架構異動的時候，Terraform 如何知道前後架構的差異呢？ 每次在執行 $ terraform plan 時，專案目錄底下有 terraform.tfstate 檔案，用 JSON 描述架構中的所有資源，每當下次執行 $ terraform plan 時，Terraform 會根據 tfstate 中的資源 ID 取得最新的資訊，接著與描述檔做 diff 決定哪部分資源要更新\n當我們要跨團隊協作時，就需要把 terraform 描述檔 + terraform.tfstate 與團隊共享，此時會有有三個要素\n共享檔案與版本控制\n檔案共享是最基本的協作必備條件，同時將程式碼做版本控制也是很必要的功能 Lock 機制\n當共享了之後，Lock 就變成是必須考量的因素，避免團隊同時多人同步修改，造成前後衝突的狀況 獨立不同的 state 狀態\n在實際應用上，可能會有 development / staging / production 不同環境，希望共用程式碼建立雷同的架構，但又因為環境不同希望有不同的配置，例如機器大小或 VPC 等，此時就需要考量如何獨立不同環境 在跨同團隊協作很容易想到 git ，但嚴格來說 git 只能滿足第一點，所以在 Terraform 可以指定不同的 state 管理方式，除了官方的 Terraform Cloud，可以採用 AWS S3 + DynamoDB 達到上述的條件，並附帶版本控制，再之後會更詳細的描述操作流程，詳情可以參考 How to manage Terraform state\n結語 就這樣完成了單區域的 Lambda 部署與最基本的 Terraform 學習，以下是這次教學的程式碼 terraform-investigation\n下一篇將整個架構模組化，並一鍵部署多個區域\n","date":"2019-10-30T00:21:40.869Z","permalink":"https://yuanchieh.page/posts/2019/2019-10-30-%E5%88%9D%E8%A9%A6-terraform-%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%B4%B9%E8%88%87%E7%94%A8%E7%A8%8B%E5%BC%8F%E7%A2%BC%E9%83%A8%E7%BD%B2-lambda-%E4%B8%8A/","title":"初試 Terraform - 基本介紹與用程式碼部署 Lambda (上)"},{"content":" 當初在 Youtube 看到 Douglas 為書籍做導讀的影片，重新介紹每個 JS 日常開發的部分，從型別開始 Number / Date / Function / Object / String / Array / Ｄㄍ； 延伸到 Promise / Exception / Generator / Async \u0026amp; Await； 最後寫一個自創的語言 Neo 解釋 Transpiler 的工作，Tokenizing / Parsing / Runtimes；\nJavascript 是個神奇且矛盾的存在，它有許多錯誤與不好的設計，但避開這些坑他又是一門具備彈性、受到廣大熱愛的程式語言，再我讀來，Douglas 試著從前作《JavaScript: The Good Parts》描述如何避開 JS的坑擁抱正確的 JS編寫技巧，到這本《How JavaScript Works》，重新帶讀者認識 JS、如何讓 JS 變得更好 -\u0026gt; 如何寫出更好 JS Code，並試著傳遞出自己的強烈價值觀\n下個世代程式語言的樣貌\n是的，這本書雷同於前作，都表達了 Douglas 對於 JS 編程技藝的審美觀與價值觀，所以內容有些可能會與主流價值觀衝突，孰是孰非就見仁見智了； 但這本書帶給我的衝擊是如何去思考一門程式語言的設計，以往都在思考如何寫出更好的語法跟演算法，從沒想過程式語言的本身也是工具，自然就有被討論與改進的空間，跟個 Douglas 的腳步去思索，覺得是個蠻棒的思想試煉，這或許才是這本書最有價值的地方!\n那你心中的程式語言又是怎樣的樣貌呢？\n這本書適合對 JS 有一定認識的開發者，如果是新手就不建議閱讀。\n前言 一般的工具書在前言通常是講一些 Terminology 、寫作緣由、適合誰閱讀等，這本書也不例外，但有趣的是 Douglas 補充了他對某些英文單字頗有微詞，例如說 one，這個單字開頭是 O，但也很像是 0，但其實他表達的是 1，而且發音 /wʌn/，文字外在意象跟實質意義不吻合，且與發音也不一樣，對 Douglas 來說這是個非常差的設計，所以他主張要改成 wun，而整本書的 one 都用 wun 取代，包含 wunce； 另一個單字 through，這個字有一半的字母沒有發音，所以他省略為 thru。\n坦白說，一開始看到覺得有點無聊，想說這是沒事找碴的概念嗎 XD 但看完整本書後，覺得自己果然道行太淺，Douglas 想表達的是\n正確命名就是寫好程式的第一步，不留給自己任何思維上的模糊地帶\n任何有模糊解釋空間的地方，就會有誤解跟錯誤的產生，所以從源頭的命名開始就杜絕這種壞習慣，才是根本寫好程式的方法！ 這樣的精神貫穿整本書籍，用最挑剔的方式重新檢視 JS 的語法與實作設計。\nHow Names Work 在命名上，Douglas 認為程式語言應該要支援 當作變數名稱的合法字元，但目前 JS 是不行的，所以他建議用小寫字母開頭並用 _ 蛇行命名法切割多個單字的變數名稱； 至於 $、_ 開頭則避免使用，這些應該被保留當作 code generator 、macro processor 使用。\n在 JS 中，一大令人困惑的地方是 function 跟 constructor 無法區分，如果 function 用 new 呼叫就變成 constructor，所以 Douglas 建議 contructor 用大寫開頭作為區分，但可以的話連 new 都不要用，後續會在補充做法。\nHow Numbers Work 這也是 JS 很常被詬病的誤解 0.1 + 0.2 !== 0.3，但其實 JS 是參照 IEEE 754 其中的雙精度浮點數規範，其餘如 Java 等採用同樣的規範也都會有一樣的問題，畢竟用有限的 bits 怎麼可能對應到無窮的有理數呢，所以在精度上的缺陷才會導致這樣的結果。\n1 要小心切換儲存的數值與實際表示的數字，這兩者會有數學公式的對應關係，但文字上容易產生誤解 JS 只支援單一種的數字型別就是 Number，這是個良好的設計，因為不需要涉及數字型別的轉換，例如 int 轉 float 或 double 等，在現在機器記憶體非常充足的情況下，程式語言以開發者友善的方式設計會比較適合\nNumber 以 64 bits 儲存，拆分成3個部分儲存，significand 是介於 0.5 ~ 1.0 的數值\nsign(1) + exponent(11) + significand(52) 轉成數值代表 (-1) ^ sign * 1.significand * 2^(exponent - 0x3ff)\nsign 是一個位元表示正負； exponent 則是用 11個位元表示 2的次方倍，為了表示正區間到負區間所以預定有偏差值 (bias value 0x3ff)，也就是 exponent 0x000 代表的其實是 (-1 * 0x3ff)； significand 用剩餘 52 bit 表示，但因為浮點數表示表達為 1.X * 2^n 次方，因為1固定會存在，所以就可以記在記憶體中，算是額外的 bonus bit，所以最大數值表示可以到 2^53 次方\n幾個常見的數字對應 binary 表示\n1 2 3 4 5 6 7 8 -0: 8000 0000 0000 0000 +0: 8000 0000 0000 0000 1：3ff0 0000 0000 0000 =\u0026gt; 2 ^ (0x3ff - 0x3ff) * 2^53 (別忘了 bonus bit) = 1 最大數： 7fef ffff ffff ffff =\u0026gt; (2^54 -1) * 2^971 = 2 ^ (0x7fe - 0x3ff) * (2^54 - 1) 特殊字 無限大：0x7ff0 0000 0000 0000 NaN：0x7ff + 後面出現任意非0的bits，這也是為什麼 NaN !== NaN 須謹記只有在 Number.MAX_SAFE_INTEGER 到 Number.MIN_SAFE_INTEGER 之中的整數才是 1:1 mapping，也就是一個數字對應到一個真實的數值，在這個區間才保證數學運算的正確性，超過範圍的數值會喪失部分精準性，例如\n1 Number.MAX_SAFE_INTEGER + 1 === Number.MAX_SAFE_INTEGER + 2 // true 如果要檢查數字，請用 Number.{method}，例如 Number.isNaN、Number.isSafeInteger 等\nHow Big Integers/Floating Point/Rationals Works 受限於 Number的精度，如果像銀行系統等不容許半點失真的數學運算，就必須自己額外處理，這部分 Douglas 分別寫了 Big Integers / Floating Point / Rationals 的處理，Rationals 是指用兩個 Big Integer 相除表示的數值，可以更精確表示某些 Floating Point，詳細可以看原始碼 howjavascriptworks，書中這三節都是拆解程式碼講解，但是排版看起來有點麻煩，不如螢幕拖拉看來得方便。\n這邊 Douglas 拋出一個討論，他覺得大數運算不應該放進程式語言的原生支援，而是讓需要的用戶自行去採用 Library，JS 生態圈已經有良好現行的解法了，因為程式語言應該維持小而美，提供最核心的支援，其餘的應該是開放讓用戶自行選擇\n但 BigInt 已經在 Firefox/Chrome 試行了 XD 我自己也是偏向 Douglas 的想法，為了少部分用戶，而讓 JS 從原本只有 Number 多加一個數字型別，覺得沒有這麼強烈的必要\nHow Booleans Work JS 支援 Boolean 型別，也就是 typeof true / typeof false 會回傳 boolean，但在條件表達式(if/for/\u0026amp;\u0026amp;/||等)中，JS 支援 boolish，也就是將其他型別轉換成 boolean 型別，尤其是一些彆扭的 falsy 值，例如 0 / \u0026quot;\u0026quot; / undefined / null / NaN等\n良好的習慣是在條件表達式中只要 Boolean，而且判斷式用 === 而非 ==\nHow Array/Object Work Array 是用來表達一個連續的記憶體區間，並以某 size 切分成若干等份的集合，Array 在 JS 中只是一個特殊的 Object，所以 typeof array === 'object'，要確實判斷是否為陣列請用 Array.isArray\n原生的 Array 提供很多方法，但要注意 sort/reverse 是 in place 發生，也就是他是改變原Array，理論上這兩個應該要是 pure function 才是，但可惜 ECMAScript 並沒有規範\n關於 Object 可以利用 Object Literal 方式創建，Key 要採用 String，Value 可以其他型別包含 function等\n1 2 3 let my_obj = { ... } 另外 Douglas 不建議在 Object 中儲存 undefined，因為這樣無法區分到底是 Object 不存在這個 Key 還是 Key存在但儲存 undefined\n因為 JS 沒有繼承的概念，或是說透過 Prototype Chain 模擬繼承的方式，在使用上 Douglas 建議用 let new_object = Object.create(null)，因為 null 沒有 prototype，所以創建的新物件沒有多餘的 prototype 需要考量，整體上比較乾淨，運行上也比較有效率； 最好是再加上 Object.freeze，讓其他操作者無法任意更動 Object 屬性\nWeakmap 有點類似於 Object，但好處是可以用拿 Object reference 當作 Key，如果有人要操作屬性就必須有 key 跟 weakmap，如\n1 2 3 4 5 6 7 8 9 10 11 12 13 function sealer_factory(){ const weakmap = new WeakMap(); return { sealer(object){ const box = Object.freeze(Object.create(null)); weakmap.set(box, object); return box; }, unsealer(box){ return weakmap.get(box); } } } 可以將物件封裝載 weakmap 中，並回傳 Key Object reference box，只有拿 weakmap 跟 box 才能拿到當初封裝的東西。\nHow Strings Work String 在 JS 中是以 Immutable 的連續 16 bit 陣列，可以透過 String.fromCharCode 生成 String，數值範圍從 0 ~ 65535； 可以用 Array 的方法去操作 String，例如 my_string.indexOf(), my_string[0] 等\n但世界語言何其多種，需要表示的字遠遠不止 65536 種，Unicode 編碼模式也因此而誕生，目前總共規範了U+0000到U+10FFFF(20bits)，共有1,112,064個碼位（code point），碼位代表儲存的位元表示方式，其一對一表示一個人類閱讀的字元； 接著以 65,536 為單位，將 1,112,064 切割成 16個平面，每個平面含 65536 個碼位； 其中第 0 碼位稱為 BMP，也就是基礎多語言平面，其他稱為輔助平面。\nUTF-16 代表用 16bits 當作儲存的最小單位，為了要支援全部的 Unicode 語言平面，UTF-16改採用 2個單位來儲存一個碼位，將 20bits 切割上下兩個 10bits，高位元加上 0xD8，低位元加上 0xDC 組合\n1 2 3 4 5 6 7 8 U+1F4A9 =\u0026gt; 0001 1111 0100 1010 0111 ----- 前10位元： 0001 1111 01 後10位元： 00 1010 0111 ----- 補足 16 bits 並加上對應的預設值 0001 1111 01 =\u0026gt; 0000 0000 0111 1101 + 0xD8 = 0xD83D 00 1010 0111 =\u0026gt; 0000 0000 1010 0111 + 0xDC = 0xDCA9 所以 U+1F4A9 在 UTF-16 儲存方式為 0xD83D 0xDCA9 UTF-16 只是 Unicode 實際儲存方式的一種實作，包含常見的 UTF-8 也是指實作方式\n在 JS 中，要表達 Unicode 可以用\n1 2 3 4 5 \u0026#34;\\uD83D\\uDCA9\u0026#34; === \u0026#34;\\u{1F4A9}\u0026#34; //或是 String.formCharCode(55357, 56489) === String.fromCharCode(128169) How Generators Work Douglas 認為 Generator 是個好東西，但是 JS 實作的太糟了，他認為原本的 JS 就可以用 Closure 實作 Generator，使用上用原本的 Function 表達可以更清楚； 而 JS 後來導入的 Generator 在命名上、用戶操作上都不慎理想，可以從言語中看出的他不屑與憤怒 XD\n這是我少數完全認同的章節，直接看程式碼比較快 以下是 JS 標準的 Generator\n1 2 3 4 5 6 7 8 9 10 11 12 function* counter(){ let count = 0; while(true){ count += 1; yield count; } } const gen = counter(); gen.next().value; gen.next().value; gen.next().value; 以下是 Douglas 認為不用 standard 編寫的 Generator\n1 2 3 4 5 6 7 8 9 10 11 12 function counter(){ let count = 0; return function counter_generator(){ count += 1; return count; } } const gen = counter(); gen() gen() gen() 從幾個地方去看出為什麼 Standard Generator 是糟糕的語言設計\n彆扭的命名 在 JS 中，function 是 first class citizen，大家也都非常熟悉於 High order function，將 function 當作參數或回傳值使用； 今天 Standard Generator 是在 function 後加入 * 標記，但其行為又跟 function 不同，例如 function 是用 return 回傳值，但是 Generator 是用 yeild 偏向 OOP 設計而非 FP 當然要走 FP 或 OOP 比較像是個人偏好，但依據 Standard Generator，使用上必須宣告 while(true) 並搭配 yeild 實作，Douglas 認為應該要盡量減少 Loop 的使用 回傳值的操作 用 Standard Generator 初始化後，要呼叫下一個值必須用 gen.next().value 回到類似第一點的設計錯誤，一個 Generator Function 沒有顯示 return 的值，居然是以一個物件的形式，呼叫其 next function，而且還要加上 .value，這表達的方式實在是很怪異，遠遠不如 gen() 來得直接明瞭。 Standard Generator 被批評的很徹底，但確實 Douglas 講得也很有道理，如果有人看過資料或有不同的意見，歡迎交流～ 我也很好奇其他人或是當初語言標準制定時是如何評量的\n接著 Douglas 分享他一套 Generator 的組合技，還蠻厲害的，有興趣可以去書店翻閱，這部分程式碼沒有被放到 Github 上。\nHow Exceptions Work Exception 比較偏向指意外，但是在一般的系統設計常常會把錯誤( Error )與意外混為一談，例如說查詢檔案時如果檔案不存在，這應該是可以被預期的錯誤，應該要一般的流程處理，但我們還是會用 try catch 來傳遞這樣可被預期的錯誤\n在 JS 中，錯誤拋出使用 throw，throw 可以搭配各種型別的值，使用上最簡單就用 String，有Error Object 但沒有太強使用的必要； 在 Compile 過程中，每個 function 都有一個 catchmap，如果發生錯誤，會依序從呼叫的 function 開始找 catch 機制，如果沒有則持續往上找 caller\n使用 try catch 會有個安全風險是假使使用兩個外部的 module A/B，A 有網路存取的功能，而 B 用於內部的加解密運算，假使某天我們在調用是用 B 解密後再將值傳給 A 走網路傳輸，假使B 拋出錯誤 throw private_key 而 A 有註冊 catch 意外捕獲這個錯誤，那就會有資安疑慮的風險了\n","date":"2019-10-02T00:21:40.869Z","permalink":"https://yuanchieh.page/posts/2019/2019-10-02-how-javascript-works%E8%AE%80%E5%BE%8C%E6%95%B4%E7%90%86-%E4%B8%8A/","title":"《How Javascript Works》讀後整理 上"},{"content":"介紹 SDP 是一種標準化的資訊傳達方式，用來表達多媒體的內容、傳輸的位址及其他傳遞所需的 metadata，主要應用場景於多媒體傳輸的前置溝通，例如說視訊會議、VoIP(Voice over IP) 通話、影音串流等會話(Session) 在規範中並沒有定義 SDP 該怎麼被傳輸，可以自由選用 HTTP / XMPP / RTSP / Email 等傳輸協定等\n\u0008# 名詞定義\nConference: 兩個以上的用戶正在相互通信的集合 Session: 有一個發送者，一個接收者，兩者間建立一條多媒體串流的通道，資料由發送者寄發到接收者 Session Description： 讓其他人可以成功加入 Conference 的資訊 要求與建議 SDP 主要功用為傳遞多媒體會話中多媒體串流的資訊，溝通會話的存在 / 及讓其他非參與者知道如何加入此會話(主要用於廣播 multicast)，內容大概分成幾個\n名稱跟目的 會話有效的時間 會話中涵蓋的多媒體 要如何接收串流的資訊 (如 位址、Port、格式等) 會話需要的帶寬資訊 個人的聯絡資料 媒體與傳輸資訊 這部分包含了\n媒體形式 (影片、聲音等) 傳輸協定 (RTP/UDP/IP等) 媒體格式 (H. 264/MPEG等) 另外還會包含位址與埠口的資訊，SDP 傳輸形式包含單播(unicast)跟多播(multicast)，多播則包含多播的群組位址，單播則是單一台的位址\n時間資訊 任意數量的開始與結束時間組合 週期性表示 (如每週三早上十點一小時) 時間表示是全球統一格式，不包含時區或是日光節約時間 私人會話 SDP 本身不涉及 public 或 private session，如果需要加密或是限定，則在傳輸時自行決定\n其他 SDP 本身就應該夾帶足夠的資訊讓參與者知道是否該加入 session，但如果有其他額外資訊要夾帶，可以放在另外的 URI 中；\nSDP Spec 文字編碼上，SDP 採用 ISO 10646 字符集並用 UTF-8 編碼方式，但是在屬性/欄位上採用 UTF-8 的子集合 US-ASCII，只有在文字欄位可以使用完整的 ISO 10646 字符集\nSDP 是由這樣格式的文字組成\n\u0026lt;type\u0026gt;=\u0026lt;value\u0026gt;\ntype 必須是單一個大小寫區分的字元； value 則是相對應有結構的文字，多個值可以用空白分隔； 切記在 = 兩側不可以有空白\nSDP 中包含了 session-level 區段與多個 media-level 區段，session-level 區段以 v= 開始，其屬性套用在所有的 media 區段上，但如果 media 區段有相同屬性則會被覆蓋； 而 media-level 則是 m=開始直到下一個 media level 區段開始\n在 SDP 定義的順序很重要，主要是幫助更快的錯誤偵測與容易實作 parser\n有些欄位是必填有些是選擇性，但重點是一定要按照順序，選擇性欄位以 * 註記，屬性欄位大致介紹含義，不會完整介紹\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 Session description v= (protocol version) o= (originator and session identifier) s= (session name) i=* (session information) u=* (URI of description) e=* (email address) p=* (phone number) c=* (connection information -- not required if included in all media) b=* (zero or more bandwidth information lines) One or more time descriptions (\u0026#34;t=\u0026#34; and \u0026#34;r=\u0026#34; lines; see below) z=* (time zone adjustments) k=* (encryption key) a=* (zero or more session attribute lines) Zero or more media descriptions Time description t= (time the session is active) r=* (zero or more repeat times) Media description, if present m= (media name and transport address) i=* (media title) c=* (connection information -- optional if included at session level) b=* (zero or more bandwidth information lines) k=* (encryption key) a=* (zero or more media attribute lines a= 屬性機制主要是擴展 SDP，由各個使用 SDP 的協定去使用 有些屬性有制式的含義，有些則基於應用程式的解讀，例如\n有些定義在 Session-level 的屬性如 連線相關 c= 或是屬性相關 a= 會套用在 Session 底下所以的 Media，除非被特別指定覆寫\n如以下範例，所有的 media 都會被冠上 recvonly 的屬性\n1 2 3 4 5 6 7 8 9 10 11 12 v= 0 o=jdoe 2890844526 2890842807 IN IP4 10.47.16.5 s=SDP Seminar i=A Seminar on the session description protocol u=http://www.example.com/seminars/sdp.pdf e=j.doe@example.com (Jane Doe) c=IN IP4 224.2.17.12/127 t=2873397496 2873404696 a=recvonly m=audio 49170 RTP/AVP 0 m=video 51372 RTP/AVP 99 a=rtpmap:99 h263-1998/900000 文字欄位如 session 名稱和資訊等可以是包含任意位元組的字串除了以下 0x00 Nul 、 0x0a (new line) 、 0x0d (carriage return) ； 字串以 CRLF(0x0d0a) 當作斷行，但 parser 也應該將 newline 視為斷行的標誌； 如果沒有 a=charset 屬性指定字符集，則預設為 ISO-10646 字符集搭配 UTF-8 編碼方式\n如果是包含 domain name，則必須確保符合 ASCII Compatible Encoding (ACE)，也就是要經過編碼跳脫字元，因為有些 SDP 相關協定定義早於國際化 domain name，所以不能直接用 UTF-8 表示\n欄位介紹\n協定版本 (v=) 沒有其他版本號，就是 v=0\n來源 (o=) 描述Session 的發起者資料\n1 o=\u0026lt;username\u0026gt; \u0026lt;sess-id\u0026gt; \u0026lt;sess-version\u0026gt; \u0026lt;nettype\u0026gt; \u0026lt;addrtype\u0026gt;\u0026lt;unicast-address\u0026gt; sess-id：數字組成的字串，由 四者組成的字串必須是全域唯一的 (globally unique)，sess-id 的產生可自行定義，但建議採用 NTP 格式的 timestamp 保證唯一性 sess-version：此 Session 描述的版號，在改變 SDP 內容時確保版號是遞增的，同樣建議用 NTP 格式 timestamp nettype：採用網路類型 addrtype：指定 address 的類型，例如 IP4 / IP6 等 unicast-address： 創建 session 機器的位址，可以是 domain name 或是 IP 表示法，不可使用 local ip 因為不確定對方是否在 local 範圍內 如果有隱私問題， username / unicast-address 可以被混淆，只要不影響全域唯一性\nSession 名稱 (s=) 文字欄位，表示 Session 名稱，每份 Session description 中只能有一個，不能為空值且必須採用 ISO-10646 字元 (除非有另外指定字符集)，如果沒有要特別指定 Session 名稱，可以用空白代替如 (s= )\nSession 資訊 (i=) 文字欄位，每份 session description 中每個 session 最多只能有一個，每個 media 最多也只能宣告一個； 這資訊主要是寫給人類閱讀的，用來表示 session 或是 media stream 的用處\nURI (u=) 選擇性欄位，用來表示關於Session額外資訊的位址連結，最多只能有一個，必須放置在 media 欄位之前\nEmail 及電話號碼 (e= , p=) 聯絡人的 email 跟電話號碼\n連線資訊 (“c=“) c=\u0026lt;nettype\u0026gt; \u0026lt;addrtype\u0026gt; \u0026lt;connection-address\u0026gt; 一份 session description 必須包含一個或是 media description 至少一個\nnettype： 採用網路類型，IN 表示 internet，但未來可能有其他的支援 addrtype： 位址類型，可以是非 IP 家族的 connection address： 依據位址類型，顯示不同的格式 如果是應用在多播的場景下，IPv4 需要在網址後加上 TTL，而 IPv6 沒有 TTL 的概念 在階層式的編碼下，資料串流可能被依照不同頻寬拆分成不同的來源，可以在位址加上來源數量，IP 位置會以連續的方式呈現 例如說\n1 2 3 4 5 6 c=IN IP4 224.2.1.1/127/3 這等同於在 media description 中如此表示 c=IN IP4 224.2.1.1/127 c=IN IP4 224.2.1.2/127 c=IN IP4 224.2.1.3/127 頻寬 (b=) 顯示預計使用的頻寬，根據不同的 bwtype 有不同含意 b=\u0026lt;bwtype\u0026gt;:\u0026lt;bandwidth\u0026gt;\nCT：conference total 全部 Conference 帶寬上限，可能一個 Conference 包含多個 session，則建議所有 session 使用的帶寬加總合 AS：application specifivc Timing (t=) t=\u0026lt;start-time\u0026gt; \u0026lt;stop-time\u0026gt; 表示 Session 的開始結束時間 如果沒有指定 stop-time，則 Session 為 unbounded，表示在 start-time 之後 Session 一直保持活躍 如果連 start-time 也沒指定，則表示 Session 是永久存在\n建議不要採用 unbounded Session，因為 client 不知道 Session 何時結束，也不知道如何排程\n重複次數 (r=) r=\u0026lt;repeat interval\u0026gt; \u0026lt;active duration\u0026gt; \u0026lt;offsets from start-time\u0026gt; 這會搭配 t 做使用，例如說一個節目是每次播放一小時，於週一 10am 開播，接著每週二 11am 每週播放持續三個月，則表示法為\n1 2 3 4 t=3034423619 3042462419 r=604800 3600 0 90000 // 或這樣表示 r=7d 1h 0 25h 3034423619 是開始時間，也就是某週一 10am 3042462419 是結束時間，也就是開播三個月後的週二 11am\n7d 是播放間隔，所以是 7天的秒數 1h 是播放的時長 0 25h 是距離 start time 的時間間隔，也就是(週二 11am - 週一 10am)\n如果是以月或年重複的播放，則不能使用 r 表示，需要改用多個的 t 表示播放時間\n時區 (z=) 這欄位會影響 t 跟 r z=\u0026lt;adjustment time\u0026gt; \u0026lt;offset\u0026gt; \u0026lt;adjustment time\u0026gt; \u0026lt;offset\u0026gt; .... 如果是一個重複播放的 Session，可能會遇到日光節約日，所以要主動減去一小時，又因為不同的國家與地區對於日光節約日的計算不同，所以保留表示的彈性，如 z=2882844526 -1h 2898848070 0 在重複播放時間是 2882844526 要減去一小時，但是在 2898848070 就恢復正常\n加密金鑰 (k=) 如果 SDP 是在已安全以及可被信任的方式傳遞下，可以考慮傳遞加密的金鑰(加密 media stream 而非 session description 本身)，這個欄位不傳達加密的演算法、金鑰類型等，這些全留給採用 SDP的協定去規範\n目前支援以下幾種定義\nk=clear:\u0026lt;encryption key\u0026gt;：key 沒有改變過 k=base64:\u0026lt;encoded encryption key\u0026gt;：用 base64 將 key 編碼 k=uri:\u0026lt;URI to obtain key\u0026gt;：指名去 URI 拿 key，通常 URI 會走安全通道，例如 HTTPS 等 k=prompt：雖然 Session 有加密但是 session description 沒有提供 key，用戶要額外去索取 再次強調 SDP 本身要是在安全的情況下才能加 k 欄位\n屬性 (a=) 1 2 a=\u0026lt;attribute\u0026gt; a=\u0026lt;attribute\u0026gt;:\u0026lt;value\u0026gt; 屬性值主要用來擴展 SDP，可以用在 session level 補充對 conference 的資訊，也可放在 media level 傳遞 media stream 的資訊，在 SDP 會有若干的屬性值宣告\n屬性值有兩種宣告方式\nFlag概念 (a=)，例如 a=recvonly 鍵值 (a=:)，如 a=orient:landscape 至於如何處理與定義屬性值，有些屬性有定義的含義，其餘則應用程式可以彈性處理\nmedia description (m=) 1 m=\u0026lt;media\u0026gt; \u0026lt;port\u0026gt; \u0026lt;proto\u0026gt; \u0026lt;fmt\u0026gt; ... 一份 session description 中可能含有多個 media description，每一份 media description 從 m= 開始直到下一個 m=或是 session description 結束\nmedia 類別，可以是 audio / video / text 等 port 串流從哪個 port 發送，這會根據 connection infomation (c=) 而決定，對於某些傳輸協定，如 rtp 會使用兩個 port (RTP+RTCP)，所以宣告時會是 m=video 49170/2 RTP/AVP 31，表示 RTP 使用 49170 / RTCP 使用 49171； 如果 c 指定多個 IP 位置，則 port 成一對一映射關係，如 1 2 c=IN IP4 224.2.1.1/127/2 m=video 49170/2 RTP/AVP 31 則因為 RTP 一次使用兩個 port，這表示 224.2.1.1 使用 49170、49171，224.2.1.2 使用 49172、49173 proto 指定傳輸協定，常用的有 udp 、 RTP/AVP、RTP/SAVP 等 fmt 媒體格式，會跟著 的定義，如果 是 udp 則 需要指定 audio / video / text / application / message 一者 SDP Attributes a=cat: 用逗點分隔，用來過濾 category a=keywds: 類似於 cat 屬性，透過關鍵字篩選想要的 session a=tool: 表示用來創建 session 工具的名稱與版號 a=ptime: 用 ms 表示一個封包中媒體的總時長 a=maxptime: 用 ms 表示一個封包中媒體的總時長上限 a=rtpmap: / [/] 搭配 media type(m=)宣告，補充RTP所採用的編碼方式，雖然 RTP 檔案本身就會包含 payload 格式，但是常見做法是透過參數設定動態改變 例如說 u-law PCM coded single-channel audio sampled at 8 kHz，他的 encode 方式固定只有一種，所以不需要另外宣告rtpmap 1 2 3 4 5 m=audio 49232 RTP/AVP 0 ```bash 但如果是 `16-bit linear encoded stereo audio sampled at 16 kHz`，希望用 RTP/AVP payload 格式 98 的話，就必須另外宣告解碼方式 m=audio 49232 RTP/AVP 98 a=rtpmap:98 L16/16000/2\n1 2 3 4 5 6 rtpmap 可以針對 payload 格式做映射，如 ```md m=audio 49230 RTP/AVP 96 97 98 a=rtpmap:96 L8/8000 a=rtpmap:97 L16/8000 a=rtpmap:98 L16/11025/2 參數對應的參數可以參考 [RTP Profile for Audio and Video Conferences with Minimal Control](https://tools.ietf.org/html/rfc3551) a=recvonly 表明單純接收 a=sendrecv 可以接收與發送，此為預設值 a=sendonly 單純發送 a=inactive 不接收也不發送媒體，基於 RTP 系統的即使是 inactive 也要持續發送 RTCP a=orient: 用於白板或是介紹的工具，可以指定 portrait / landscape / seascape(上下顛倒的 landscape) a=framerate: video frame rate 最大值，只有 medial level 的 video 類型需要 a=sdplang: SDP 資訊採用的語言，如果有多種語言建議每種語言都拆成獨立的 session description a=fmtp: 用來傳達某些特定格式，且這些特定格式不需要是 SDP 所能理解的 安全考量 SDP 常用於 offer/answer 模型的 SIP 中，用來溝通單播的會話機制，當採用這樣的模式時，要記得考量協定本身的安全性\nSDP 只是用來描述多媒體會話的內容，接收方要注意 session description 是否通過可信任的管道與來自可信任的來源，否則網路傳輸過程可能遭遇攻擊，必須要自己承擔安全上的風險\n常用的傳輸方式是 SAP，SAP 本身提供加密與驗證機制； 不過有些情況下無法採用，例如接收者事前不知道發送者的時候，此時就要特別小心 parse，並注意權限的管控(僅開放有限的軟體可以操作)\n","date":"2019-10-02T00:21:40.869Z","permalink":"https://yuanchieh.page/posts/2019/2019-10-02-sdp-spec-%E9%96%B1%E8%AE%80%E7%AD%86%E8%A8%98/","title":"SDP Spec 閱讀筆記"},{"content":"後端工程師最基本的技能要求是設計符合 HTTP-based REST 的API，工作兩年多快三年，自己腦中第一反應大概會是\n把URL視為資源路徑的描述，把對應的CRUD 操作對應至 HTTP Method，例如要下一筆訂單是 POST /booking ，取得單筆訂單是 GET /booking/1\n但世界沒有這麼單純，如果是遇到複雜的操作時，難以運用 HTTP 的 Method動詞 + URL 名詞的方式描述，那該怎麼辦呢？\n例如說最近在工作上遇到如何設計一次刪除多筆資料的API該怎麼辦？\n尋找答案的過程中，看到 Google 與 Microsoft 有公開他們的 API Design Guideline，並分享其中思考的眉角，其中包含了\nREST 的基礎觀念 應付複雜場景的考量 錯誤碼的處理 參數名稱與版本控制 API Design Guide | Cloud APIs | Google Cloud\nmicrosoft/api-guidelines\n以 Google 文件為主，Microsoft 文件為輔，整理過後分享個人筆記\n一點 REST 介紹 Roy Fielding 在西元 2000年提出了 Representational State Transfer(REST ) 網站架構設計規範，同時他也是 URL / Http 1.0 / Http 1.1 標準制定的參與者，所以 REST 的概念很自然的與 HTTP 相當吻合，一開始 REST被誤以為是 HTTP object model一種 HTTP的實作，但實際上 REST描繪的是一個正確架構的 Web 應用程式：用戶選擇連結(state transition)，進而得到下一個結果(representing the next state of the application) ，其中提出了 REST 架構需要符合以下六大原則\nClient / Server 依據關注點分離( Separation of Concern)，將用戶介面跟資料儲存切割，方便兩者獨立運作與維護\nStateless 每一次的連線本身都攜帶足夠的資訊，而不用依賴於上一次連線的狀態，增加服務的可靠性與擴展性\nCache 根據 Request，Response 可以決定是否能被緩存，增加使用效率\nUniform Interfaces 如同程式設計，元件間也需要制定介面(interface)解耦合與溝通，雖然會降低一些效率，不過增加元件獨立的運作與維護，其中包含四個規範\nidentification of resources：定位到特定資源上，資源可以是圖片、文字、特定服務(如今天加州天氣)、一個實體的人等等 manipulation of resources through representations：Server 提供可以操作資源的方法，包含了描述資源本身的meta-data，以及如何操作 data的 Control data self-descriptive messages：訊息本身資訊量是足夠的，在跨元件之間可以不斷的被傳遞而不需要有額外的處理(Stateless) hypermedia as the engine of application state(HATEOAS)：\n模擬瀏覽網頁，加載完首頁後，後續操作都是利用網頁上的超連結探索網站；\n套用同樣的邏輯至 REST Server，Response 包含針對此資源探索的連結與操作方式，如跟醫師預約後，回傳結果，同時包含查詢預約、查詢醫生資料、更改預約等操作都一併回傳 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \u0026lt;appointment\u0026gt; \u0026lt;slot id = \u0026#34;1234\u0026#34; doctor = \u0026#34;mjones\u0026#34; start = \u0026#34;1400\u0026#34; end = \u0026#34;1450\u0026#34;/\u0026gt; \u0026lt;patient id = \u0026#34;jsmith\u0026#34;/\u0026gt; \u0026lt;link rel = \u0026#34;/linkrels/appointment/cancel\u0026#34; uri = \u0026#34;/slots/1234/appointment\u0026#34;/\u0026gt; \u0026lt;link rel = \u0026#34;/linkrels/appointment/addTest\u0026#34; uri = \u0026#34;/slots/1234/appointment/tests\u0026#34;/\u0026gt; \u0026lt;link rel = \u0026#34;self\u0026#34; uri = \u0026#34;/slots/1234/appointment\u0026#34;/\u0026gt; \u0026lt;link rel = \u0026#34;/linkrels/appointment/changeTime\u0026#34; uri = \u0026#34;/doctors/mjones/slots?date=20100104\u0026amp;status=open\u0026#34;/\u0026gt; \u0026lt;link rel = \u0026#34;/linkrels/appointment/updateContactInfo\u0026#34; uri = \u0026#34;/patients/jsmith/contactInfo\u0026#34;/\u0026gt; \u0026lt;link rel = \u0026#34;/linkrels/help\u0026#34; uri = \u0026#34;/help/appointment\u0026#34;/\u0026gt; \u0026lt;/appointment\u0026gt; Layered System 一個完整的系統可以由多個子元件疊加，例如 Cache Server / Api Server / Proxy / Agent 等，每個元件僅可意識到相鄰的元件\nCode On Demand Client 可以依照需求加載新的 script 執行\n這代表 REST 不一定要綁定 HTTP，只是 HTTP很巧妙的跟 REST概念十分貼近，REST的概念也可以對照到 HTTP 實作上(畢竟 Roy Fielding都有參與其中\n回過頭來看常見的 RESTful API定義，比較像是 REST + HTTP 的混合產物，快速翻完 Roy Fielding 的 Architectural Styles and the Design of Network-based Software Architectures，其中的 6.2 URI 定義是特定資源的表徵 (representation of the identified resource)，最一開始的 Web 是在傳輸文件或超連結(hypertext)，所以 Resource 常直接對應到檔案文件，而 URI 則對應到實際檔案的路徑；這會帶來幾個不好的影響，例如說文件被修改了該如何表示 / 如何表達服務( Service )而非文件本身 等等；\n良好的 Resource 定義應該是**盡可能固定不變** 且 抽象於實際檔案儲存本身，而是一種高層級的映射概念，當 Server 收到後去找出對應的實作內容，更重要的是傳達使用者的意圖，所以一個 Resource 概念可能橫跨多的檔案，也可以多個 Resource 描述同一個檔案\n用 URI描述資源後，需要再加上用戶對於資源的操作(representation )，就可以組成語意 (Semantic)，對應回 HTTP，不同的操作對應不同的方法( Method )，如 GET 表示要取得 URI所代表資源的資訊 / POST 表示創建 URI資源的子資源等，則定義在 HTTP 1.1 當中的 Method Definitions\n以個人淺見，談到 REST Architecture 指的應該是符合 Roy Fielding 規範的 6大原則的網站架構設計；\n而目前常用的 RESTful API 設計原則則是 REST中的 Resource 解釋加上 HTTP 對於 Method 操作的補充，兩者混合的設計原則\n回歸 RESTful API / REST API 根據 Google 文件，RESTful API 定義主要是 可個別呼叫的「資源」(API 的「名詞」)「集合」做為模型。不同的資源有各自的參照名稱，也就是所謂的**[**資源名稱**](https://cloud.google.com/apis/design/resource_names?hl=zh-tw)**，並且是透過一套「方法」來操控\n這樣的理念可以被很好的套用在 HTTP 1.1 上，因為 URL 可以用來描述資源路徑，HTTP Method 常用的也就是 GET / POST / PUT / PATCH / DELETE，正好對應資源的操作，根據 Google 文件，有 74%的公開 HTTP API 都是依據 REST 設計規範；\nHTTP被廣泛應用，但也不是唯一的跨進程溝通的規範，如果是公司內網或是要求更低溝通上的overhead，會採用 RPC ( Google推廣自家的 gRPC)，REST 也可以被套用在這上面\n資源 Resource 面向資源導向設計的 API，需要先規劃資源的層級，每個資源的節點可以是單個資源(Resource)或是同一項資源的集合(Collection)\n每個資源或集合必須有獨特的 id 去區分，在命名時盡可能表達清楚，避免使用以下的名詞如 resource / object / item 等；\n且命名時已複數名詞表示，例如 /users/123/events/456\n例如 Gmail API 會有一群用戶的集合，每個用戶底下有訊息的集合 / 標籤的集合等等\n-- user\n|\u0026mdash; message\n|\u0026mdash; label\nURL 的長度在Spec 中沒有規範，但在現實中有些瀏覽器會有長度限制，可以的話還是保持在 2000 字元以內比較保險\nWhat is the maximum length of a URL in different browsers?\nMicrosoft 文件也是標榜類似的寫法，但額外建議不要讓資料的層級超過三層 /collection/item/collection ，例如說 /customers/1/orders/99/products 可以分拆成 /customers/1/orders + /orders/99/products\n另外在 API的資源層級不要底層的資料結構有太緊密的關係，例如使用關聯性資料庫不一定要剛好對準 Table，API 應該是要更高層度的抽象化，避免之後資料庫的更動耦合了 API的資源路徑。\n操作 Methods 面向資源導向設計的 API 側重於資源的描述而非操作的描述，所以大量資源的描述僅會搭配有限的操作，標準的操作是 List / Get / Create / Update / Delete 這五項\n操作有可能不會是立即發生，需要一段時間才會生效，此時可以回一個長時操作的資源，用以查詢進度或是狀態等的方式，有點像是叫號取餐的遙控器\nList 必須使用 GET Request 不能有 Body Response Body 包含以陣列表示的資源，與其餘選擇性的操作(如分頁操作) GET 必須使用 GET Request 不能有 Body Response Body 對應到完整的資源描述 CREATE 必須使用 POST Request Body 必須包含新增資源的內容 如果支援 client side 指定 _id，則提供該欄位於 Request Body，但如果發生衝突需回傳 ALREADY_EXISTS Response Body 可以 Update 如果是部份更新使用 PATCH 如果是全部更新(覆蓋)使用 PUT，如果 Request Body 沒有夾帶的欄位，視為清除該欄位 如果 Patch更新不存在資源，API 可以選擇是否支援 Upsert更新不到就創建的功能，如果否則回傳 NOT_FOUND Response Body 必須是更新後的資源本身 Update 僅用於更新，如果是其餘複雜操作如重新命名資源、改變資源路徑等，請使用客製化操作 如果以 JSON 為資料交換格式，Patch 有兩種方法 JSON patch / JSON merge patch，在資料儲存中，null 的含義有些模糊地帶，如果 Request JSON 欄位夾帶 null，這是代表移除該欄位還是更新欄位成為 null 呢？\n如果 Header 中的 Content-Type 是 application/merge-patch+json 則代表 null 移除該欄位，此時需注意資料結構就不建議欄位儲存 null避免混淆，更多參考 RFC: JSON Merge Patch；\n如果希望針對欄位有更精確的操作描述，例如新增、刪除、取代、複製、搬移、驗證(test)等，可以參考 JSON patch，Content-Type 為 application/json-patch+json ，用陣列表述操作的集合，操作範例如下\n1 2 3 4 5 6 7 8 [ { \u0026#34;op\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/a/b/c\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;foo\u0026#34; }, { \u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/a/b/c\u0026#34; }, { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/a/b/c\u0026#34;, \u0026#34;value\u0026#34;: [ \u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34; ] }, { \u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/a/b/c\u0026#34;, \u0026#34;value\u0026#34;: 42 }, { \u0026#34;op\u0026#34;: \u0026#34;move\u0026#34;, \u0026#34;from\u0026#34;: \u0026#34;/a/b/c\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/a/b/d\u0026#34; }, { \u0026#34;op\u0026#34;: \u0026#34;copy\u0026#34;, \u0026#34;from\u0026#34;: \u0026#34;/a/b/d\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/a/b/e\u0026#34; } ] DELETE 必須使用 DELETE 不能有 Request Body 如果是立即刪除，則 Response Body 為空值 如果是需要長時間執行，回傳相對應的長時操作 如果只是把資源標記刪除而非硬刪除，回傳更新的資源 刪除必須是冪等性操作，不論操作幾次都必是是刪除該資源，但回應內容可以改變，第一次確實刪掉資源回復成功，後續刪除可以回覆 NOT_FOUND Custom Method 除了上述五種操作外，可能會有些操作不再這五種範疇之中，此時就可以自行定義，例如說取消刪除、大量更新等\n自定義的方法須以 : 放在 URL的最後方，且常見的搭配是用 POST，因為可以夾帶Body；\n但也可以視情況使用其他的 HTTP Method (Patch 不建議使用外)，但仍須遵守 HTTP Method 使用的規範，例如冪等性與是否能夾帶 Body等\nGoogle 提供幾種自定義方法的用途\n1. POST :cancel 取消操作\n2. GET :batchGet 一次性取得多筆資料\n3. POST :move 將資源移到別處\nbatchGet 也可以使用 POST，例如 POST https://mybusiness.googleapis.com/v4/{name=accounts/*}/locations:batchGet` 在 Body 中有對查詢的內容有更近一步的描述\nMicrosoft 提議就把非名詞放進 URL中，例如一個計算機的加法 API可以設計為 GET /add?operand1=99\u0026amp;operand2=1 ，其餘的就遵守 HTTP Method 操作方式。\n在 AWS 文件中，Custom Method 是以 Query String 方式存在，如 刪除多筆物件/?delete ，個人是覺得容易與其他的 Query String 混淆，在 Parsing上也比較麻煩點，不是很喜歡這樣的作法\n個人偏好 Google的做法，讓 URL組成全部都是名詞，乾淨的表示資料層級，Custom Method 就以一個特殊的方式宣告，可以良好與 REST API共存\nHTTP Media Type 與 Header 最後別忘了 Request 與 Response 應盡量遵守 HTTP規範，使用正確的 Status Code 與 Header，讓 API設計更精確，例如\n交換的資料格式為 json 就要宣告 Content-Type: application/json;charset=utf-8 200\n201(Created) : 建立新資源\n202(Accepted)：接受請求，但不是立即發生\n204 (No Content)：沒有要回傳資料 Versioning 在維護 API過程，會遇到更動資料結構或是修改邏輯的地方，如果可以的話又不希望影響原有 API的操作，此時就需要做版本控制，版號的命名可以參考 Semantic Version X.Y.Z 方式表示\nX 大版號表示有 breaking change，向前不相容 Y 小版號表示有新增 function，或是更動是向前相容的 Z 補丁版號表示 Bug 修正 一般來說 API 對外用大版號表示，如 v1 / v2，小版號跟補丁版號出現在文件的版本號上\nversion 常見可以放在幾個地放\nURL 當中，放在 Domain 後的第一層，如 https://example.com/v1 夾帶在 Query String中，如 https://example.com?version=v1 放在 Custom Header當中 放在 Header Accept 中，如 application/vnd.adventure-works.v1+json 在採用 versioning 時，要考量到 web cache 的機制，通常 cache 是針對 URL，所以前兩者比較推薦\nError 常見作法會回傳錯誤代碼、錯誤的簡述、錯誤的詳細內容，方便開發者做錯誤處理，例如以下格式\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \u0026#34;error\u0026#34;: { \u0026#34;code\u0026#34;: \u0026#34;BadArgument\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Multiple errors in ContactInfo data\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;ContactInfo\u0026#34;, \u0026#34;details\u0026#34;: [ { \u0026#34;code\u0026#34;: \u0026#34;NullValue\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;PhoneNumber\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Phone number must not be null\u0026#34; }, .... ] } } Code 常見還可以用數字表示，方便開發者透過文件快速查詢，但也不要忘了回傳該次錯誤的內容描述\n別忘了要搭配正確的 HTTP 錯誤碼使用\n400(Bad Request)：Server無法理解，例如參數缺少或錯誤\n401(Unauthorized)：授權錯誤，可能是 Token失效等\n403(Forbidden)：無訪問權限，想取得超過用戶授權的資源\n404(Not Found)：查無資源\n…..\n","date":"2019-09-18T00:08:40.869Z","permalink":"https://yuanchieh.page/posts/2019/2019-09-18-%E5%A6%82%E4%BD%95%E8%A8%AD%E8%A8%88-rest-api/","title":"如何設計 REST API"},{"content":"文字版：https://dev.to/pmlopes/javascript-on-graalvm-120f\n身為一名工程師，想辦法讓自己的程式碼執行的更快，似乎是一種天性，當我們想要優化 Node.js Server 的執行速度時，第一印象應該都是如何寫出更好的程式碼、用更快的演算法，這些當然都是優化的一環，但如果拉遠一點想，有沒有辦法去優化 Node.js Runtime本身呢 ?!\n作者提到在著名的 framework benchmark 中，JS 的執行效率排名很後面，如果用 FlameChart 分析，會發現絕大多數的時間是在底層也就是 V8 Engine 與 Libuv 上\n左圖：https://blog.zenika.com/2011/04/10/nodejs/ 右圖：截自影片\n所以我們再怎麼優化 JS code，就如同冰山一角般，是很難取得非常大幅度的效能提升，那該怎麼辦呢？\nPaulo Lopes 設計了ES4X 專案，主要是用 GraalVM 取代 V8 與 Vert.x 取代 Libuv ，基本上就是將底層從 C/C++ 工具鍊換成了 JVM 為主的工具鍊，根據作者的 benchmark，在不同應用面可提升 120% ~ 700% 的效能\nRun your JavaScript server 700% faster!\nVert.x Vertx 類似於 Libuv 的功能，提供 Event-Driven 與 Non-blocking 的框架，提供如 TCP / File 操作 / DNS / HTTP / Timer等 API支援，其核心概念是透過 Event Loop 方式達到事件驅動的開發方式，大概瞄過去跟 Javascript 的開發理念非常吻合，像是\nCallback Function 為每個事件註冊 Handler，等到事件完成後推進 Event Loop 等待執行， Don't call us, we will call you.\nNever block Event Loop 這點雷同於 Node.js，但是有一點不同是 Node.js 採用 Single Event Loop，但是Vertx 採用 Multiple Event Loop，主要是更有效運用 multi-core 的機器；\n雖然採用了 Multiple Event Loop，但是為了避免 Context Switch 與增進性能，如果 handler A 在 Event Loop1 執行，後續的 Callback 也都位在 Event Loop1 執行\nHow to run blocking code 現實上不可避免還是會需要執行耗時的運算或是運行用 sync 設計的函式庫，這時可以用 executeBlocking，類似於 Promise的設計，但會運作到從 Worker Thread Pool 取得的 Thread 上，避免卡住 Event Loop\n但文件描述到 executeBlocking 雖然是用於 blocking code execution，但建議超過 10s 的操作還是另外管理 Thread\n1 2 3 4 5 6 7 vertx.executeBlocking(function (promise) { // Call some blocking API that takes a significant amount of time to return_ var result = someAPI.blockingMethod(\u0026#34;hello\u0026#34;); promise.complete(result); }, function (res, res_err) { console.log(\u0026#34;The result is: \u0026#34; + res); }); Concurrent composition 非同步操作一大要解決的問題就是操作順序，Vertx 提供像 CompositeFuture.all、CompositeFuture.any 等 API，從 JS 角度應該都不陌生\n直接使用 Vertx 的 JS 方案有一個很大的問題\n目前是基於 Nashorn 這套 JS Engine，僅支援到 ES 5.1並不支援 ES6 以後的語法，如果 Library 用 ES6 以後語法寫也都不支援\n這算是致命傷了吧，如果不能整合現在的 JS 生態系那應該很少人敢採用這套方案。\n所以 ES4X 是一個全新基於 Vertx 開發的 JS Runtime，加入了 ES6+ 的支援更好與現在 JS生態系整合，所以 Promise / async-await 等等都可以採用，同時 Vertx 的語法也同時支援，作者提到下一版的 Vertx 會棄用現有的 Vertx JS 改用 ES4X。\n題外話：\n作者人很好，在文章底下提問很多疑惑都在 24 hr 內得到解答，真的是很感謝\nGraalVM GraalVm 是一個提供多語言的運行環境，透過 Graal Compiler 編譯成 Java Bytecode 並執行於 Java HotSpot VM上，所以可以兼容於 JVM-based 的語言如 Java/Scala/Kotlin等；\n透過 Truffer framework，可以兼容其他的程式語言，如JS/Python/R/Ruby等等\n許多程式語言都必須要有一個良好的運行環境 Runtime，但是要打造一個安全、高效的運行環境是非常困難且耗時的，例如說 JS的 V8 Engine 也是 Google多年的投入才有這麼好的成果，其他語言相較之下沒有這麼多資源，自然運行的速度就很慢；\n所以 GraalVM希望透過通用化虛擬化技術，讓不同的程式語言只要用 Java 透過Truffle Framework 實作該語言的 AST，後續的運行就交給 GraalVM，降低新語言開發的困難\n除了讓原本的程式語言執行得更快，採用 GraalVM另一大好處是可以混合語言(Polyglot)開發，例如 JS內使用 R的套件，讓不同的語言發揮各自的長處，而且效能不受到跨語言的影響，因為不同的語言最後都是通過 Truffle Framework 生成 Graal Compiler 了解的AST，最後 Compile 出來的機器碼不會因為不同語言而有所差異\n更多的細節可以參考這支影片\n實作 — 用網頁顯示圖表 /採用 GraalVM 與 ES4X 簡單整合兩者的 Example Code，用 ES4X 執行一個 Web Server，並用 GraalVm 當作運行環境，採用 R繪製圖表。\n安裝 到 https://github.com/oracle/graal/releases/tag/vm-19.1.1 下載安裝檔，解壓縮後加入路徑\n1 2 $ export PATH=\u0026lt;檔案位置\u0026gt;/graalvm-ce-19.1.1/Contents/Home/bin:$PATH $ export JAVA_HOME=\u0026lt;檔案位置\u0026gt;/Contents/Home 設定好之後，可以使用以下幾個 command，node 改成用 GraalVM執行，預設僅支援 Java/Javascript，如果需要其他語言則要 $gu install python 等\n1 2 3 4 5 6 7 8 9 10 11 $ js runs a JavaScript console with GraalVM. $ node replacement for Node.js, using GraalVM’s JavaScript engine. $ lli is a high-performance LLVM bitcode interpreter integrated with GraalVM. $ gu (GraalVM Updater) can be used to install language packs for Python, R, and Ruby. 簡單試一下跨語言的特性\n1 2 3 4 5 // 儲存成 test.js var array = Polyglot.eval(\u0026#34;python\u0026#34;, \u0026#34;[1,2,42,4]\u0026#34;) console.log(array[2]); $ node --polyglot --jvm test.js 使用 Ployglot.eval 可以執行其他指定語言的語法，\n剛執行時會需要花比較多的時間 warn up，所以如果要當作 Command line tool 或其他生命週期短的應用程式，就建議繼續使用 Nodejs 即可；\n長時間運行的話，GraalVM 的效能與 V8 其實是差不多的，但如果多考量跨語言的特性，GraalVM 會是個不錯的選擇\nES4X 先安裝初始化\n1 2 3 4 5 6 7 8 9 10 $ npm install -g es4x-pm $ mkdir test $ npm init $ es4x init // 安裝所需套件 $ npm install @vertx/unit --save-dev $ npm install @vertx/core --save-prod $ npm install @vertx/web --save-prod $ npm install index.js，接著用 $npm run start 執行\n1 2 3 4 5 6 7 8 9 /// \u0026lt;reference types=\u0026#34;@vertx/core/runtime\u0026#34; /\u0026gt; // @ts-check vertx .createHttpServer() .requestHandler(async function (req) { req.response().end(\u0026#34;hello world.\u0026#34;); }) .listen(8080); 需注意，如果要達到最佳性能的提升需要使用它提供的 web framework @vertx/web，如果是使用像 Express.js底層的 System API Call 就不會走 Vertx，而是用當下 Runtime 環境的處理方式 (Node.js or GraalVM)，細節還需要更近一步理解專案才能解釋，不過從作者的描述現況是如此，所以舊專案要移植會重寫的門檻\n另外目前不支援 GraalVM的 Polyglot，已回報給作者需要等他實作。\n總結 跳脫框架，以前沒想過透過替換 JS Runtime 可以換取性能的提升，也不曾接觸過像 GraalVM這樣多語言支援的 Runtime，細節有蠻多關於 Compiler 相關的知識，自己很多都忘了，需要在加強才行\n總結目前使用 GraalVM與 ES4X\nGraalVM 可以測試通過 npm 90%的 package，除了一些 native code 編寫的 library，其他不太需要擔心支援度問題； GraalVM 語法支援到最新的 ES2019/2020，這點還蠻不錯的 GraalVM 性能對比與 V8不差多少，只是要一段 warm up 的時間，如果有跨語言整合需求，可以考慮看看 ES4X 可以結合 Vert.x 與 GraalVM優點，得到大量的性能提升，但是專案還沒穩定，需要考量；\n開發也必須用他的 Framework，使用 Express.js / Koa.js 等用戶無法無痛轉移 ","date":"2019-08-18T11:31:29.945Z","permalink":"https://yuanchieh.page/posts/2019/2019-08-18-%E8%AE%93-node.js-%E8%B7%91%E5%BE%97%E6%9B%B4%E5%BF%AB-es4x-%E5%B0%88%E6%A1%88%E8%88%87graal-vm-%E4%BB%8B%E7%B4%B9/","title":"讓 Node.js 跑得更快! ES4X 專案與Graal VM 介紹"},{"content":"在最新的一期 Javascript Weekly 中看到 V8 部落格 2019/07/09 的文章，裡頭提到 ECMAScript Spec 將 Sorting 改成 Stable，也就是如果排序上兩個元素順序相同，則最終排序完成的陣列中的相同順序元素，會依照原本的順序排列，例如\n1 [{name: \u0026#39;a\u0026#39;, value: 2}, {name: \u0026#39;b\u0026#39;, value: 2}, {name: \u0026#39;c\u0026#39;, value: 1}] 依照 value 排序 ===\u0026gt;\n1 2 3 [{name: \u0026#39;c\u0026#39;, value: 1}, {name: \u0026#39;a\u0026#39;, value: 2}, {name: \u0026#39;b\u0026#39;, value: 2}] // a 跟 b 順序相同，而 a 維持在 b 之前 先前的 Spec 沒有特別定義，所以各平台的實作不同，之後Chrome 70+ 與 Nodejs 12+ 開始支援 Stable Sorting，其餘的平台還不確定。\nTimSort Timsort\n作者原文\nhttp://svn.python.org/projects/python/trunk/Objects/listsort.txt\n參考自維基，Tim Sort 原理是由 Tim Peter 基於 Merge Sort 與 Insertion Sort所設計的排序演算法，Tim 觀察到實際世界的陣列多半是由多個部分排序的數列所組成，所以 Tim Sort 會先找出陣列中排序好的子陣列(稱為 run)，不斷的合併 run 直到整體排序結束，基於這樣的原理可以設計出更好的排序演算法。\n作者提到，random array 最佳的排序法被 bound 在 O(nlog n) (文中是用 O(n!)，兩者等價)，但是不同的 O(nlog n)等級的 sorting 演算法離 O(log n) 的極限值還是有差距，Timsort 大概距離極限值 1%，而一般的 quick sort 則是 39% 之遠；\n與原本 Python native 的 sample sort 相比兩者差異不大，random array Timsort 較差，但是 Timsort 針對部分排序的陣列表現更佳。\n目前 Python、Android SDK、Java SDK、Chrome 、Swift 內部的 sort() 都採用 Tim Sort，應用非常廣泛。\n基本觀念 Understanding timsort, Part 1: Adaptive Mergesort\n直接死磕 Wiki有點看不懂，每個英文句子都懂但就是無法理解背後設計的含義，網路上看到這篇最淺顯易懂的解釋，以下簡單摘錄重點幫助理解。\nTim Sort 脫胎於 Merge Sort，試著用這三個方向去優化 Merge Sort\n讓 merge 過程更快 讓 merge 次數變少 在特殊條件下用更好的方式取代 Merge Sort 試想有個陣列\n{5, 6, 7, 8, 9, 10, 1, 2, 3}\n用怎樣的方式可以用最少的 merge 完成排序？\n直覺來看，把陣列拆成兩個已經按照升序排序的陣列 {5,6,7,8,9,10} 與 {1,2,3} 然後合併，只需要一次 merge 就可以完成\n假使我們先提出一個演算法：找到陣列開頭最長的連續升序排序的陣列，其餘當作第二子陣列，以遞迴方式持續處理\n這個演算法利用了已排序的陣列去減少 merge 的次數，但有個問題 如果陣列剛好是倒序，就會落入 worst case O(n²) ，稱不上是個理想的排序演算法，所以實作時會補上如果發現連續的子陣列是降序，則 in memory 反序。\n先退回最基本的 Merge Sort\n1 2 3 {{1}, {2}, {3}, {4}} {{1, 2}, {3, 4}} {{1, 2, 3, 4}} 這是 Merge Sort 的流程，將陣列分拆到為一然後在逐步 merge，我們將第一步分拆的過程改為已排序好的區間(稱為 Run)，然後進行 merge。\nTim Sort 在 merge 時會盡量 Balance，所以會設定 minrun，如果這次的 RUN 長度小於 minrun，則會補足到 minrun長度後，接著使用 binary insertion sort，如何選擇 minrun 是個學問，作者提到一個實務上的設定是\ntake the first 6 bits of N, and add 1 if any of the remaining bits are set\nN 是陣列長度，最主要是希望如果遇到 random array，則N 盡可能被 minrun 切成 2 次方倍 RUN，在 merge 時可以最平衡。\nminRun 是希望剩餘的數再分minrun的時候能盡可能是2的冪次，後續再做merge的時候才能兩兩合併效率較高；\n實作上會不斷將n除以二 直到n\u0026lt;MIN_MERGE，而中間只要有任何一次不能被2整除，最終結果就加一，這樣取的minRun會讓分組最接近2冪\n(感謝公司同事 Frank 補充)\n例如 N = 2112，minrun = 32 會切成 66個RUN，則合併時最後會變成 2048 + 64，兩者非常不平衡；\n但如果 minrun 是 33，則會被切成 64個RUN，切割成 2的 n 次方個數合併起來就很平衡 (perfect balanced)。\n決定何時 merge? Tim Sort 會用一個 Stack 暫存 RUN，並不是一開始就輪詢整個陣列產生RUN，而是逐步進行，避免要用掉過多的記憶體。\n基於這兩個原則，Tim Sort 實作上有個函式 merge_collapse ，這個函式主要判斷目前 Stack 最上層的三個 RUN 是否符合以下規則\n1 2 1\\. A \u0026gt; B+C 2\\. B \u0026gt; C 如果符合，則將下一個 RUN push 到 Stack上，反之則比較 A跟Ｃ，較小者與 B merge，例如\n1 2 A:30 B:20 C:10 A \u0026lt;= B + C 所以不符合 merge_collapse，又因為 C \u0026lt; A，所以 C 跟 B merge 變成 A:30 BC: 30 透過這樣的方式，讓保留在 Stack 上的 RUN 長度平衡，另一個重點是合併只能是 A+(B+C) 或是 (A+B)+C，因為要保持 Stable，所以 merge 一定要相鄰兩個 RUN。\n如何 merge 要將兩個相鄰子陣列，用 in memory 方式 merge 在實作上會有困難，實務上因為要造成大量的 memory 操作其實也不會比較快，所以 Tim Sort 使用一塊暫存記憶區，大小為 (A,B) 陣列的最小長度\n如果 A \u0026lt; B，則先將 A 放進暫存記憶區，最直覺的方式是 A 跟 B 從頭開始比對，小的放進 A 的位置，直到 A 跟 B 排序完成，如同一般 merge sort 的做法，又稱為 one-pair-at-a-time。\n但因為 A 跟 B 都已經是排序好的陣列，所以有個優化的方式是找出 B[0] 在 A陣列排序的位置，然後A該位置之前都是小於 B[0]，所以可以整段放進去，接著找剩餘A[0]在 B的位置，不斷輾轉直到排列結束，也就是 gollaping mode。\n1 2 3 4 5 6 7 8 9 10 11 12 A: [1,2,3,7,8,9] B: [4,5,6,8,9,10,11] A \u0026lt; B，所以 A 放入暫存區 temp 先找 B[0] 在 A 的位置，也就是在 A[2]、A[3]之間，因為 B[0] \u0026gt; A[2]，也就是 A[0]~A[2] 可以直接放回去，變成 A: [1,2,3] B: [4,5,6,8,9,10,11] temp: [7,8,9] 接著找 temp[0] 在 B的位置，也就是 B[2]、B[3] 之間，變成 A: [1,2,3,4,5,6] B: [9,10,1] temp: [7,8,9] 接著找 B[0] 在 temp 的位置，持續反覆 這優化的方式大多數是有正向的效果，但如果遇到 random array，可能會比原本的 one-pair-at-a-time 還要慢，但作者提及\nIt\u0026rsquo;s generally true in this algorithm that we\u0026rsquo;re willing to gamble a little to win a lot, even though the net expectation is negative for random data\n風險與收益評估後，整體還是正向的，所以就決定採用。\n實務上有一個固定參數 MIN_GALLOP =7 與變動參數 minGallop，一開始採用 one-pair-at-a-time mode，一直到某個陣列的首位元素持續大於另一陣列，才切換到 galloping mode；\n如果 galloping search(看後續)找到的元素位置小於MIN_GALLOP 則退回 one-pair-at-a-time mode，反之則持續用 galloping mode；\n如果一直在 galloping search，會下修 minGallop，也就是更容易進入 galloping mode 的意思。\n反覆執行到兩個陣列合併完成\n找出某數在已排序陣列中的位置 進入 galloping mode 時，會需要不斷的查找某數在已排序陣列的位置，假設 A 是較短的陣列，一開始找 A[0] 在 B陣列應排序的位置，最直覺的方式是用 binary search，但這邊作者改採用另一個演算法 galloping search(又稱 expotential search)\n相較於 binary search 不斷對半切查找，galloping search 是比對 (2^k)th 元素，也就是找 1, 3, 5, 7, 15 這樣的方式，當找到 (2^(j-i)) \u0026lt; x \u0026lt; (2^j) 時，在改用 binary search。\n比較這兩者， galloping search 的時間複雜度是 O(i)，i 指的是 x 在查詢陣列的位置，如果 i 很前面那效率就會很高；\nbinary search 時間複雜度為 O(n)，n 是查詢陣列的長度。\n而 n ≥ i ，所以從時間複雜度來看 galloping search 會比 binary search 還要快一些。\n但實際上，因為陣列是隨機的，採用 galloping search 可能會比 linear search 慢，作者列出 galloping search 對比 linear search 的計算花費，可以看到在 i=7 之前 galloping search 會需要更多的比較次數，而比較是很花計算資源的，所以 MIN_GALLOP預設是 7。\n總結演算法 稍微總結一下，Timsort 維護一個 Stack，Stack 上會 push 已排序的連續子陣列，並透過 merge_collapse 判斷是否先 merge Stack 上的陣列，盡量保持子陣列的長度接近；\nmerge 過程，則是動態在 one-pair-at-a-time 與 galloping mode 中切換，用有效率的方式合併兩個已排序好的陣列；\n進入程式碼 最終還是要看一下代碼，原作者是用 C寫，因為 java code 比較好閱讀參考 android TimSort 實作\nluni/src/main/java/java/util/TimSort.java - platform/libcore - Git at Google\n也有 js 版，但是註解較少\nmziccard/node-timsort\nminRunLength() 定義如何決定 minrun的值\n1 2 3 4 5 int r = 0; // Becomes 1 if any 1 bits are shifted off while (n \u0026gt;= MIN_MERGE) { r |= (n \u0026amp; 1); n \u0026gt;\u0026gt;= 1; } return n + r; gallopLeft() 用 galloping search 找出最左 ≤ element 的位置，因為是 pass 整個 array 的 reference，所以會有 base / hint 去定位元素，看起來稍微複雜\nmergeLo()、mergeHi() 分別對應 (A+B)+C / A+(B+C) 兩種情況，邏輯類似， outer 段落就是在 one-pair-at-a-time mode，只有任一邊陣列連續大於 minGallop 才會切到 galloping mode；\n接著再 galloping mode，找到的元素必須位置大於 MIN_GALLOP，否則就會跳出 galloping mode，同時 minGallop會 +=2，也就是下次待在 one-pair-at-a-time mode 會更久。\n透過 minGallop 與 MIN_GALLOP，確保 merge 在兩種模式中取得較佳的效率。\nBUG 如果再查 TimSort，你可能會找到這一篇文章\nEnvisage: Engineering Virtualized Services\n主要在講 java 版的實作，可能會出現 OutMemeroyBound 的問題，主要是因為在 allocate Stack 的長度時，會有以下極端狀況導致 Stack 長度預設過短而記憶體不夠用\n問題出在timsort本身的約束條件\n1 2 1\\. runLen[i-2] \u0026gt; runLen[i-1] + runLen[i] 2\\. runLen[i-1] \u0026gt; runLen[i] 在runLen為以下情況時，120, 80, 25, 20, 30，25\u0026lt;20+30所以進行run[3]，run[4]合併，變為，120, 80, 45, 30\n這時候由於 80\u0026gt;45+30, 45\u0026gt;30 條件滿足了，merge就終止了\n但此時120\u0026lt;80+45是不滿足約束條件的，但我們只對上層進行判斷\n如果（精心策劃）一些特殊數組造成大量這樣的情況，而在原始碼中 空間是這樣申請的\n1 2 3 4 5 int stackLen = (len \u0026lt; 120 ? 5 : len \u0026lt; 1542 ? 10 : len \u0026lt; 119151 ? 19 : 40); runBase = new int[stackLen]; runLen = new int[stackLen]; 上面stackLen,是滿足上面提到的約束條件跟MIN_MERGE情況下去估計的最大可能數量，但剛也說了，只對上層進行判斷，會有例外狀況導致所需要的大小可能超出原本預想的，至於修復的方式是把檢查最後三個runLen變成檢查最後四個runLen\n1 2 3 4 5 6 7 8 9 10 11 private void newMergeCollapse() { while (stackSize \u0026gt; 1) { int n = stackSize - 2;** if ( (n \u0026gt;= 1 \u0026amp;\u0026amp; runLen[n-1] \u0026lt;= runLen[n] + runLen[n+1]) || (n \u0026gt;= 2 \u0026amp;\u0026amp; runLen[n-2] \u0026lt;= runLen[n] + runLen[n-1])) { if (runLen[n - 1] \u0026lt; runLen[n + 1]) n--; } else if (runLen[n] \u0026gt; runLen[n + 1]) { break; // Invariant is established } mergeAt(n); } } 以上 Bug 部分也是由強者公司同事 Frank 補充，有興趣者可以點進去看原文，原文包含說明了如何用工具與方法找出問題的，但因為還沒有到非常理解就不多做說明。\n這 Bug 已經被修復，在 Python 的 Bug回報討論中，Tim Peter 提到其實現有的機器沒有足夠的 Memory 去產生這樣的問題，Java 版實作也是有一些改動才有辦法復現，不過最終基於邏輯的完整性，還是先修復了此問題，避免未來有問題。\nV8的實作 Getting things sorted in V8\n前言 在評估排序演算法上，會考量比對次數跟記憶體用量，在動態語言包含 JS 中比對次數相對重要，因為在比對的時候會使用到用戶寫的程式碼\n1 2 3 4 5 6 7 8 const array = [4, 2, 5, 3, 1]; function compare(a, b) { // Arbitrary code goes here, e.g. `array.push(1);`. return a - b; } // A “typical” sort call.array.sort(compare); 比對函式回傳 0 、1(或其他正值)、-1(或其他負值)分別代表 等於、大於、小於 ，在比對函式中用戶可能會有 Side-Effect 操作等\n1 2 3 4 5 6 7 8 const array = [4, 2, 5, 3, 1]; array.push({ toString() { // Arbitrary code goes here, e.g. `array.push(1);`. return \u0026#39;42\u0026#39;; }}); // Sort without a comparison function.array.sort(); 預設的比對函式會先呼叫 toString() 轉成字串比對\n接著把 Spec 先放在腦後，有一部分是 Implementation-defined ，在一些 Spec 保留實作彈性的部分，工程師有機會去自由發揮，做出理想中用戶會希望看到的行為，但這部分各個 JS Engine 行為差異很大，例如說遇上了 accessors(getter/setter) 或prototype-chain ，強烈建議不要這樣寫程式，這裡僅作說明\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 const array = [0, 1, 2]; Object.defineProperty(array, \u0026#39;0\u0026#39;, { get() { console.log(\u0026#39;get 0\u0026#39;); return 0; }, set(v) { console.log(\u0026#39;set 0\u0026#39;); } }); Object.defineProperty(array, \u0026#39;1\u0026#39;, { get() { console.log(\u0026#39;get 1\u0026#39;); return 1; }, set(v) { console.log(\u0026#39;set 1\u0026#39;); } }); array.sort(); const object = { 1: \u0026#39;d1\u0026#39;, 2: \u0026#39;c1\u0026#39;, 3: \u0026#39;b1\u0026#39;, 4: undefined, __proto__: { length: 10000, 1: \u0026#39;e2\u0026#39;, 10: \u0026#39;a2\u0026#39;, 100: \u0026#39;b2\u0026#39;, 1000: \u0026#39;c2\u0026#39;, 2000: undefined, 8000: \u0026#39;d2\u0026#39;, 12000: \u0026#39;XX\u0026#39;, __proto__: { 0: \u0026#39;e3\u0026#39;, 1: \u0026#39;d3\u0026#39;, 2: \u0026#39;c3\u0026#39;, 6: undefined, }, } }; Array.prototype.sort.call(object); V8 在排序前後的處理 在 Spec中，將排序的元素分成三部分\nnon-undefined value，這部分會套用比對函式決定排序 undefined value，放在排序的最後 holes，不存在的值 V8 在實作上的步驟大概為\n找出 Array 或是 Object 的 length 設變數 numberOfUndefineds 為 0 在範圍 [0, length) 中\n- 遇到 holes 不處理\n- 遇到 undefined 將 numberOfUndefineds++- 遇到 non-undefined value，放到暫存陣列上 接著用 TimSort 對暫存陣列做排序，之後將暫存暫列寫回原陣列或物件中，並在後面補足 numberOfUndefineds 數量的 undefined，刪除剩餘的長度(移除 hole)，這樣就完成排序了。\n過去的實作 過去 V8 是小陣列(length \u0026lt; 10) 用 Insertion Sort 其餘採用 Quick Sort，而 Quick Sort 中分割的小陣列長度小於 10 也是用 Insertion Sort；\n因為 Quick Sort 是採用遞迴的方式，小陣列改用 Insertion Sort 效率較好。\nQuick Sort 在選擇 pivot 的點很重要，選不好就會跑到最差情況 O(n²)，V8 採用兩種策略：\n1. 選擇子陣列中第一個、最後一個、第三個數的中位數\n2. 對於大的陣列，選擇被排序過數列的中位數\nQuick Sort 優點在於是 In-Place 排序，但缺點有可能會落到 O(n²)。\n現行就改成 TimSort 了。\n","date":"2019-08-09T05:41:03.975Z","permalink":"https://yuanchieh.page/posts/2019/2019-08-09-v8-%E5%85%A7%E7%9A%84%E6%8E%92%E5%BA%8F%E6%BC%94%E7%AE%97%E6%B3%95-timsort/","title":"V8 內的排序演算法 — Timsort"},{"content":"\n划龍舟邁入第二年，開始習慣三月到六月以划龍舟當作一年的開端，重新調整自己的心態與體態；\n邁過第一年痛苦的入門期，第二年漸入佳境，開始可以享受划船的樂趣與比賽的刺激，也開始在這過程體會到一些其他心境上的成長，關於自信與團隊這件事。\n前情提要 龍舟競賽在端午節時會有多個地方政府獨立舉辦，例如我們隊上同時參加台北市與新北市，其他如新竹、台南也都有賽事；\n新北市只有大型龍舟，船上有奪標手、鼓手、8對槳手、舵手，今年賽制是奪標手在決賽才上場；\n台北市則隊伍眾多，組別也非常多，類別再分成大型龍舟跟小型龍舟，大型龍舟有 10對槳手，小型則有 5對槳手；\n比賽都是划 500公尺，大型龍舟有奪標手看奪標時間，但如果奪標失敗則看船尾通過的時間；\n小型龍舟沒有奪標手，比的是船頭通過時間，但如果發生掉槳或是其他失誤，則看船尾通過時間，通常比賽時我們會船尾通過才停。\n關於自信 還記得第一天在台北市大佳河濱公園預賽的時候，小型龍舟男子組是排在 11 點的賽程，當時正處於逆風逆流的狀態，大概會是順風順流兩倍的完賽時間，加上原本大佳的船重槳也重，雖然知道每隻隊伍都面臨一樣的處境，但想想也是蠻崩潰的，對每個人來說都是體能上與精神上很大的消耗；\n但教練在集合的時候就說：「等等逆風逆流是耐力賽，會划很久，但不用怕，我們平常最遠都划過 5公里，一定撐得完的！」\n聽到這看著隊友相視而笑，今年隊上的訓練模式比較注重耐力，還記得在四五月進入備賽期時，每天早上固定會先拉長距離暖身，從一開始 500公尺慢慢拉到 800、1200 到 2000公尺；\n有一天早上例行暖身，從碧潭碼頭出發繞到上游再一路拉到吊橋下，完成 2000公尺的暖身，看到吊橋心裡覺得差不多該停了吧，但奇怪的是教練怎麼沒有停的意思，只有一直喊「不要放，繼續拉長」後來舵手繞了一圈回頭從吊橋再度往上游出發，大家心裡都覺得不妙了，是不是教練算數不好哩程數算錯了，但教練在大家也不敢偷懶，只能咬著牙繼續拼；\n那時候肌肉已經開始酸痛，尤其是肩膀要負責回槳跟壓槳入水，整個三角肌緊繃，身體好像被灌水泥一樣的沈重，但沒有人敢停下動作或是分神，因為教練在船頭虎視眈眈的注視著你；\n就像在沙漠行走快要失水而死的旅人，渴望停下來休息，在還沒停下來的過程，開始會思考這樣會不會受傷，逃避或偷懶的念頭產生，另一方面也會想要逼出更好自己再多撐一下，就在兩種念頭不斷交雜的過程，終於到了，還記得那天划了 3200 公尺，比之前多了 50%的里程；\n後面教練就逐步突破大家忍受的極限，最遠一趟划了 5000公尺。\n回想到整個訓練的過程，與身邊的隊友們一起經歷過的這些訓練，突然覺得心中的不安與惶恐都散去，力氣也慢慢湧上，將過去的積累化成最踏實的信心，相信自己相信隊友，後來預賽也順利通過，今年開始每一槳都很穩定的抓水到最後，很累但也很過癮，一路挺進半決賽與最終決賽，最後拿下第四名的成績。\n我相信在任何領域想要成功都必須要加倍的付出，這些付出不單單帶來實力的提升，還有心態上正面的影響；\n運動算是相對單純的領域，持續的練習，就會持續的看見進步，到了比賽要上場的那一刻，手上磨出來的繭化成握槳最有力的自信。\n看見其他比我們厲害的隊伍，會自知差了一截但也不致於氣餒，反而會更尊重他們的付出，因為他們更加的刻苦與努力才會有了今天的佳績；\n看到其他實力稍弱的隊伍也不會輕視，因為可能他們的生活重心不在龍舟上，就珍惜能夠一同競技的緣分。\n龍舟有趣的地方在於整隻隊伍沒有英雄，不像棒球或籃球，雖說是團隊競賽但季後賽還是會需要球星的挺身而出；\n但龍舟就是一榮俱榮，整齊劃一的槳法與均衡的力量才能贏得比賽，而且一湊就要湊滿10人或是20人，相對不太容易，但也因為這樣比賽起來很熱鬧，也很過癮。\n工商時間 新店同心救難隊龍舟隊歡迎大家的加入，目前都是三月中旬訓練到端午節，固定早上 5:20 ~ 6:30 於碧潭練習，有興趣練身體當運動，或是想要享受比賽的人都很歡迎，不過就明年請早 XD\nOS. 訓練強度如果身體不適還是可以休息，不用擔心會操到受傷，但就是要自己評估跟衡量；\n隊上也有分老手拼比賽，人數夠新人也會湊一艘練習\n結語 又挺過一年了，今年隊上拿了不少名次，女子組成績依然亮眼，其他組別也稍稍有所突破了。\n在比賽期間很感謝所有的隊員，包含行政的工作人員與負責開車接送的隊友，除了比賽訓練還要幫忙其他雜物真的很辛苦；\n另外還有到場沒有上場機會的隊友，感謝大家犧牲連假一同到場幫忙加油，要撐起一隻隊伍需要很多人的努力與付出，感謝大家。\n","date":"2019-06-10T23:08:00.265Z","permalink":"https://yuanchieh.page/posts/2019/2019-06-10-%E9%BE%8D%E8%88%9F%E7%AC%AC%E4%BA%8C%E5%B9%B4-%E9%97%9C%E6%96%BC%E8%87%AA%E4%BF%A1/","title":"龍舟第二年 — 關於自信"},{"content":"划龍舟邁入槳手二年級生，體能與划槳上慢慢可以跟上大家，雖然還有很多需要調整的細節與磨練，每次練習也都是累得半死，但確實比較得心應手些，慢慢享受龍舟的樂趣；\n但不得不說一開始以為槳手，身為船上的動力輸出是最累人的，平常舵手大哥們厲害到讓我在划槳時完全沒感受到他們的存在，誤以為船比來就該直直的跑，所以舵手感覺起來很輕鬆，但這種觀念錯得離譜!!\n包含上課的資深教練 馮建堂教練也提到，現在很多參賽隊伍的教練也有些偏差的觀念，都不太重視，也不知道如何訓練舵手，導致比賽時連出碼頭、到比賽定點都有問題，更甚者翻船、擦撞等安全危險，這也是台北市體育局要辦舵手研習營的初衷。\n很感謝台北市體育局舉辦和隊上讓我有機會參加舵手研習營，從舵手的角度學習，從新對龍舟這項運動有不同的認知與體會，也讓我更加崇拜默默付出的舵手大哥了，以下是參加 2019 年度的舵手研習營的筆記。\n出處：馮建堂教練的 FB 相簿\n舵手的工作 舵手主要操舵，在龍尾控制船前進的方向，舵又分成活動舵與固定舵，固定舵在船尾有多一塊結構讓舵穿過，在調整方向時有個支點可以稱；\n而活動舵則無，必須靠自身的力量或船板當作支點。\n在有些比賽舵手可以助划，教練表示舵手助划厲害的可以抵上3～4 名槳手，十分的驚人。\n舵是利用擋水的方式，讓船尾受力，進而改變船運行的方向，當船尾受到往左的力量，船頭自然就會相對往右，反之亦然；\n因為主要是透過水的反作用力，所以船在有動力的情況下才能改變方向，所以在轉彎時也是要請槳手輕槳划；\n如果再沒有動力下，靠舵手一人挖水或推水，那會十分的累而且沒效率。\nhttps://www.facebook.com/taipei2017/photos/pcb.1227131774129091/1227131447462457/?type=3\u0026amp;theater\n（槳與舵的對比，剛好這位是我實際演練的指導教練）\n偏滑-船為什麼不會乖乖走直線 偏滑又可分成人力與天然因素\n人力因素主要像是槳手左右雙方的體力與划槳力量不均；\n天然因素的種類就很多，像是風浪、潮汐、水底地形等都會受到影響。\n在講解的過程中，教練提到水是有空間的，當槳手插槳入水往下往後推時，水會被推擠的，但水是佔據空間的，當觸底時就會回彈產生湧，如果水深不夠就會向上推擠，導致船的晃動，有種被黏住的感覺，這也是為什麼國際賽事規定賽道水深一定要3米深以上 ；\n水流的部分，教練舉大佳河濱公園為例，大佳河濱公園位於基隆河上，水流是從松山往圓山方向流，因為河道天然彎曲的緣故，水流在外側會比較湍急，也就是靠近大直的方向，所以3,4 水道的水流會比較快，舵手如果可以找到順水流的方向，就可以讓船跑得更輕鬆；\n但除了水流外，潮汐也會影響，再漲潮時剛好與水流方向相反，會產生兩股力量的交會與碰撞，確切的點不是很固定，所以就很靠舵手的經驗，教練表示有些資深舵手沒有注意也會栽在上頭。\n教練也有提到去香港比賽，在出海口附近會有側浪，又或是有的場地會有水草，比較慘的就會被水草絆住影響成績，不同的比賽場地有不同的水文，這都是舵手需要仔細去研究，找出賽道上最適合的位置。\n實際下水 訓練期間有下水練習六次，每次每人大概練10–15分鐘，一下水連走直線都是個困難，後來比較知道說要把舵插深，讓舵受力去讓船抓直，上手要握好但又要有點放鬆，去感受水流的作用；下手扣緊船版，讓舵可以靠在船邊，方便使力；\n一個操控的重點是提前修正，當發現船微微偏向時，就要開始修正，此時會比較輕鬆，也不會擋太多水拖累船速，如果當真的偏移時，要在修正就非常的困難，尤其是因為船有慣性，沒有控制好就會暴走，在原地打轉，我就打轉了幾次，因為轉彎時來不及修正；\n這也是為什麼女性舵手不見得會輸男性舵手，水感好能夠提前修正，其實不需要到非常大的力量，反而是經驗與臨場反應比較重要。\n但如果沒有及時修正，整艘船的力量十分可怕，在大轉彎時我用全身的力量才有辦法定住船，轉一個彎10秒鐘就滿身大汗，下手扣船版的大拇指到現在一週還有點酸緊。\n另外剛好培訓那幾天風浪比較大，所以在進出碼頭就相對比較危險，因為船在低速很容易受到自然偏滑的影響，動作不夠精準很容易又被浪推回碼頭出不來，又或是瞬間被打橫無法操作，但教練表示很慶幸在練習時遇到風浪大的狀況，提前練習總比比賽時手忙腳亂來的好。\n其他訊息 教練有分享一些龍舟的資訊很有趣，像是他有帶隊參加冰上龍舟\n另外還有海上龍舟，之後龍舟有機會列入奧運的正式項目，十分的期待。\n結語 舵船真的不簡單，整艘船的命運與方向就決定在舵手中，不單是體能上，更是心靈與精神上的鍛鍊。\n","date":"2019-05-25T03:13:32.795Z","permalink":"https://yuanchieh.page/posts/2019/2019-05-25-%E9%BE%8D%E8%88%9F%E8%88%B5%E6%89%8B%E7%A0%94%E7%BF%92%E7%87%9F%E5%BF%83%E5%BE%97/","title":"龍舟舵手研習營心得"},{"content":"前陣子做全球不同地區 API Server 的部署，希望用戶基於延遲性選擇最靠近的 API Server，透過 Route53 + API Gateway 實作非常簡單。\n但量測延遲性，理論上只能由 Client 向多個 Server 發送，最後評比整段 Http Request 完成的時間；\nRoute53 身為 DNS Server，如果是從 DNS Server 去打 Server，那量測的結果應當是 Route53 到 Server 的延遲，而不能代表 Client 到 Server；\n好比說 User 在台灣，Route53 Server 在美國，Server1 在美西，Server2 在東京，那從 Route53 角度一定是美西的 Server1 比較近，但對 User 來說會是日本的 Server2 比較近才是；\n又如果說 Route53 是全球部署，那Route53 又如何決定 User 要連到哪個地區的 DNS Server ? 又例如說 CDN，同樣會遇到要去哪個 Local CDN Server 比較快的問題？\n以下是研究這個問題的過程。\nHow Amazon Route 53 Uses EDNS0 to Estimate the Location of a User 參考官方文件，Route53 支援 DNS protocol 額外擴充 EDNS0 中的edns-client-subnet。\nDNS 技術發展於 1980 年代，當時 protocol 設計只有留 512 bytes 可以夾帶資訊，但隨著時間演進，人們希望加入更多的功能，例如說更多的 IPv6、[DNSSEC](https://medium.com\nedns-client-subnet 主要是讓 client 再發起 DNS resolve 時可以在 query 中夾帶自己 IP 的 subnet，讓 DNS Server 可以知道 client 確切的 IP 來源而不會再 Recursive 解析過程中被置換，更詳細解釋於下一節；\n如果 client 不支援 edns-client-subnet，則 Route53 會拿 IP當作來源位置判斷。\nRoute53 是透過 IP 判斷用戶的位置，並用此位置當作量測的基準，Geolocation / Latency based 都是如此；\n而 IP 位置來源優先使用 edns-client-subnet，其次用 source IP。\nedns-client-subnet (ECS) 摘要一些 RFC 內容\nRFC 7871 - Client Subnet in DNS Queries\nDNS 查詢的方式是一層一層的，從最頂級的域名一路向下，例如說 hello.example.com，就會從 .com Nameserver → example.com Nameserver 逐步查詢。\nClient 會透過 Stub Resolver (可理解一個 DNS Agent)，透過 Intermediate Nameserver 向 Authoritative Nameserver (握有 DNS zones 域名區域) 發起請求。\n其中 Intermediate Nameserver 有兩種：\nForwarding Resolver：不會遞迴解析，僅會傳遞給下一個 Recursive Resolver。 Recursive Resolver：遞迴 domain chain 直到域名解析完成，會透過 cache 快速返回查詢。 目前來說，Recursive Resolver 使用日益增加，因為集中化管理有幾個優點 cache 更多資訊 、審查用戶的 DNS查詢 ，但是傳統的 Recursive Resolver 在遞迴查詢時，會將 Source IP 改成自己的 IP而不是 Client 的 IP，但 Recursive Resolver 跟 Client 可能隔很遠(網路拓墣上的距離)；\n如果此時 Authoritative Nameserver 希望解析 Client IP 提供量身定做的 DNS Answer (Tailored Response)，就沒有辦法，因此制定 edns-subset-client 解決此問題。\nOption Format 1 2 3 4 5 6 7 8 9 10 11 12 +0 (MSB) +1 (LSB) +---+---+---+---+---+---+---+---+---+---+---+---+---+ 0: | OPTION-CODE | +---+---+---+---+---+---+---+---+---+---+---+---+---+ 2: | OPTION-LENGTH | +---+---+---+---+---+---+---+---+---+---+---+---+---+ 4: | FAMILY | +---+---+---+---+---+---+---+---+---+---+---+---+---+ 6: | SOURCE PREFIX-LENGTH | SCOPE PREFIX-LENGTH | +---+---+---+---+---+---+---+---+---+---+---+---+---+ 8: | ADDRESS... | +---+---+---+---+---+---+---+---+---+---+---+---+---+ edns-subset-client protocol 是基於 EDNS0 制定，以下是他的 package 內容：\nOPTION-CODE\n兩個八位元組，固定是 0x00 0x08 OPTION-LENGTH\n兩個八位元組，代表 payload 長度 FAMILY\n兩個八位元組，代表 address 的 family (IANA標準)，目前支援 IPv4跟 IPv6 SOURCE PREFIX-LENGTH\nAddress 遮罩，Client 用來指定查詢的 IP 遮罩 SCOPE PREFIX-LENGTH\n同樣是 Address 遮罩，但是是由 Response 表示 Address 覆蓋範圍。 SOURCE PREFIX-LENGTH 跟 SCOPE PREFIX-LENGTH 這兩個參數相對比較重要\n當 Client 或 Stub Resolver 發起 name resolve 時，會指定 Address 與 SOURCE PREFIX-LENGTH，SOURCE PREFIX-LENGTH 算是希望保留 Client IP 部分隱私，不見得要全部都送往 Nameserver；\n但 SCOPE PREFIX-LENGTH 必須設為 0 因為這是用在 Response 上 當 Recursive Resolver 收到時，會透 SOURCE PREFIX-LENGTH 遮罩查詢 Cache，如果有則返回；沒有則繼續查詢，此時遞迴查詢的 SOURCE PREFIX-LENGTH 僅可小於等於來源查詢的 SOURCE PREFIX-LENGTH Authoritative Nameserver 如果回傳的 SCOPE PREFIX-LENGTH 小於 SOURCE PREFIX-LENGTH，代表不需用提供這麼多 bits；\n反之 SCOPE PREFIX-LENGTH \u0026gt; SOURCE PREFIX-LENGTH，則代表需要提供更多得 bits 才能得到更精準的 Answer。 Authoritative Nameserver 處理 Cache 時要多加注意，不可以有 Prefix overlapping ，避免匹配到短的 prefix 回傳錯誤的 RRsets；\n例如說原本的 cache 是 1.2.0/20 A，但此時多加了 1.2.3/24 B，就需要拆成 1.2.0/23, 1.2.2/24, 1.2.4/22, 1.2.8/21 A，1.2.3/24 B，避免疊合。 資安風險 生日攻擊\n如果駭客向 Intermediate Nameserver 發送大量的假 DNS Answer，如果不小心被吻合到，Intermediate Nameserver 就會回傳被釣魚的IP，這個問題在原本的 DNS 就會出現。\n→ Intermediate Nameserver 必須對 DNS Answer 做欄位檢查，最好支援 DNSSEC 減輕問題發生機率 Cache 污染\n支援 ECS 後，Cache 機制變得更加複雜，會需要基於 FAMILY / SCOPE PREFIX-LENGTH / ADDRESS Cache，造成 Memory 用量大增；\n如果駭客運用這一點，用洪水攻擊製造大量不容易命中 Cache 的查詢，對 DNS Nameserver 做 DDos 攻擊。\n→ Nameserver 必須自行做好評估 示範案例 Spec 中有個示範案例\n1. Stub Resolver (SR)，IP位置是 2001:0db8:fd13:4231:2112:8a2e:c37b:7334，準備透過 Recursive Resolver (RNS) 查詢 www.example.com\n2. RNS 支援 ECS，查詢 www.example.com 是否在 cache中，沒有則開始查詢\n3. RNS 向 root Nameserver .com 查詢，向 root Nameserver 查詢不需要夾帶 ECS 選項\n4. 接著 RNS 準備去找 .example.com Authoritative Nameserver (ANS)\n5. RNS 要傳遞的封包，必須加入 ECS 選項\nOPTION-CODE: 8 OPTION-LENGTH: 0x0b，固定 4 bytes + 7 bytes 的 Address FAMILY: 0x00 0x02，代表 IPv6 SOURCE PREFIX-LENGTH: 0x38，遮罩代表 /56 bits SCOPE PREFIX-LENGTH: 0x00，這是 Answer 用的 ADDRESS: 0x20 0x01 0x0d 0xb8 0xfd 0x13 0x42，也就是前 56 bits 6. ANS 收到後，產生結果(Tailored Response)回傳，其餘內容相同\nSCOPE PREFIX-LENGTH: 0x30, 代表 /48 7. RNS 收到後，比對 FAMILY, SOURCE PREFIX-LENGTH 和ADDRESS，如果不吻合則拋棄\n8. RNS 基於 ADDRESS, SCOPE PREFIX-LENGTH 和 FAMILY 做 cache\n9. RNS 回傳結果給 SR，此時不需要 ECS 選項\n透過 dig 檢驗 dig 支援 edns-client-subnet 參數，藉此觀察 dns 回傳的 A record，指令為 dig [@8](http://twitter.com/8 \u0026quot;Twitter profile for @8\u0026quot;).8.8.8 {測試 domain name} +subnet={測試的 ip} ，網路上很多資料是使用 +client={測試 ip} ，我實測是用 +subnet 才可以。\n前面提到的 SCOPE PREFIX-LENGTH ，Route53 回傳 24，也就是最多提供 24 bits 的 Address 就可以取得最佳解。\n測試的 domain 是公司內部透過 Route53 與 API Gateway 架設，不方便公開，但透過 VPN 取得不同區域的 ip，例如日本、印度、加拿大、阿根廷等地，放入 subnet 參數後，DNS 回傳的 ANSWER 確實跟著地區而改動；\n有點弔詭的是巴西不走國內反而是到法國 Frankfurt的伺服器，而阿根廷是到巴西 São Paulo的伺服器；\nGeoIP 查詢透過 Maxmind，他有每日查詢上線，是綁定 IP限制。\n","date":"2019-04-29T00:18:14.937Z","permalink":"https://yuanchieh.page/posts/2019/2019-04-29-route53-latency-based-routing-%E6%A9%9F%E5%88%B6-dns-%E5%A6%82%E4%BD%95%E8%A9%95%E4%BC%B0%E5%BB%B6%E9%81%B2/","title":"Route53 Latency-Based Routing 機制 — DNS 如何評估延遲"},{"content":"架構上使用 elb 當作 load balancer proxy，後端接 nodejs api server，但是偶爾拋出 502 錯誤，elb log 顯示該次連線沒有進到 api server，麻煩的是機器 health check 正常，絕大多數的 api 測試也都正確，錯誤不太好復現，直到後來才發現是 proxy 與 api server 在 persistent connection 的 time-out 機制有所不同。\n以下是研究 http 1.0 / 1.1 keep-alive / persistent connection 機制，並重新復現與解決問題。\npersistent connection 簡介 https://en.wikipedia.org/wiki/HTTP_persistent_connection#/media/File:HTTP_persistent_connection.svg\n參考自維基百科，persistent connection 主要是希望在短時間內有多個 http request 時，可以重新利用 tcp connection，而不是每次都重新建立 tcp連線，降低 tcp handshaking 時間等開銷。\n在 http 1.0 並沒有正式支援且默認關閉，但是蠻大多數的 http agent 都有支援，在 header 註明 Connection: keep-alive ，如果 server 也支援會把 header Connection 再次回傳；\n直到 client 或 server 決定斷線才會發送 Connection: close 斷開連線。\n在 http 1.1 默認開啟，如果關閉 persistent connection 則必須主動在 http request 夾帶 Connection: close 。\n在 reverse proxy 中或是 application server 都可以設置 keep-alive timeout，決定 server 在閒置多少秒數後沒有收到新的 http request 就主動斷開連線，以下透過 wireshark 實際觀察 persistent connection。\nserver.js 的程式碼很簡單\n1 2 3 4 5 6 7 const http = require(\u0026#39;http\u0026#39;); const server = http.createServer((req, res) =\u0026gt; { res.end(); }); server.listen(3000); http 1.0 ： client ← → server 如果直接使用 nodejs http module或是其他二次開發的模組，預設是採用 http 1.1 且不能修改，此時必須使用更底層的 net 模組發送 http 1.0 request\nServer 很明確知道此次 connection 沒有要復用所以就可以直接斷開連結。\nhttp 1.1: client ← → server http 1.1 request 預設會使用 persistent connection，程式碼把上面的 Http/1.0 改成 Http/1.1 即可，觀察 wireshark server 的回應就有所不同\n觀察到 http 1.1 預設支援 persistent connection，所以 server 會等到 keep-alive timeout (nodejs 8.0 後預設 5秒)才會斷開連結，從封包顯示是由 client 斷開連結。\n為什麼 client 會主動關閉而沒有走 persistent connection 呢？\nhttp 1.1: keep-alive 在 nodejs中，http request 如果要使用 keep-alive，必須透過 http agent 發送，http agent 會生成 socket pool 並控制 socket 的復用與關閉時機；\n如果不使用 http agent，即使 header 或預設支援 keep-alive，每個 request 結束後client都會主動發送 [FIN,ACK] 斷開連結。\n觀察到 tcp 少了一次完整的交握 (建立與結束)，省了 9 個 tcp packets 來回的時間。\nhttp 1.1: keep-alive timeout 預設 server 是五秒，那如果 client 第二個 request 超過五秒發送會發生什麼事？\n根據觀察結果，在第一個 request 完成後，每隔一秒(秒數可調整)會從 client 發送 [ TCP Keep-Alive ] 封包，五秒到 server 主動斷開連線；\n下一個 http request 就必須重新建立 tcp 連線。\n小結：\n沒有走 persistent connection，通常是由 server close connection，因為 server 決定 response 的長度；\n但是 persistent connection 下，可能由 client 或 server 任一者 close connection，不過新的 request 還是由 client 發起。\n現在加入 Nginx Reverse Proxy，模擬幾種情況\nNginx Proxy — 不使用 KeepAlive 從最基本的設定檔開始，最單純轉發的動作\n接著用 docker 執行\ndocker run -v /Users/zhengyuanjie/Desktop/Nodejs/persistent-connection/ka_nginx.conf:/etc/nginx/nginx.conf -it -p 8080:80 nginx\n目前分成兩段 client \u0026lt;---\u0026gt; nginx \u0026lt;---\u0026gt; nodejs server\n分別查看 wireshark 封包\n左圖為 client ← → nginx / 右圖為 nginx ← → server\n因為 nginx keep-alive 設置 timeout 為 65秒，所以 client \u0026lt;---\u0026gt; nginx 處於 persistent connection；\n但是 nginx \u0026lt;---\u0026gt; server 這段是每次 request 都重新建立，預設到 server 這段 nginx 不會建立 persistent connection。\nNginx Proxy — 開啟KeepAlive 且 timeout 大於 nodejs server 簡單修改一下 nginx conf，讓 nginx \u0026lt;---\u0026gt; server 這段也走 persistent connection\n觀察到一個現象是 client ← → nginx 已經由 client 主動斷開連線，但是 nginx 到 server 卻要等到其中一者 timeout 才會斷開連線，雙方都不會主動發送 Connection:close；\n多個 client 發送，n 個 client 會建立 n 個到 nginx connection，但是 nginx ← → server 會用同一條 connection 。\n偶發的502 錯誤 — Keep-Alive Race Condition Tuning NGINX behind Google Cloud Platform HTTP(S) Load Balancer\n** 解决 AWS ELB 偶发的 502 Bad Gateway 错误_ - Timon Wong**\nThe NGINX timeout might be reached at the same time the load balancer tries to re-use the connection for another HTTP request, which breaks the connection and results in a 502 Bad Gateway response from the load balancer.\n看到幾篇相關的問題，主要都是發生 race condition，某一方在剛好收到 [FIN, ACK] 後又收到下一個 request，導致了 tcp connection error 回傳 [RST]；\n解決方法通常是拉長 api server 端的 keep-alive timeout，讓 load balancer 自己斷開 connection。\nTCP Close Connection Transmission Control Protocol - Wikipedia 當 TCP 決定要關閉連線時，需要四次交握，主要是 client → server 與 server ← client 兩個方向都有一組的 FIN , ACK 交換，才能確保說關閉連線後對方不會再送資料；\n假設目前是 A , B 在通信\nA 送出了 [FIN] 表示即將關閉 A → B 的 Connection B 回 [ACK]，表示收到，A → B 就關閉了 但這時候 B → A 還是可以繼續傳送資料，直到 B 送出 [FIN] A 回傳 [ACK]，tcp connection 才真正關閉。 這個狀態是 half-open ，某一個方向關閉但另一向還是通的。\n在某些系統(Linux)的實作上僅支援 half-duplex，當要關閉某一向的 connection 時會連讀取都一併關閉，如果還沒處理完所有的資料，此時 host 會回傳 RST，另一方視此次 request 失敗。\nNginx 優化 nginx优化\u0026ndash;包括https、keepalive等\nkeep-alive 有幾個相關的參數，可以依據服務內容而調整，簡單筆記幾個重點\nkeepalive_requests\n一個 persistent connection 最大的服務 request 數量，超過則強迫關閉 keepalive_timeout\nconnection idle 超過時間就會被強迫關閉 keepalive\n最大同時 connection idle 數量，如果超過則 idle connection 會被回收 結語 keep-alive 優點在於重複利用 tcp connection，但必須注意\nclient library 的實踐，確認是否有主動發送 connection: close 機制，否則 server 都會等到 timeout 才斷開連線，會造成太多不必要的 idle。 nginx ← → server 這段的長連接 timeout 雙方都可以拉長，如果 QPS (query per second) 很高的話，多個 client connection 也會利用同一條 nginx ← → server 的 persistent connection。 如果有使用 GCP / AWS load balancer 偶發 502錯誤，且 api server 端沒有收到任何連線紀錄，多半是 keep-alive timeout 問題。 讓 client timeout 大於 server timeout，因為 request 是由 client 發起，就能有效避免 keep-alive race condition。 ","date":"2019-04-08T23:46:01.643Z","permalink":"https://yuanchieh.page/posts/2019/2019-04-08-http-persistent-connection-%E7%A0%94%E7%A9%B6%E8%88%87-proxy-server-keep-alive-timeout-%E4%B8%8D%E4%B8%80%E8%87%B4%E7%9A%84-502-%E9%8C%AF%E8%AA%A4/","title":"Http persistent connection 研究與 proxy — server keep-alive timeout 不一致的 502 錯誤"},{"content":"以下介紹如何使用 Linux Kernel 內建的工具 tc ，完成\n網路流量管制，當一台主機上有多個網路服務，希望可以分配不同的網路頻寬、封包傳送的優先序等 模擬惡劣網路環境下的狀態，例如封包延遲、隨機性掉封包 環境則是在 MacOS 用 VM跑 Ubuntu 16.04。\n背景知識介紹 Network Layer tc 是基於網路分層中的 Network Layer 網路層進行封包的控制，針對封包的 ip 內容進行篩選與控制。\n網路層，主要關注於如何將源端的封包透過路由器，一步步(hop)正確的傳送到接收端，中間的路由算法等略過不談，這邊主要關注於壅塞控制與服務質量\n壅塞控制 ( Congestion Control ) Congestion 壅塞是指 當主機接收的封包量增加，大於發送的量，此時會被暫存於 Buffer緩存區當中，但如果封包量填滿了緩存區，就會遺失部分封包，網路也開始壅擠。\n避免壅塞的方式有幾種\n增加網路處理的能力 流量感知\n主動將封包繞開壅塞區域，走其他利用率較低的路由器，這會需要修改路由算法，將帶寬、傳送延遲等都一併放入 link state routing 的權重分配，決定封包該送往哪個路由器 准入控制\n如果發現負載過高，拒絕新的連線建立 流量調節\n當路由器發現有壅塞的可能，可以主動發出訊息，再封包上標記，又或是傳送 choke package ，告知發送者應該要降低發送的速度 負載脫落\n如果路由器來不及處理封包，為了避免壅塞會直接丟棄封包；\n做法有 RED( Random Early Detection)，當某個暫存區超過某個閥值，就會開始隨機丟棄封包，此時發送方就會意識到需要放緩發送速度。 服務質量 (QoS，Quality of Service) 除了壅塞控制外，應用程式對於網路的要求也會有所不同，例如說大型文件需要較大的帶寬，但是延遲性與網路抖動沒這個敏感；\n但相對的影音頻直播，對於網路抖動就非常敏感，或許你可以接受一個影片穩定 delay 10秒鐘開始，但無法接受播到一半卡頓。\n服務質量有兩種類別\nIntegrated Service (綜合服務)：\n在封包傳送的路徑上，所有的路由器都保留一定額度的資源，形成專用端到端的數據流，就好比公車專用道就只能由公車行駛；\n好處是能夠很穩定的服務質量，但缺點就是需要非常龐大的資源，且效率不高，具體協議可以參考 RSVP，但這因為難度過高幾乎沒有實踐。 Differential Service (區分服務)：\n對比於綜合服務提供整段網路的服務質量，區分服務僅提供單個路由器的服務質量，也就是在每一次的 hop 間由該路由器保留資源(Per Hop Behavior)，等到下一跳再由下一個路由器預留資源；\n在 IPv4 的封包中，header 包含一個欄位 ToS( Type of Service)，共 8 bits，前 6 bits 表示服務類型，後 2 bits 表示壅塞訊息。 為了提供服務質量，路由器必須決定封包發送順序( Packet Scheduling )以及控制流量(Traffic Shaping 流量整形) 的功能\nTraffic Shaping 主要是針對突發性流量所採取的攤平流量方式，避免一次性塞滿緩存區導致封包掉落，常見的方式有兩種\nLeaky Bucket\n有一個漏洞的桶，桶的尺寸與出口的最大流速固定；\n只要上方有水注入，桶就會自動從出口出去，如果注入速度高於出口流速，就會開始在桶中累積，如果滿了就溢出。 https://www.geeksforgeeks.org/leaky-bucket-algorithm/\n2. Token Bucket\n桶子內改存放 Token，如果桶子滿了就不再放 Token；\n如果此時有 packet 要發送，就必須從桶子中消耗一個 Token，如果沒有 Token 則 packet 不能發送。\n對比 Leaky Bucket優點在於 - Leaky Bucket 桶子滿了會開始丟封包，而 Token Bucket 則是丟棄 Token\n- 當流量低時，Token Bucket 有機會可以累積 Token，等到高峰時一次性使用掉，而 Leaky Bucket 無法做資源的保留\nhttps://gateoverflow.in/39720/gate2016-1-54\nPacket Scheduling 當路由器一次收到多個服務質量請求的封包時，會需要 packet scheduling 機制處理發送的先後順序，最基本的就是一條 FIFO 的Queue，所以的封包都按順序排隊發送，但是這無法提供良好的服務質量保證；\n另一個是 Fair Queueing (公平隊列)，每個 data flow 都獨立一個 queue，路由器以輪播的方式，輪流發送封包；\n在此之上還有 WFQ (加權公平隊列)，每個 Queue 權重不同，權重高的發送頻率會高於權重低。\n小結：Network Layer 提供壅塞控制與服務質量的控制，而服務質量下流量整形與封包調度機制的實作，也就是後續要用 tc 控制的部分\n常用網路指令 找出裝置上目前的 network interface：\nNetwork Interface Card(網路卡) 一端是連接通訊介質(乙太網、Wifi等)，另一端則是提供主機的網路操作介面(network interface) 1 2 3 4 5 // 在 mac os 上 $ networksetup -listallhardwareports // 在 ubuntu 上 $ ip link show 2. 觀察某 network interface 流量\n參考此篇文章 18 commands to monitor network bandwidth on Linux server，挑選了兩個不錯的工具 speedometer 用長條圖即時表現封包流量、tcptrack 觀察 tcp 連線狀態，方便後續實驗量測\n3. 找出 domain name 對應的 ip\n因為 tc 是網路層工具，所以要篩選封包需要透過 ip ，透過 $host {domain} 找出 ip。\ntc 基本介紹 tc 的流量控制主要由四個概念組成\nShaping ：控制封包的傳輸速度 Scheduling：控制封包的發送順序 Policing：收到封包後的處理機制 Dropping：流量超過帶寬後的丟封包機制 實作上有三個物件\nQdisc (queueing discipline)：\n當 kernel 想要透過 network interface 發送 package 時，會先全部被放在 queue 上面，預設是 pfifo，不做任何處理單純的 First-In First-Out。 Class\n有些 qdisc 可以定義 Class，Class 主要用於做流量的管制，支援 Class 的 Qdisc 又稱為 classful qdisc，每個 Class 又包含自己內部的 Queue，所以能組成層狀的網路流量控制 Filter\n決定 package 要送往哪個 class，每個 filter 是綁定在 qdisc 上。 幾種 Qdisc 介紹 qdisc 有非常多種，這裏挑幾個簡單介紹，屆時可依照屬性選擇使用\nclassless qdisc pfifo / bfifo\n針對 Packet / Byte Buffer 容量限制的 First-In First-Out Queue，這種 queue 並不會對封包有額外的操作，收到就依照順序轉發，超過 Buffer 就丟掉，這是最簡單的 qdisc。\n使用上如 sudo tc qdisc add dev enp0s5 root pfifo limit 1000 pfifo_fast\n預設的 qdisc，內建是三條 pfifo(稱為 band)，會依據 IPV4 header 中的 ToS 轉發到對應的 band，如果 band 中有封包，會從低順位的 band 開始發送，直到清空後依序換高順位的 band classful qdisc tbf\n也就是 Token Bucket 實作，主要用於流量整流增加平均流速 0.5mbps 高峰 1mbps+Buffer空間 5kb\ntc qdisc add dev eth0 handle 10: root tbf rate 0.5mbit burst 5kb peakrate 1mbit prio 非常類似於 pfifo_fast，同樣預設有三條 band，但優點是可以自訂 priomap，決定 ToS 要對應到哪個 band (pfifo_fast 無法自定)；\n另外也支援 tc filter，用其他的條件決定封包轉發到哪個 band。 htb\n用於控制 outbound 帶寬流量，基於 Token Bucket 實作 * 補： working-conserving scheduler:\n每種 qdisc 可分為 working-conserving 或是 non-working-conserving，working-conserving 表示不會讓資源 idle下來；\nnon-working-conserving 則是會在某些條件下延遲，可用於消遲抖動 (jittering )或是需要預期性的排程。\n例如說 pfifo 收到封包後，就會立即送出；\n對應的 tbf 就是 non-working-conserving，收到封包後，會看是否有 token 才會送出封包，並不一定是立即發生。\ntc — 延遲封包 透過基本的 classless pfifo qdisc，實作：\n透過封包延遲，放緩 http連線到 stackoverflow 透過 bandwidth(帶寬)限制，調整 aws s3 上傳的速度 以下指令參考\n1. 列出目前所有的 network interface 的設定 1 2 3 $ tc qdisc ls qdisc noqueue 0: dev lo root refcnt 2 qdisc pfifo_fast 0: dev enp0s5 root refcnt 2 bands 3 priomap 1 2 2 2 1 2 0 0 1 1 1 1 1 1 1 1 tc-prio 是預設的隊列規則，如果有綁定新的隊列規則會直接覆蓋過去。\n預設 prio 會有三個 band，而 priomap 則表示對應 IP 封包中的 4bits TOS 欄位，將封包透過該 band 發送。\n需注意 enp0s5 是我自己的 network interface，記得替換成自己裝置上的 network interface\nhttp://linux-ip.net/articles/Traffic-Control-HOWTO/classless-qdiscs.html\n2. 預設將所有的流量導向 band 2 為了避免其他封包被影響，先將所有的封包都走 band 2(指令計數從0開始)\n1 $ sudo tc qdisc add dev enp0s5 root handle 1: prio bands 10 priomap 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 handle 1: 指的是綁定到 qdisc 的 root\nbands 10: 可以創建 10組 band\n3. 設定封包延遲 $ sudo tc qdisc add dev enp0s5 parent 1:1 handle 10: netem delay 100ms 10ms\nnetem 是 tc 的工具之一，可用來增加延遲、掉封包、重複封包等模擬工具，netem delay 100ms 10ms 表示每個封包延遲 100ms(+- 10ms) parent 1:1 表示在 class id 1 底下，建立一個 id 為 1的子節點，因為當前的隊列沒有多層次的 class設計，所以 1:1 就對應到 band 0 handle 10: 表示創建一個 class id 為 10 的節點 雖然目前有增加了 band0 的延遲，但因為所有封包都是走 band2，還不會受到延遲的影響，先用工具量測目前的網路延遲，這邊我是用 curl\n1 $ curl -o/dev/null -w \u0026#34;\\\\n%{time_connect}:%{time_starttransfer}:%{time_total}\\\\n\u0026#34; -s [https://stackoverflow.com/](https://stackoverflow.com/) 在設定前不專業量測約為 1.5 s。\n4. 加入過濾器，將符合條件的封包轉送到 band 0 $ sudo tc filter add dev enp0s5 protocol ip parent 1:0 prio 1 u32 match ip dst 151.101.0.0/16 match ip dport 443 0xffff flowid 1:1\nprotocol 可以指定 ipv4 , ipv6 , tcp 或udp prio queue 的 filter 只能綁定在 1:0 底下，所以 parent 1:0 是固定的 u32 是 tc 的工具，主要是針對 ip 內文做篩選，例如 match ip dst 表示對封包的目標位置坐篩選；\n可以設置多個 match 條件 flowid 1:1 表示此 filter 綁定到 band 0 上 多個 filter 可以綁定到同一個 band 上 設定完之後， 響應時間變成 12秒!\n因為 netem 是針對 package 做 delay，一個 http(s) request 會來回多次，每次傳送不等多的 package，所以 每個 request delay 時間會遠比 1s 長。\n使用 prio 非常方便的是，底下的 band 可以再另外掛其他的 qdisc，例如說加上 tbf 就可以限制帶寬，加上 netem 就可以流量整形，而且 band 數量可以自行定義，相當好用。\ntc — 帶寬限制 剛才使用 pfifo qdisc，搭配 netem 做基本的流量控制，但如果需要控制帶寬，就需要用其他的 qdisc，配置 Token Bucket 或是 Leaky Bucket。\n以下使用 classful qdisc HCB。\n測試限制 aws s3 上傳的帶寬\n1. 刪除舊的 qdisc 設定 1 $ sudo tc qdisc del dev enp0s5 root 2. 建立 htb 1 $ sudo tc qdisc add dev enp0s5 root handle 1:0 htb 3. 加入 bandwidth 限制 1 $ sudo tc class add dev enp0s5 parent 1: classid 1:1 htb rate 100kbps ceil 100kbps 比對 s3 上傳的前後，發現上傳速度確實被限制在 100kbps\n如果單純想要限制上傳速度，也可以用 tc-tbf，但 htb 優點是用層狀架構限速，leaf 會受到 root 的限制，例如分配一條帶寬 10 Mbps，可以在之下分配 4 Mbps 給某網域 6 Mbps 給其他網域等等。\n筆記會用到的其他指令\n使用 dd 創建指定大小的檔案\nif: 檔案內容來源，/dev/zero 是特殊檔案，讀取時會提供無限的空字元\nof: 檔案內容輸出\nbs: 每次寫入的區塊大小 (byte)\ncount: 總共寫多少個區塊\n$dd if=/dev/zero of=file.txt count=1048576 bs=1024\n這樣會產出 file.txt 於當前目錄並佔 102MB aws-cli 上傳檔案\n$aws s3 cp file.txt s3://{bucket 名稱} aws s3 在上傳檔案會用多個 thread 與 ip，所以要限制比較困難。 參考資料 網路相關參考自《计算机网络(第5版)》一書。\n","date":"2019-04-05T12:20:15.159Z","permalink":"https://yuanchieh.page/posts/2019/2019-04-05-linux-traffic-control-tc-%E7%A0%94%E7%A9%B6/","title":"Linux Traffic Control (tc) 研究"},{"content":"最近在教人寫程式，發現自己雖然有寫部落格的習慣，但是要如何用口語表達卻還有很大一段的落差，更困難的是「我不知道從何教起」\n先前看到有人分享到 大腦其實是善於記憶，因為思考太花能量，所以大腦經過反覆的練習就會將成果內化為記憶；\n而教學就是加速這個過程，並提供學習方更有動力，希望可以自己上完課後可以更懂得如何教其他人用快樂的方式學寫程式XD\n電腦科學 Computer Science 在做什麼 提到電腦科學，大家的刻板印象通常是 一個帶著反社會人格的帽T男瘋狂敲著電腦寫程式，但其實電腦科學專注於 解決問題\n1 input -\u0026gt; [ ] -\u0026gt; output 輸入我們想要解決的問題，通過中間黑盒子的運算，我們得到問題的解法。\n數人問題 今天假設要數幾個人來上課，我們可以在黑板上，用「正」字符號表達，一個人頭代表一比，最後累積起來看有多少人；\n但如果沒有黑板，我們可以用手指頭來數，手指升起代表加一，但是一隻手指有五根手指頭，這樣我們的上限只能數到五，這樣似乎有點少 ?!\n回過頭來思考，在人類世界常用 10 進位制，一個位數只能表達 0–9，但如果我們加入進位的概念，例如 123，這代表的是\n123 =\u0026gt; 100 * 1 + 10 * 2 + 3 * 1 = 123\n同樣的如果數手指用不同的 模式 表達，例如說手指舉起代表 1，手指收下代表 0，並加入了進位的概念，這樣的話數手指可以表達的數字範圍變得更大了，例如我想要比 9可以用\n01001 =\u0026gt; 0*16 + 1*8 + 0*4 + 0*2 + 1*1 = 9(十進位)\n數手指的表達範圍瞬間從 0–5 變成 0–31，這也就是二進位的表示法。\n除了用手指表示，我也可以用手機的手電筒開光代表 0 / 1，眾所皆知目前大多數電腦只能分辨 0 / 1，也就是電源的開與關，或更精準的說是電晶體狀態的表示；\n而但一一個位數 0/1 我們稱為 位元(binary / bit) ，五根手指頭代表我們用 5個 bits來運算，而 8 bits 會稱為 位元組(byte)。\n但對人類來說，兩者數字想表達的概念是一樣的，但 (二進位)01001 還是不太直觀，此時我們可透過轉換公式變成我們好理解的表示法 (十進位) 9。\n數字如何表達文字 人類除了數字外，我們還想要傳遞文字訊息，例如說 ABCD….，單純用多個0,1該如何表示呢？\n剛剛提到，我們可以將 01001 轉換成 9，那同樣的我們修改轉換公式，就可以輸出不同的結果。\n在電腦世界中，最基礎的編碼方式是 ACII，透過 8 bits 為一組表示，總共 對應128個常用的美國輸入字，例如說 A 是用 65 表示，也就是 0100 0001，當電腦收到後會自動以 8個bit為單位切割，透過一張轉換表，將二進位轉換成文字。\n73 72 33 =\u0026gt; 收到這三個字用 ascii 轉換會變成什麼呢 ? =\u0026gt; HI!\n但如果是繁體輸入，中文字遠遠超過 128個，又或是其他國家不同語言，ascii 顯然遠遠不夠用，後來出現了 unicode 萬國編碼，用更多的 bits 表達一個字，解決文字顯示的問題。\n數字如何表達圖像 同樣是收到很多的 0/1，這時候我們需要不同的轉換方式，常見的色碼系統是 RGB，用三個數字 表達紅、綠、藍的深淺程度，從0–255，三者混合就變成了色彩；\n為了表達 0–255，我們需要 8 bits。\n所以剛才的 (73, 72, 33) 在 ascii 中代表 HI!，但是在 RGB中代表的是淺黃色。\n在螢幕上，一張圖片其實近看是由許多許多的像素 pixel組成，一個pixel顯示一個顏色，也就是3個一組的8bits。\n而影片，則是由一張張圖片連續快速播放所構成。\n抽象化 剛才提到了電腦用二進位，但為了人類的需求，我們又把 0/1 轉換成不同的表示法，例如說有 十進位轉換成易讀的數字、ascii轉換成文字、RGB轉換成顏色，這個轉換的過程我們稱為「抽象化」\n抽象化的概念在電腦科學非常的基本與重要，抽象化是指\n我們不用管底層的實作方式，注重於概念上的表達\n例如說 一個像素是由 3 byte 組成 -\u0026gt; 多個像素可以組合成一張圖 -\u0026gt; 多張圖快速播放可以組成一部動畫，透過一層一層的抽象化，可以做到更多的事情。\n同樣的，平常我們在使用計算機算數學、上網傳訊息、看圖片看影片，我們完全不用去管背後是用多少個 0/1 去表示，只要電腦可以自動幫我將 binary 轉換成我需要的顯示就好了。\n總結以上，電腦最終只看得懂 0/1 ，而對於人類有意義的是 數字、文字、像素等，可以透過不同的轉換方式構成不同的表達 Expression，也就是 Input / Output 的格式。\n演算法 剛才提到，電腦主要功能是在於解決問題\ninput -\u0026gt; [ algorithm ] -\u0026gt; output\n中間的運算過程稱之為 演算法 algorithm\n電話簿找人 今天桌上有一本電話簿，裡面按照英文字母順序排列，希望可以找到「Mike Smith」的電話號碼，此時我們可以怎麼做?\n從第一頁，一個一個查閱，但這會非常花時間，尤其是電話簿如此大一本 兩頁兩頁翻，如果翻到過頭就在往回翻一頁，這個做法會比第一個省下一半的時間 直接翻到電話簿的中間，如果目標的英文字母順序在後半段，我只要翻後半段的電話簿就好，另一半直接丟掉，反覆這個過程，就可以非常快找到；\n這也就是 二元搜尋法 Binary Search 在電腦科學中，通常我們會說第一種是暴力解，就是用最直觀最原始的方式解決問題；\n但通常這樣暴力解效率都不高，會開始逐步優化，優化有很大的方向是 拆分擊破 devide and conquer，將大問題拆解成小問題，並透過持續解決小問題就可以得到最後的答案；\n例如説翻電話簿，我先從中間對切，問題量(需要查閱的電話數)就剩一半了，再持續對切，問題越來越小，直到找出答案。\n我們可以用數學函式圖形表達演算法的效率，x 軸代表問題需要花費的步驟，而y軸代表問題的數量\n以電話簿找人為例，y軸代表的是電話簿的厚度，x軸的話舉第一種演算法來說，代表説電話簿多厚，演算法就需要翻閱多少頁才能解決問題。\n這邊僅簡略帶出 演算法複雜度的概念\nPseudocode 剛才三種演算法都有一個解決問題的流程，我們可以用 偽代碼 pseudocode ，用一種類似於草稿的概念，表達我們實際解決問題步驟。\n圖片出自於影片內容\n這裡可以細拆成四個部分\nFunction： 代表某種動作 pick up / open / look / call / quit Condition 判斷機制 if / else if Boolean Expression 實際判斷的條件 if Smith is earlier in book Loop 持續的做某件事 go back to step 2 Variable 暫存結果的變數 Thread (進階) 多工處理 Events (進階) 事件觸發 Let’s Write Some Scratch! Scratch - Imagine, Program, Share\nScratch 是 MIT 開源的一套用圖形介面程式語言編輯器，透過拖拉的方式，用最簡單的方式就可以寫程式，看到某些範例，覺得功能很完整，能夠發揮的空間十分的大。\n以下挑選幾個範例做說明\nScratch 註冊後主要有三個區塊，右邊的執行結果頁、中間的程式邏輯、左邊的程式元件，設計的都相當直覺。\n在開始 Scratch 時，都會透過綠色的旗幟符號開始，所以程式碼會先放上圖橘色區塊開始執行，後續依照積木的順序，執行你設計的演算法，例如最基本的「Hello, World!」\nHello, World! 是程式界在嘗試新事物的第一個慣例測試，主要是因為某位大神寫了一本非常知名的書，裡頭的第一個範例就是 Hello, World! ，後續就演變成一種文化。\nLoop 我希望發出喵聲4次，中間相隔一秒鐘，可以看到以下有兩種寫法\n左邊是很直覺的寫法，我要 Meow 四次我就複製貼上四次，但這樣最大的缺點是如果要十次同樣的事情就要做十次…. ，又或是我想要變成 Woof，就需要改十次；\n遇到重複做某件事數次，可以改用 Loop 的概念，讓程式自動重複而不用一個一個的自己手動作。\nDon’t Repeat Yourself (DIY)\n是寫程式一個很重要的原則。\nCondition 這裡我寫的邏輯是\n定義一個變數 counter，在開始執行後將 counter 設為 0，接著當每次按下 Space 後，顯示 counter 數值，接著將 counter 每次都加 1；\n如果目前 counter \u0026gt; 10，就不要發出聲音，反之 Meow 一聲。\nFunction 類似於上面的事情，但我這時候希望可以目前 counter 多少就 meow 幾次，此時我們需要 Function 的幫忙；\n先定義一個自己客製的 Block，這個 Function 的內容就是 Repeat Meow 數次，次數由參數 n 決定。\n當然也可以直接把 repeat 那一整段直接塞到 else 當中，但是Function 的好處是可以將某些行為從程式碼中抽取出來，這樣可以讓程式碼更好理解，閱讀到 MeowManyTimes 可以從字面上的意思很直覺想到這是跟發出 Meow 有關。\n","date":"2019-02-19T00:00:17.464Z","permalink":"https://yuanchieh.page/posts/2019/2019-02-19-cs50-lecture-0/","title":"CS50 — lecture 0"},{"content":"最近陸陸續續在看 AWS re:invent 2018 的影片，主要專注於 CI/CD部分，不得不說看完真的是非常振奮人心啊！\n以往在做架構設計，可能是先畫個架構圖，接著打開網頁登入 AWS Console，看要開幾台 EC2 、VPC 架構、 ALB 設定、CDN 設定、開 S3 Bucket 等等，基本上大多的事情都是透過網頁完成，或是使用 Command Line Tool 完成；\n但這樣的缺點是如果公司突然想要重建一份同樣的架構當作 staging 環境，又或是往後需要改動架構，此時必須自己同步文件、修改架構圖等等，在管理上有相當多不方便的地方。\n為什麼不用程式碼來管理 這也就是這篇要分享的方法，使用 AWS-CDK，AWS-CDK 是一個 nodejs 的 package，安裝後當作 cli tool 使用；\n在 AWS-CDK 下提供各個 AWS 既有服務的套件(如 lambda 、 S3 等)，目前提供 C#、Javascript、Typescript、Java，開發時引入這些套件就可以使用了。\n透過程式碼管理，我覺得有幾個好處\n更好的封裝與結構\n如果是用網頁做設定，基本上行為只能透過文件來溝通，如果是用 shell script 或許也可以做到一部分的自動化，但整體理解上還有蠻多不方便之處，更別說維護的人寫法可能不同，造成維護的困難；\n使用 AWS-CDK，他本身是用 OO 的概念封裝既有的 AWS 服務，官方也有提供工具幫助初始專案，也給予一定的程式碼架構建議，後人接手一定會好理解非常多 版本控制\n隨著公司發展，可能會更動架構，透過代碼管理工具可以協助良好管理架構的演進，Rollback 也可以有依據 (例如費用爆掉的時候…) 跨區域部署\n如果你想要跨區域部署同樣的架構，用網頁操作應該會非常頭痛，而且容易出現人工錯誤；\n用程式碼就是多一行指定區域， DONE! AWS-CDK 底層其實是將程式碼轉成 Cloud Formation，AWS 自定義的服務定義語法，可以理解成 AWS服務的 Assembly Code。\n＊WARNING：目前發文時間 2019/01/27 還不建議使用於正式環境，但因為他底層是轉成 Cloud Formation，理論上轉換不出錯應該就蠻穩的，目前測試尚未遇到 Bug\n在開始 Coding之前，建議使用 AWS-CDK 之前要先熟悉 AWS 服務，例如你想串 Lambda 建議先至少看過文件、用網頁版完整操作過一次，因為有很多參數實際用過才知道怎麼填寫，目前官方網站有良好的文件，但這些文件僅限於參數說明，如果沒實際用過不會知道參數設定後的實質意義與用途；\n另外目前我查了一下沒有太多實戰分享，只有一些基本的串接與操作，所以在服務間的串接蠻多是 Try-And-Error 玩出來的，也是我想寫這篇文章的目的，幫助大家上手。\n本篇比較適合已經在使用 AWS 服務一段時間，希望可以用更好方式管理架構的工程師；\n後續會使用 Typescript 當作範例，不過概念都是一樣的。\n最後再次提醒，因為還在 pre-release 階段，如果你發現以下程式碼有誤或無法執行，煩請留言。\n安裝 AWS-CDK 與專案初始化 cdkworkshop.com\n在開始之前，請確保完成官網的 prerequest 步驟\n安裝了 aws-cli 完成 iam role 設定，實驗性質先開 admin 權限比較方便 完成 aws configure 安裝 Nodejs v8.12 以上 安裝 aws-cdk， npm install -g aws-cdk@0.22.0 aws-cdk 主要提供幾個指令\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // 用於初始化專案，可以指定語言 $ cdk init // 查看目前所有的 stack (後續介紹什麼是 stack) $ cdk ls // 查看目前程式碼轉換的 cloud formation $ cdk synth // 查看目前架構的變動 $ cdk diff // 部署 $ cdk deploy // 初始化環境 $ cdk bootstrap 目前我習慣使用 Typescript 開發，搭配 VSCode 有連好的自動提示非常方便。\n實戰：部署 Scheduled Event 觸發 Lambda Let’s write some code~ 這次做一個部署到多個區域，定期 2 分鐘去打 https://www.president.gov.tw/ 看台灣總統府頁面在全球的網頁回應速度，接著發訊息到 slack 頻道。\n到專案目錄下，執行 \u0026gt; cdk init app — language=typescript\n接著同個目錄下，我們需要開兩個 shell，一個 shell 執行 \u0026gt; npm run watch ，主要是為了自動轉譯 typescript 成 javascript 才可以讓 nodejs 執行；\n另一個 shell 用來執行後續的 cmd。\n在目錄下會有常見的 nodejs 專案檔案，有兩個資料夾比較重要 /bin 、 /lib；\n/lib 主要是放架構定義，初始化之後會有一個 aws-cdk-stack.ts，在 AWS-CDK中， stack 代表的就是一個架構的 class，例如說目前要設計的架構就是 (scheduled event + lambda)，這就是一個小型的架構；\n後續可以讓小架構組合成大架構，就看需求決定如何封裝。\n/bin 主要是放 app，也就是最終架構的檔案，主要是整合Stack 初始化與部屬的方式，定義在 aws-cdk.ts 之中。\naws-cdk-stack.ts 是不是比想像中的清晰簡單啊，載入對應模組宣告即可，而且 lambda 可以參照本地端的資料夾，自動幫忙打包與部署超方便的；\n另外 aws-cdk 文件一個好處是都有把對應的參數文件對照好，所以查起來蠻快的，但是參數量非常大 …..\nStack 在初始化需要綁定是在哪個 app下，以及獨立的 ID，這後續會再網頁中看到。\naws-cdk.ts 基本上就是初始化 Stack 並定義部署的區域，另外的 lambda 與 slack 就看 git repo 參考，不再贅述。\n程式碼完成後，執行以下步驟\n檢查\n\u0026gt; cdk list 查看App下的所有 Stack，接著針對各個 stack 下 \u0026gt; cdk synth AwsCdkStack2 ，大概檢查有沒有 error 與 cloud formation 定義。 部署\n\u0026gt; cdk deploy 如果是第一次部署，他可能會跳出需要執行 \u0026gt;cdk bootstrap {專案id/region} 的錯誤，其他的話就等他慢慢部署； cdk 部署時有良好的打印訊息，最棒的是 cdk 會自動處理 aws 服務 iam role 的權限，如果有處理過權限設定就知道這非常的頭痛 XD 因為 AWS 服務切太細了，好處是可設定的非常精準，但缺點就是查找起來很頭痛，通常都要跑個幾次看錯誤log 補權限。\n以下是網頁與 Slack 回應的截圖\nlambda\nSlack 訊息，看來歐洲回應時間最長，東北亞最快，結果是也不太意外 XD\n最後附上完整專案的 github\nsj82516/aws-cdk-demo\n刪除 Stack 假設我想要移除美西的部署，不要直接從程式碼刪除Stack 重新 deploy ，我實測的結果這樣是不會幫你把東西刪掉的，要改用以下步驟\n\u0026gt; cdk destroy AwsCdkStack 接著才從程式碼刪除 結語 在雲端服務中，AWS 不得不說是這個行業的領頭羊，而且就技術上個人覺得領先 GCP 不少，基礎服務不好說，但是在跨服務的整合上，AWS 做得非常良好，這或許才是企業用乎所在乎的未來，整套 solution 而不是單個單個服務還要自己慢慢整。\n有了 AWS-CDK 要分享、重用架構就極度方便，之後再來做一系列基於 AWS-CDK 的 AWS 架構研究與分享。\n","date":"2019-01-27T03:29:50.283Z","permalink":"https://yuanchieh.page/posts/2019/2019-01-27-aws-cdk%E6%95%99%E5%AD%B8-infrastructore-as-code-%E7%94%A8%E7%A8%8B%E5%BC%8F%E7%A2%BC%E7%AE%A1%E7%90%86%E6%9E%B6%E6%A7%8B/","title":"AWS-CDK教學 — Infrastructore As Code 用程式碼管理架構"},{"content":"Forcing a device to disconnect from WiFi using a deauthentication attack\n內容參考 @Brandon Skerritt 的創作，以下僅為個人轉譯成中文，執行平台從作者的 Kali Linux 改為 Mac，紀錄一些執行的細節。\n緣由 https://www.itcodemonkey.com/article/4742.html\n當 Suppliant(終端裝置) 希望與 AP 建立 Wifi 連線時，需要經過四次 Handshake，而在第三次交握時 Suppliant 會輸入 Wifi密碼，這個 Wifi 密碼會經過加密，如果密碼正確則成功建立連線，反之則斷開連線。\n而 deauthentication attack 利用這樣的交握特性，在第三步假裝是 AP 回覆給 Suppliant驗證錯誤，因此強迫裝置斷開 Wifi 連線，這並不是透過什麼漏洞攻擊，而是 Wifi 傳輸協定原有的機制。\n這樣的機制使得 Wifi 非常的不安全，deauthentication attack 可以中斷任意裝置的 Wifi連線，這往往只是攻擊的第一步；\n一般裝置 Wifi 斷線後，會用同樣的密碼再次嘗試與 AP建立連線，而此時 Wifi 無線通訊的機制，會使得相鄰的用戶也可以偷聽到這些封包，進而拆解封包，從中暴力破解 AP 的密碼；\n又會是邪惡的用戶偽造一個假的 AP，其他人不明所以就傻傻的連線，透過簡單的釣魚，就可以完成 Middle man attack。\n看似邪惡的工具，但不要用在破壞也能用在自保上，在 2015 年爆出多起 Airbnb 透過 Wifi Cam 偷窺住戶，作者寫了一個 script 可以自動斷開所有的 Wifi Cam\nDetect and disconnect WiFi cameras in that AirBnB you\u0026rsquo;re staying in\n同作者還寫了可以自動斷開 Google Glass 的 Script\nFind a Google Glass and kick it from the network\n*警告 這項有力的工具可以用來保護個人隱私，但作者不鼓勵用於非法或是惡意的攻擊!\nDeauthentication Attack 首先要確認兩件事\n準備要斷開的裝置 該裝置連結的 Router 原本是希望找到對應的 Cmd Tool 在 Mac 平台上，但後來參考此文章\nWPA wifi cracking on a MacBook Pro with deauth\n可以直接用 JamWifi GUI Tool，打開後開始 Scan，Scan 後可以找到 AP 並透過 MAC Address 指定裝置斷線，我在家裡嘗試過是可行的。\n在同篇文章中有提到後續破解 Wifi 密碼的方式，透過 deauthentication attack 斷開裝置，接著開 tcpdump 監聽所有的封包；\n接著用 cap2hccapx將 tcpdump 的資料轉換格式；\n最後用 hashcat 類似暴力破解的方式，嘗試還原被 hashed 過的密碼，這裡他的演算法可以輸入 wordlist，網路上有人提供常見的 wifi 密碼，雖然說 wifi 密碼的完全字元(a-zA-Z0–9)可能性是 62 ^ 8 以上，但是利用人類的惰性，其實可能的組合沒有這麼多，這可以省下非常大量的計算時間。\nberzerk0/Probable-Wordlists\n_Version 2 is live! Wordlists sorted by probability originally created for password generation and testing - make sure…_github.com\n可以看一下自己的密碼有沒有在上面 XD\n設定密碼不要用 12345678 或是 0000000 ，另外要記得常換密碼\n","date":"2019-01-20T01:12:22.083Z","permalink":"https://yuanchieh.page/posts/2019/2019-01-20-%E5%A6%82%E4%BD%95%E7%94%A8%E8%A7%A3%E9%99%A4%E6%8E%88%E6%AC%8A%E6%94%BB%E6%93%8A%E5%BC%B7%E8%BF%AB%E8%A3%9D%E7%BD%AE%E6%96%B7%E7%B7%9A-wifi-%E9%80%A3%E7%B7%9A/","title":"如何用解除授權攻擊強迫裝置斷線 Wifi 連線"},{"content":"開始學泡茶也學了三個月，想說培養興趣最困難的其實是第一步，在學茶的過程有些經歷蠻有趣，覺得分享出來供其他人對茶葉感到好奇的路人，跟我一起入門體驗喝茶的樂趣。\n首先要澄清一件事，喝茶在台灣的刻板印象可能是老人的行為，但我自己覺得喝茶是一種飲品，重點就是喝得開心，接著才是近一步細細品嚐其中滋味，與背後的典故與文化；\n喝茶跟喝咖啡應該都是件很棒的休閒，不過確實喝茶風氣在年輕一輩似乎不太盛行，略感可惜 XD\n如何開始 我本人是完全零經驗，小時候也只是老爸會泡烏龍茶，飯後跟親戚坐著閒聊，但其實就沒有很講究\n後來隻身到台北工作，覺得在工作之餘可以培養不同的興趣，所以臨時起意就上網拜 Google，一個人就硬著頭皮報名了台北書院的茶藝課程\n茶藝課程 - 臺北書院\n一期六堂課 3600元，目前是上林莉書老師的課程，老師非常有趣，上課會從最基本的茶席與茶具介紹起，這裡就不一一細講，接著後續上課老師會依照學生進度，上不同的茶類，最基本的六大茶 綠、白、黃、青、紅、黑，目前我上到第二期上了前面四種，在學習茶類時老師會準備書面資料介紹典故與產地等，以及製茶的過程，讓學員有更全面的了解；\n接著會試茶，三公克茶葉用鑑定杯泡兩分鐘(仿間有不同的時間)，接著一一品茶。\n茶葉介紹 先跳出來介紹一下茶葉，茶葉基本可以分 茶樹種與製程\n茶樹種是原料，例如說常聽到的金萱(台茶十二號)、紅玉(台茶十八號)、鐵觀音、青心大右等，這些是茶樹種； 接著所謂的綠茶、紅茶、烏龍茶(青茶)、鐵觀音、包種茶代表的是不同的工藝製程，大抵上是發酵程度的不同，綠茶走無發酵直接殺青，所以強調的是香味；其餘不同的發酵度與焙火決定茶的不同風味，之後等我了解後再細說。\n所以你可能會喝到 金萱做的綠茶、紅茶或烏龍茶，所以下次人家請你喝烏龍茶也可以問是哪種茶樹種，這也會決定茶葉的風味\n茶菁(也就是茶樹上的芽葉)，製茶的原料會影響成茶的風味，而茶樹的生長受到地理氣候的影響，所以有些知名的地點會標記如梨山烏龍、阿里山烏龍；\n另外採茶的時機很重要，所謂「春茶做香、冬茶做水」，春冬兩季的茶葉品質通常最好，價格當然也最貴。\n所以買茶葉記得問 什麼製程、什麼茶樹種、什麼時候採收、年份多久、發酵度如何種種，甚至當天採收的時間跟氣候都會影響\n上課主要是學不同的茶，例如說白茶，又可細分不同等級 白毫銀針、白牡丹、貢眉等，要學會分辨就需要不斷的比較與學習，仔細去品嚐入鼻的香氣與入喉的韻味，這部分我真的就很弱，老師常常說我是不是舌頭壞掉 … XD 但我還是樂在其中阿，至少慢慢有在進步，另外有趣的是每個同學的香味記憶都不同，有些人會描述花香果香，但其實我都聽不懂，因為我完全沒有聞過…\n泡不同的茶，再搭配不同的茶壺、茶杯、不同的水溫、沖泡時間、回沖次數等，都有不同的風味，上課的另一個好處是老師有多種的茶具，以及同學間可以彼此交流。\n整體上品茶其實蠻個人，就是要多看多喝，逐步找出自己喜愛的茶。\n買茶具！ 上課上了一期決定入坑，但是身為初心者，最煩的就是完全沒有經驗不知道怎麼挑，五花八門的器具該怎麼買，尤其是看到價位也不知道是高是低，不想買太差但又不想當盤子，一整個超糾結的 XD\n那就我個人經驗，給大家一些參考，我是在淘寶雙十一買的，台灣也有很多茶行或是鶯歌都有賣，之前去台南路過很多間神秘的書局也有在賣\n第一把朱泥壺買 1500 台幣，我在臺灣問 2000 左右是基本款，這些是手作壺的價碼，如果是機器做在鶯歌逛時大概就3、500一支；\n聞香杯、品茗杯、公道杯是瓷器，大概一個杯子100元左右；\n其他的茶針、茶則就看個人，也大約1、200而已。\n總共加總大概2500~3000 搞定基本的茶具組，當然只是入門的開始 ……\n更：茶海喜歡玻璃質感的話，建議買雙層隔熱的玻璃茶海，不然很燙手\n我有問老師怎麼挑壺，老師就是說看得順手花得下去就買，這真的都要一直買一直試才知道。\n泡茶的器皿大概兩種蓋杯跟茶壺，材質可分 瓷器、朱泥、紫砂、陶，材質大致上因為孔隙的大小，會對茶葉有不同的修飾；\n像是瓷器孔隙非常小，所以最能還原茶的原味，不加分不減分，試茶的時候最好用瓷器；\n但有些茶火味太重，或是有人偏好圓潤口感會用其他的茶器，因為孔隙會吸附茶味，所以味道不會太衝。\n買茶葉 買茶葉一開始要入門好難啊，之前有去木柵的茶園走走，可是看到茶行就不太敢進去，可能跟一般的消費習慣去逛百貨公司、大賣場不同，價格看起來不透明不敢進去，本來有想要網購就好，BUT\n茶葉一定要試喝，不要隨便買！\n要如何評斷茶的好壞與對應的價格呢 ? 我在一本書上面寫到\n「台灣茶農每台斤生產的茶葉所需要負擔的成本機器採收約 270元，手工則是 500~1000元」\n如果再算上加工、通路、行銷等成本，我也不知道明確價格但至少有個底，通常換算一斤約 2000 元上下，但每次買大約都是 75g / 150g 的份量，也就是 400~500 元一罐我覺得是合理的價格，也都沒有踩雷過；\n低於這個標準可能是混茶，摻入不同國家如越南的茶葉，這部分因為茶葉屬於加工食品，所以當原料低於某個比例可以不寫產地何處，小心不要受騙。\n不同茶類有不同風味，但是大抵上好茶必須要入喉舒服、會生津不會澀，像鐵觀音等有時焙火中會有很強烈的火味(恕我詞窮)，那種味道不見得大家能接受，但好茶就是不會澀，入喉後口水會分泌(生津)，過了幾秒唇齒依然留香；\n這算是最基本的標準，最好的方式就是少克數長時間沖泡 (三克五分鐘)，等茶泡久了又放涼，好處壞處一覽無遺。\n喝到不好的茶葉對身體不好，例如我之前就喝到刮胃，不好的體驗可能也會中斷興趣的培養，個人建議不敢去茶行買的話可以去茶館，在台北的永康街有非常多間，內用一壺大概200上下，喜歡在買\n個人之前去過台南的壹二茶堂，剛好是朋友介紹，店主很細心介紹，可以試到不同的台灣有機茶，先去走訪會是不錯的體驗。\n台北部分大多位於永康街，等我走訪過後再來推薦\n接著是去茶藝博覽會，我這次剛好 2019/1/4–7 在世貿有展覽，去走訪就硬著頭皮坐下來喝茶，開始有禮貌地請教，店家通常都願意跟你介紹，畢竟展覽大多是推廣為主，銷售是其次，這會是買茶葉與更認識茶葉的好機會，這次去就試到我個人非常喜歡的茶，順手就買點回家喝\n分享一下我個人口味，台灣茶大多是烏龍跟少許紅茶，其他茶種罕見，但是目前市場台灣的高山烏龍都是輕發酵，味道不太是我喜歡的，而且有點會刮胃，不太舒服\n後來喝到生活居茶行的老樹種紅茶，喝得就蠻喜歡的，而且十餘泡味道不減，十分圓潤甘甜，也是這攤的攤主跟我說慢慢喝不要急著走，即使我買的不多他依然熱情的介紹，很感謝這位攤主\n#更：後來老師有帶我們去桃園龜山的益壽茶廠參訪，體驗製作東方美人茶，整個過程十分有趣，從一開始參觀茶園，看看傳說中的小綠葉蟬，接著就參與曬菁 / 揉捻的過程，看著茶菁從鮮綠慢慢萎凋，從濃厚的草味轉變成青蘋果香，沒有實際製茶沒有想像到這樣美妙的風味流轉的過程，從早上九點一路忙到晚上十一點，後來要趕車浪菁、殺菁、成團的過程就沒有參與，幾天後收到親手做的東方美人茶有滿滿的回憶與感動！\n這邊也推薦益壽茶廠的茶葉喔，看臉書也有很多參訪活動，有興趣可以查查看～\n知識補給站 我蠻喜歡從有系統性方式開始學習，找老師或是看書就非常的適合，林莉書老師有推薦兩本介紹臺灣烏龍茶的書，其中也有茶葉介紹，包含茶的苦味、香味、澀味是怎麼來的，大抵的製程之類的，看完就會有個初步印象\n烏龍茶的世界：全方位茶職人30餘年心血結晶，從種茶、製茶、飲茶，告訴你烏龍茶風味的秘密\n台灣茶第一堂課：頂尖茶人教你喝茶一定要知道的事！\n另外是聽到茶友推薦的專業茶葉論壇\n臺灣 T4U 茶藝論壇 - Powered by Discuz!\n裡面有各式茶葉的討論，目前仍在努力爬文中\n最後補充一點，茶葉有賞味期但沒有保存期，也就是有些茶葉在某些年份會比較好喝，通常清香型當年度會比較好，其餘的放久了味道會「轉」，但只要用夾鏈袋陰乾保存，放個幾十年都不成問題，如果此時的味道不喜歡，再給他一點時間，過一陣子說不定就合胃口了。\n總之就是鼓起勇氣，多喝多看多問，可以去現代的茶館試試，也可以在有展覽去。\n結語 喝茶就是要找得適合的茶，用開心的心情喝茶，講究什麼的倒是其次 (轉述前輩所言\n這篇算是剛入門不久的心得，如有錯誤煩請糾正，長路漫漫，願茶香一路相隨\n","date":"2019-01-05T14:35:28.013Z","permalink":"https://yuanchieh.page/posts/2019/2019-01-05-%E8%8F%9C%E9%B3%A5%E8%8C%B6%E5%AE%A2-%E5%BE%9E%E8%B7%AF%E4%BA%BA%E5%88%B0%E5%85%A5%E9%96%80/","title":"菜鳥茶客-從路人到入門"},{"content":"這份是在 2018/11/20 由 V8 Team 釋出的文件，主要描述用一種新的Async 錯誤追蹤機制，此新機制僅適用於 async await ，promise chain則沒有效果，目前需要透過 --harmony-await-optimization flag 啟用這項功能。\n以下內容與圖片摘錄於該文件。\n緣由 參考下列程式碼\n1 2 3 4 5 6 7 8 9 10 async function foo(x) { await bar(x); } async function bar(x) { await x; throw new Error(\u0026#34;Let\u0026#39;s have a look...\u0026#34;); } foo(1).catch(e =\u0026gt; console.log(e.stack)); 如果運行在 V8 7.1 版本，會發現 error.stack 僅會印出部分錯誤資訊\n1 2 Error: Let\u0026#39;s have a look... at bar (\u0026lt;anonymous\u0026gt;:7:9) 而調用者 foo 則消失無蹤，這是因為從 JSVM角度，當 bar 因為 await x 而暫停執行，接著從 microtask queue 恢復執行後，在 stack 上已經沒有其他 function 了\n目前可以開 Chrome Dev Inspector ( \u0026gt; node --inspect)，執行同樣的程式，可以看到完整的 error stack，但這會有很大的性能影響，所以只建議在開發中使用。\n解法 目前的困境是發生錯誤時，因為非同步特性要追蹤完整的錯誤棧不容易。\n正好 await 會停止目前的 function 運作，所以不但可以知道目前停在什麼地方，也可以知道是從哪裡被呼叫；\n這特性跟 promise chain相同，因為 promise 透過 .then 連結，所以能夠輕鬆反序重建出 function 呼叫相依。\n先行條件 我們必須運用 promise chain，才能有足夠的條件找出完整 stack trace；\n但如果 promise 是被另一個 promise resolved (在 fulfill 方法中 return 另一個promise)，這樣 promise chain 間就沒有直接的依據，必須等到下一個 tick [PromiseResolveThenableJob](https://tc39.github.io/ecma262/#sec-promiseresolvethenablejob) 執行時才知道這兩個 promise 是有相依的。\n這是目前 ES2017 的規範所帶來 await 麻煩之處\n1 2 3 4 5 6 7 8 const _.promise_ = @createPromise(); @resolvePromise(.promise, x); const _.throwaway_ = @createPromise(); @performPromiseThen(_.promise_, res =\u0026gt; @resume(_.generator_object_, res), err =\u0026gt; @throw(_.generator_object_, err), _.throwaway_); @yield(_.generator_object_, _.outer_promise_); 目前 await 都必須用另一個 promise .promise 重新包裝，即使 x 本身已經是promise，問題在於 .generator_object 與 x 沒有直接的關連；\n而是必須等到下個 tick PromiseResolveThenableJob 將 .promise ← → x 與 .promise ← → .generator_object 才能關聯。\n所以目前需要額外建立 x ← → .promise 的關聯 或是 查詢 microtask queue，兩者都需要開銷。\n好在最近有提案要修改此部分，將多餘 .promise 刪除\nconst .promise = @promiseResolve(x);\nconst .throwaway = @createPromise();\n@performPromiseThen(.promise,\nres =\u0026gt; @resume(.generator_object, res),\nerr =\u0026gt; @throw(.generator_object, err),\n.throwaway);\n@yield(.generator_object, .outer_promise);\n最主要差別在 const _.promise_ = @promiseResolve(x); ，使用 promiseResolve 如果 x 本身是 native promise 就不會額外再包一層，可以在 V8 v7.2 --harmony-await-optimization 開啟此功能。\n概覽 接下來是 V8 內部的 await Promise實踐，透過上述的 Spec 修改後，達到 async stack trace 目標。\n針對以下的程式碼，畫出實際 V8 的 Call Stack\n1 2 3 4 5 6 7 8 9 10 async function foo(x) { await bar(x); } async function bar(x) { await x; throw new Error(\u0026#34;Let\u0026#39;s have a look...\u0026#34;); } foo(1).catch(e =\u0026gt; console.log(e.stack)); 因為 bar(x) 中 await (1)，而 1 不是 promise 所以需要透過 Promise().resolve(1) 多包一層，所以在 Call stack 上 bar 被 await fulfilled 呼叫；\n「如果 async stack 功能開啟」，此時會去搜尋 “current microtask”找到 包裝x的PromiseReactionJob ，找到的話就會順勢找到 generator object ，這個 object 也就是 async function 所轉化而來的，同時包含 async function 的 promise reference。\n另一種做法 另一種做法則是 await 產生的內部 promise ( .promise)有辦法找到外部 promise (也就是 async function， JSPromise)，如果全部都是用 async await 串連，可以找到 PromiseReaction ，裡頭包含兩個特殊的閉包 Await Rejected 跟 Await Fulfilled ，兩者共同分享一個 Object AwaitContext ，這個AwaitContext 是另一個再等待 JSPromise 的JSGenerator Object ，也就是 function foo。\n這部分內部實踐看得有點暈頭轉向，目前就是大致有個模糊概念。\nError.stack 目前 Error.stack 還不是官方標準，目前已經提案處於 stage-1 的草案\ntc39/proposal-error-stacks\n這部影片是文件的附屬 reference，主要分享 Nodejs 中如何使用async stack trace。\n以下為測試程式碼\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 function p(){ return new Promise((res, rej) =\u0026gt; setTimeout(()=\u0026gt; res(1), 1000)) } async function one(){ await p(); throw new Error(\u0026#34;err\u0026#34;); } async function two(){ await one(); } async function three(){ await two(); } async function four(){ await three(); } four().catch(error =\u0026gt; console.log(error.stack)); 使用目前的 Nodejs v11.5.0 執行，打印的結果是\n1 2 Error: err at one (/Users/.../Desktop/test.js:7:8) 整個 call stack 只剩 function one 而已，這樣要 debug 相當困難。\n此時可以開啟 inspector， \u0026gt; node —inspect-brk=0.0.0.0:9229 test.js\n接著打開 chrome dev tool，記得打開 Discover network targets，點擊進去會跳出下方的 debug視窗，接著要打開 Pause on caught rejection 不然出錯就會結束了\n如同上面所說，這種方法只適用於開發。\n新的 zero-cost async stack trace 目前需要 V8 v7.2 並透過 flag 才能開啟此功能，所以現在要先下載 v8-canary版的 Nodejs，平常都是用 nvm 管理多個 Nodejs 版本，可以用以下方式下載\n1 $ NVM_NODEJS_ORG_MIRROR=[https://nodejs.org/download/v8-canary](https://nodejs.org/download/v8-canary) nvm install node 可以透過 node的 REPL 環境執行 \u0026gt; process.versions 看 V8的版號。\n接著執行\n1 $ node --async-stack-traces test.js 打印結果就會是\n1 2 3 4 5 Error: err at one (/Users/zhengyuanjie/Desktop/test.js:7:8) at async two (/Users/zhengyuanjie/Desktop/test.js:11:2) at async three (/Users/zhengyuanjie/Desktop/test.js:15:9) at async four (/Users/zhengyuanjie/Desktop/test.js:19:9) 結語 之後看來要花時間重新學 C/C++，這樣才能看懂 V8 與 JS Binding 等底層的 Nodejs 實作。\n","date":"2019-01-01T10:27:29.899Z","permalink":"https://yuanchieh.page/posts/2019/2019-01-01-v8-zero-stack-async-stack-trace-%E7%A0%94%E7%A9%B6/","title":"V8 Zero Stack Async Stack Trace 研究"},{"content":"近期看不到少關於 async / await 的好文章，這裡特別摘錄兩篇總結\nAsynchronous stack traces: why await beats Promise#then() 對比於直接使用 Promise Chain，使用 async / await 除了語法上更加友善，更重要的是 JS Engine 底層有優化，尤其是在 stack trace的時候。\nawait X() 跟 Promise 最大差別在於執行時會直接暫停 function 運作，直到 await 的 promise X結束後才繼續原本的 function；\n至於 promise.then(X) 則是把 X 放上 callback chain上，原 function 就結束了。\n這點差異有著重大的改變，請看以下範例\n1 2 3 const a = () =\u0026gt; { b().then(() =\u0026gt; c()); }; function a 執行時，會直接呼叫 function b，而 b().then 則是將 () =\u0026gt; c() 匿名 function 放上 callback chain，然後 a 就結束了，所以 function a 的 context 也跟著消失；\n試想如果此時 b 或 c 拋出錯誤，理論上要除錯我們會希望看到完整的 call stack，但是 a 早就結束拋棄了 context，所以 V8 要額外將 a 的 context 傳給 promise b 跟 function c，這樣出錯才有辦法追本溯源，這些是額外的負擔。\n改成 async / await 就不一樣了\n1 2 3 4 const a = async () =\u0026gt; { await b(); c(); }; 當 b 或 c 出錯時，因為還在 function a 的 context所以可以很快速追蹤，不需要額外的成本。\n小結 很多 ES6語法都是 syntax sugar，但是 async / await 帶了更不一樣的改變，作者給兩點建議\n盡量用 async / await 避免不必要的 transpile，透過 babel-preset-env 來避免 Deploying ES2015+ Code in Production Today\n延伸剛才的小結第二點，JS Code 如果要跑在瀏覽器上不可避免要用 babel transpile 或是加上各種 polifill，但現在已經是 2018了！\n很多瀏覽器已經良好支援 ES 2015 +，所以 babel transpile 開始變得不是這麼必要，這篇文章深入探討 如何平衡舊時代 babel transpile的 legacy js 與 modern js。\n這個議題的解法是 \u0026lt;script type=”module”\u0026gt; ，如果瀏覽器支援 ESM Module，那他就可以支援 async / await 、 class、 arrow function 、 fetch 等等\n以下是 can i use 的支援程度分佈\nCan I use\u0026hellip; Support tables for HTML5, CSS3, etc\nChrome / Safari / Firefox / Edge 近兩、三個版本都良好支援，IE就算了…..\n開發重點 開發時使用 ES 2015+ 語法開發，透過打包工具(webpack 、 gulp等)來呼叫 babel 產生兩份程式碼： main.mjs、 main.es5.js\n.mjs 是指支援 ES Module 的 js file 副檔名；而另一個就是 transpile 給過時瀏覽器使用的 ES5 語法。\nbabel config 可以參考作者在內文提供的，主要就是改 target 與 output 檔名\n1 2 3 4 5 6 7 **targets**:{ **browsers**:[ \u0026#39;\u0026gt; 1%\u0026#39;, \u0026#39;last 2 versions\u0026#39;, \u0026#39;Firefox ESR\u0026#39;, ], } 1 2 3 4 5 6 7 8 9 **targets**:{ **browsers**:[ \u0026#39;Chrome \u0026gt;= 60\u0026#39;, \u0026#39;Safari \u0026gt;= 10.1\u0026#39;, \u0026#39;iOS \u0026gt;= 10.3\u0026#39;, \u0026#39;Firefox \u0026gt;= 54\u0026#39;, \u0026#39;Edge \u0026gt;= 15\u0026#39;, ], }, 在 html page中，引用 js 改成\n1 2 3 4 5 6 \u0026lt;!-- Browsers with ES module support load this file. --\u0026gt; \u0026lt;script type=\u0026#34;module\u0026#34; src=\u0026#34;main.mjs\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- Older browsers load this file (and module-supporting --\u0026gt; \u0026lt;!-- browsers know \\*not\\* to load this file). --\u0026gt; \u0026lt;script nomodule src=\u0026#34;main.es5.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 舊版瀏覽器看不懂 type=”module” 就會忽略，改成載入另一個 module；\n反之現代瀏覽器就載入 main.mjs。\n限制與注意事項 需注意以下幾點事項\n如果瀏覽器要使用 .mjs，記得 server response header 要加 content-type:text/javascript ，否則無法仔入 ES Module 行為類似於 \u0026lt;script defer\u0026gt; ，也就是會等到 document parse 完成才會執行，如果有需要立即執行的需求，就要把 code 拆出來 Module 是跑在 strict mode ，所以要語法等支援嚴格模式 Module 行為與一般 script 略有不同，例如變數不會全局污染\n例如說 var a = 123 ，在 script 中可以用 window.a 來讀取，但是 Module 不會。 注意 Safari 10 不支援 nomodule，所以要另外裝 polyfill 如果 node_modules 有用到 ES2015+ module，記得 babel config 需要額外處理 補更 最近看到的好文章，你真的知道 Babel Transpile 過後的程式碼長怎樣嗎？\nAvoiding Babel’s Production Bloat\n這篇文章提了幾個像 class / destructure / generator 等語法如何被 Babel 轉譯成 ES5的語法，整個程式碼的複雜度與體積都驚人的成長；\n如果網站主要在舊型瀏覽器，最好是考慮用舊的語法。\n好處 Module Size\n節省約 50%! Parse TIme\n節省約 55%! ","date":"2018-12-28T01:39:30.852Z","permalink":"https://yuanchieh.page/posts/2018/2018-12-28-%E6%84%9B%E7%94%A8-async-await-%E8%80%8C%E9%9D%9E-promise/","title":"愛用 async await 而非 promise!"},{"content":"JS 基於事件驅動，大量的 Promise 充斥在應用程式中，其後在 ES2017 加入了 async/ await 語法糖後，讓非同步代碼更加簡潔與直覺；\n但是 async / await 不單單是語法糖，從一開始會有不小的 overhead，v8 team 不斷的優化到今日與原生的 Promise chain 效能不相上下，甚至使用 async / await 效能更優於 Promise chain 了!\n以下內容整理自\nhttps://v8.dev/blog/fast-async#fn2\n圖片來自 v8 官網\nV8 在執行 async await時，會通過一個轉換，流程大概是\n先將 await 轉成 promise 加上 Handlers 停止 function 運作並回傳 implicit_promise 其中的參數 promise 目的是 async function 會等到 await 完成才會下一步 ；\n而 throwaway promise 則另有他用，稍後會解釋\n所以整體上一個 aysnc / await 會造成 2個額外的 JSPromise 以及 3個額外的 Mircorticks!\n優化 取消對 await function 多包一層 Promise 大多數 await 後面接的都是 promise，那為什麼要在多浪費一個 Promise 呢？\n直接改用 promiseResolve()，如果原本是 Promise 就直接回傳，如果不是在多包一層 Promise，如此就可能優化掉一個 Promise 還兩個 MicroTasks！\n直接回傳Promise 跟多包一層 Promise 有什麼差別呢？\n參考github issue 討論的其中一個案例\n**Normative: Reduce the number of ticks in async/await by MayaLekova\n1 2 3 4 5 6 7 8 9 10 11 async function f() { let promise = Promise.resolve(1); promise.then = function(...args) { console.log(\u0026#39;then called\u0026#39;); return Promise.prototype.then.apply(this, args); }; let result = await promise; return result; } f().then(v =\u0026gt; console.log(v)); 可以試著在 Chrome 71 跟 73(現為 Canary版)執行，會發現 73 優化過後不會再印 then called ，因為少了用 Promise 多包一層，已經 resolved 的 promise 就會直接回傳而不會在 call 一次 then function。\n這部分會影響對 Promise 做 Monkey Patch 的用戶，所以在 github issue 下有不同的討論，可以看到維護者對於語言功能上與一致性上的取捨，大致上就是不鼓勵對 Promise.then 做 Monkey Patch的操作。\n去掉 internal promise ：throwaway 在文章中提到 throwaway 之所以存在是為了滿足 Spec 對於performPromiseThen API的操作 (only there to satisfy the API constraints of the internal performPromiseThen operation in the spec.)\n後來到 Twitter 發問，作者回覆說「It’s still there only when async hooks are enabled (in Node.js) — they attach some internal data on it.」\n所以在 Nodejs還會使用，但是在 Browser 就不會了，這部分看來不用太在意。\nNodejs v8 的小Bug Nodejs 在 version 8 有偷做優化，也就是第一步省掉，如果發現是 await 接promise 就不會多包一層，可以用 v8 / v10 對比跑下列程式\n1 2 3 4 5 6 7 8 const p = Promise.resolve(); (async () =\u0026gt; { await p; console.log(\u0026#39;after:await\u0026#39;); })(); p.then(() =\u0026gt; console.log(\u0026#39;tick:a\u0026#39;)) .then(() =\u0026gt; console.log(\u0026#39;tick:b\u0026#39;)); 會發現 version 8 的打印結果是 after:await -\u0026gt; tick:a -\u0026gt; tick:b ，而 version 10 則是符合目前 Spec的 tick:a -\u0026gt; tick:b -\u0026gt; after:await 。\nversion 8 因為 p 已經是 resolved promise會執行執行；version 10 多包一層 Promise 就會等到下一次 microtask queue 執行時期才會執行。\n開發重點 請使用原生 JS Engine 提供的 Promise，而不是其他 JS Library 的 Promise或非 Promise 類型的函示，這樣在底層會有更好的效能優化 ( 省了兩個 Microtask與一個多餘的 Promise )\n","date":"2018-12-25T13:06:04.006Z","permalink":"https://yuanchieh.page/posts/2018/2018-12-25-v8-%E5%A6%82%E4%BD%95%E5%84%AA%E5%8C%96-async-/-await/","title":"V8 如何優化 async / await"},{"content":"Stripe 是一間國際的金流支付公司，提供 client (Web / Android / iOS等)支付介面與 server-side API，用最短的時間就可以讓服務接上金流；\n支援支付方式有Visa/Master Card等多間信用卡支付、Google Pay 、Apple Pay等等；\n其中金流交易服務：\n入門會員服務Payment(交易)、Billing(開立發票)、Connect(中間商，向A收錢並轉給B多少錢)； 其他需要多花錢的加值服務：\nSigma(支援SQL產生報表)、Altas(美國開公司)、Radar(金融詐騙偵測，預設有支援但付費有進階功能) 邀請制的加值服務：\nTerminal(整合硬體Reader，限定持有該硬體才能交易)、Issuing(可以自己發卡，Stripe 提供虛擬卡與實體卡發送) 這次主要嘗試 Payment / Billing 的串接，內容參考自官方文件，包含 client-side(採用 React) 與 server-side(採用 Nodejs SDK)，不得不說 Stripe對於開發者非常友善，文件非常好懂且各式語言與插件SDK都支援完整，所以只需要將SDK換掉我想以下的概念都是相同的。\n2018/11: 全球地區都可以付款，只是要開通 stripe帳戶只有26個國家，目前不支援台灣\n主要介紹 Payment / Billing 的概念，以及試圖理解 Stripe背後的運行機制\nPayment 分成兩步驟，在client-side 置入 stripe 付費元件(如 html form)，用戶輸入後會產生 token轉交給 server，接著 server 用此 token驗證並實際扣款\n在Stripe 中，它定義很多物件，每個物件都有各自參數與封裝的用法，所以文件非常的 OOP，所以對開發者來說很好理解與上手\nCheckout Checkout 是一個 Stripe 提供簡化過後的 Payment方式， 開發者只要接上 Stripe 提供的 Client SDK，用戶的交易細節就會直接丟給 Stripe 處理並以 Token方式回傳，開發者再去 Server Side 拿 Token做後續的應用。\n整合 Stripe Client-side有兩種模式 Simple、Custom\nSimple 用一個 \u0026lt;form/\u0026gt; 定義如何轉交給token給 server，接著內部嵌入 stripe script 並定義參數，最基本的幾個參數\n1 \u0026lt;form action=\u0026#34;your-server-side-code\u0026#34; method=\u0026#34;POST\u0026#34;\u0026gt; \u0026lt;script src=\u0026#34;https://checkout.stripe.com/checkout.js\u0026#34; class=\u0026#34;stripe-button\u0026#34; data-key=\u0026#34;YOUR_KEY\u0026#34; data-amount=\u0026#34;999\u0026#34; data-name=\u0026#34;COMPANY\u0026#34; data-description=\u0026#34;Example charge\u0026#34; data-image=\u0026#34;https://stripe.com/img/documentation/checkout/marketplace.png\u0026#34; data-locale=\u0026#34;auto\u0026#34; data-currency=\u0026#34;hkd\u0026#34;\u0026gt; \u0026lt;/script\u0026gt;\u0026lt;/form\u0026gt; 對，就這麼簡單就完成了，接著就是到 form定義的 server api 收 token資料\nCustom 如果希望有更細緻的用戶體驗，例如客製化自己的付費表單、顯示錯誤訊息等，就需要自己客製化，這部分也十分的簡單，用 vanilla js 也可以輕鬆完成\n這部分程式碼可以參考文件\n參數設定 以上的 Simple/Custome 就是一個基本的 Element，裡頭定義的 data-* 屬性則會決定產生的 Checkout 物件屬性，常見的屬性有\ndata-key [required] 當帳號成功激活後，會有兩組 key / secret 組合，分別是 live / test ，對應就是正式機與測試機的概念 token / source [required in custom mode]\n這兩個對應 callback function，分別對應收到 Token / Source，Token主要是server可以取得用戶的部分信用卡資料，Source則是代表用戶的其他付款方式(這些資料的取得要透過後續的參數設定) data-name / data-description / data-image / data-locale / data-amount [highly recommend]\n對應會顯示在 stripe 付費頁面的資訊，locale是設定表單語言 data-zip-code / data-billing-address [highly recommend] 用戶的地址相關，Stripe推薦向用戶索取 zip-code，zip-code可以在後台設定當作 Radar的驗證條件，禁止可疑的Payment data-email / open / close [optional] 還有一些欄位可以定義就翻文件了，open / close則是 callback function會在表單開啟與關閉時候呼叫，同樣只用於 custom mode 特別注意，開發者完全不需要接觸到用戶的信用卡資料，在沒有謹慎評估之前，不要妄想儲存用戶信用卡資料！\n因為如果要「合格」儲存用戶信用卡資料，必須過 PCI-DSS 國際信用卡組織聯合規範的支付安全認證，但這部分規範嚴格，而且必須定期做漏洞掃描\n所以 Stripe 也提到他們為了讓開發者便利，所以信用卡資料都還是通過他們，僅回傳 Token形式供後續扣款使用，不用再多煩惱這些資安上的問題\nServer-side 前端取得 Token 後，就可以往 server 丟，這個 Token是一次性扣款使用，此外就等同於 Source的效力，也就是用於實際扣款的支付方式，後續會詳細介紹什麼是 Source\n以上是最基本的 Checkout 信用卡付款，因為信用卡是立即扣款就能知道結果，但如果需要支援如銀行轉帳等其他需要用戶額外授權與支付的流程，就需要以下更複雜的設計\n多元支付方式 Stripe透過產生 Source 物件代表不同的支付方式，支付概念上可以分成\n付款是同步與非同步完成(async vs sync) 錢如何從用戶轉出(push vs pull) 是否可重複使用(reusable)\n例如說信用卡就是 sync + pull ，當用戶輸入信用卡後，就會立刻執行扣款動作(sync)，就直接從用戶帳戶扣款或是產生支付紀錄(pull)\n而像銀行產生虛擬帳號提供ATM付款是 async + push，用戶可能在過幾天才去付款(async)，而用戶本身需要主動去產生支付的動作(走到ATM前面，也就是 push)\n其他還有多種地域性的支付方式，因為不熟悉就不再多敘述，可以參考文件\nSource 物件建立初始化為 source.pending，有 webhook 可以接收狀態的改變，當用戶授權後會觸發source.chargeable 、用戶拒絕授權source.failed 、過期source.canceled\nSource 此時僅代表支付方式，實際的支付要透過 Source 建立 Charge 物件，Charge 物件同樣有幾個 webhook 可以串接，非同步支付初始化會是等待用戶支付charge.pending 、成功收到用戶支付charge.succeeded 、與支付失敗charge.failed\nwebhook 部分則是在後台設定。\n以下擷取官方案例，僅作於理解使用\n1 2 3 4 5 6 7 8 9 10 stripe.createSource({ type: \u0026#39;ach_credit_transfer\u0026#39;, currency: \u0026#39;usd\u0026#39;, owner: { email: \u0026#39;jenny.rosen@example.com\u0026#39;, },}).then(function(result) { // 這裡會回傳 Source基本資料如 id等 // 還有用戶需要知道的轉帳資訊如銀行帳號 }); // 掛webhook，指定source.chargeable app.post(\u0026#34;/webhook/to/source.chargable\u0026#34;, (req, res) =\u0026gt; { console.log(req.body.read) }) 某些支付方式是可以多次扣款，例如信用卡，Stripe 提供 Customer 概念，也就是可以創建一個 Customer 代表用戶，接著將 Source 綁定到該用戶上，一個用戶可以綁定多個 Source，到時候如果要扣款可以從 Source中選擇其中一個，相當的方便\n需注意 Source 必須是 chargable 才可以扣款\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // 建立 customer stripe.customers.create({ email: \u0026#34;paying.user@example.com\u0026#34;, source: \u0026#34;src_18eYalAHEMiOZZp1l9ZTjSU0\u0026#34;, }, function(err, customer) { // asynchronously called }); // 如果要扣款的話 stripe.charges.create({ amount: 1099, currency: \u0026#34;eur\u0026#34;, customer: \u0026#34;cus_AFGbOSiITuJVDs\u0026#34;, source: \u0026#34;src_18eYalAHEMiOZZp1l9ZTjSU0\u0026#34;, // 可以不指定 source，會自動找 customer 預設的 source }, function(err, charge) { // asynchronously called }); 小結! 看到是不是有點暈頭轉向了呢，說好的 Stripe 很簡單呢？ 或許這也是種 simplicity but not simple，要提供多元的支付方式勢必帶來邏輯的複雜性，但是我覺得 Stripe 透過 OOP梳理整個金融支付的過程，帶來極棒的 Developer Experience與 User Experience，接過台灣的紅藍綠你就會知道Stripe 有多棒了\n廢話不多說，讓我來重新整理一翻\n最一開始提的 Checkout 信用卡支付取得的Token，算是 Source的簡化版，我猜目的是為了讓最快速接串好金流的作法，而且信用卡的支付行為算是最簡單的。\nBillings 用於重複性扣款與開立發票\n發票狀態流程 Invoice，代表發票的物件，一張發票可以多筆款項 InvoiceItems，因為不同的財務規劃而有比較多的狀態可以設\n初始化為 draft ，此時發票的設定都還可以做調整，確定後或是預設一個小時後會變成 open ；\n刪除就變成 deleted open 則代表發票確認了，如果用戶付款了可以調整為 paid ，可以選擇觸發後續的發票寄送等流程；\n如果發現用戶破產之類無法支付，可以在後台將此筆發票設定為 uncollectible ；\n如果發票有誤需要作廢，則設為 Void 一次性開立發票 one-off invoices 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 var stripe = require(\u0026#34;stripe\u0026#34;)(\u0026#34;sk_test_AyGgRZ5ZZkIETGtHDI3f1GAE\u0026#34;); // 先建立發票 stripe.invoiceItems.create({ customer: \u0026#34;cus_DqZrTNCO4puf2p\u0026#34;, amount: 2500, currency: \u0026#34;usd\u0026#34;, description: \u0026#34;One-time setup fee\u0026#34; }, function(err, invoiceItem) { // 針對某個用戶底下所有的InvoiceItems開立發票 stripe.invoices.create({ customer: \u0026#34;cus_4fdAW5ftNQow1a\u0026#34;, auto_advance: true // auto-finalize this draft after ~1 hour }, function(err, invoice) { // asynchronously called }); }); auto_advance 算是很重要的參數，Stripe 預設在 Invoice 創建之後會有自動化的動作，如一小時後自動轉成 open**，並會向用戶自動扣款 (預設 Source)接著轉成** paid，並Email 發送 Receipt 與 Invoice；\n如果不想要記得設為 false\n在開立發票上 Stripe 靈活性頗高，每一筆 InvoiceItem 可以設定費用/折扣/稅率，InvoiceItem 可以指定所屬的 Invoice，或是預設歸類到該用戶的下筆 open 的Invoice中\n訂閱制扣款服務 subscription 有些雲端服務都是以年或是月的訂閱收費制度，Stripe 中可以定義 Product 與 Plan，例如說有一個 Product 是 SaaS服務\n1 2 3 4 5 6 7 const product = stripe.products.create({ name: \u0026#39;My SaaS Platform\u0026#39;, type: \u0026#39;service\u0026#39;, metadata: { // store anything you want } }); 針對這個 Product，可能會有許多的收費方式 Plan，可能是月繳/季繳/年繳或是有不同的適用期限、也可能有多種國家的不同定價等設定\n1 2 3 4 5 6 7 const plan = stripe.plans.create({ product: \u0026#39;prod_CbvTFuXWh7BPJH\u0026#39;, nickname: \u0026#39;SaaS Platform USD\u0026#39;, currency: \u0026#39;usd\u0026#39;, interval: \u0026#39;month\u0026#39;, amount: 10000, }); Stripe 支援分層收費，可以指定多種模式，例如 當超過一個額度，每個商品變多少錢 / 又或是分多階層，超過部分算該價格的階梯形收費\n當先前的 Custome想要購買時，會創建一個訂閱 Subscription 的物件，並決定要訂閱的 Plan，如果要一個訂閱多個 Plan，這些 Plan必須幣別相同且收費區間一致；\n1 const subscription = stripe.subscriptions.create({ customer: \u0026#39;cus_4fdAW5ftNQow1a\u0026#39;, coupon: \u0026#39;free-period\u0026#39;, tax_percent: 6.34, trial_end: 1542721841, items: [{ plan: \u0026#39;plan_CBXbz9i7AIOTzr\u0026#39; }, { plan: \u0026#39;plan_IFuCu48Snc02bc\u0026#39;, quantity: 2, }],}); 此外，還有Coupon 可以設定，支援一次性 / 永久性 / 每次扣款用，還有 %跟數量的設定，相當有彈性\n1 const coupon = stripe.coupons.create({ duration: \u0026#39;once\u0026#39;, id: \u0026#39;free-period\u0026#39;, percent_off: 100,}); 建立 subscription 在沒有停止之前，Stripe 會自動依照時間定期扣款，並自動開立發票(可關閉)，相當的方便。\n升級或降級 如果用戶訂閱了普通版，在月中時突然想升級到專業版，該怎麼處理呢？\n可以選擇將該用戶的 subscription 物件修改，更新訂閱的 Plan\n1 2 3 4 5 6 7 8 stripe.subscriptions.update(\u0026#39;sub_49ty4767H20z6a\u0026#39;, { cancel_at_period_end: false, items: [{ id: subscription.items.data[0].id, plan: \u0026#39;plan_CBb6IXqvTLXp3f\u0026#39;, }], proration_date: proration_date // Optional，詳看下方流程 }); 更新容易，但是實際的扣款流程必須對應業務邏輯的處理，也就是收費的方式；\n例如幾種案例\n1/1開始訂閱 PlanA $30/mo，接著再 1/15 升級到 PlanB $45/mo\n因為都是每月扣款，所以 Stripe 預設會在 2/1 開始收 PlanB 也就是 $45元，所以理論上應該在 2/1才將用戶升級到 PlanB的服務 同上，但是用戶希望 1/15就升級到PlanB\n這時候有兩種做法\na. 自己手動開立 Invoice，開立Invoice當下就會扣款並產生發票\nb. 設立 Proration，這是 Stripe 提供的方法，也就是在 update subscription 時指定 proration_data ，Stripe 會在下一次收費時間多開立這部分的金額 如果用戶從月付費改成年付費，會立即扣款 如果1/1已經扣款 PlanA的錢，用戶 1/15取消，Stripe 預設不會退款；\n也可以設定 cancel_at_period_end，讓用戶在月底才取消資格；\n如果有額外產生的代付款項服，必須要手動清除 Invoice才不會在月底多扣款一次 小結二 同整一下 Invoice / Invoice Item / Product / Plan / Subscription 關係\n總結 如果要一次性付費，則可以使用 Payment，且 Payment 不需要綁定 Customer，同樣會開立發票；\n以下更複雜的扣款方式方式要先在 Stripe 建立 Customer，可以利用 Invoice，分次建立 Invoice Item，最後再一次開在一張 Invoice 下完成扣款；\n又或是建立 Product 與對應的付費機制 Plan，並用 Customer 角色訂閱 Subscription， Stripe會處理定期扣款等機制；\n結合 Coupon 可以提供彈性的折價機制。\n總體上 Stripe 最讓我覺得方便的是 Dashboard 可以設定，這樣就可以少蓋一個後台自己麻煩，而且把Customer / Payment / Billing 等分門別類，以及個別子項目，非常好管理金流服務。\n","date":"2018-11-17T07:53:08.683Z","permalink":"https://yuanchieh.page/posts/2018/2018-11-17-stripe-%E4%B8%B2%E9%87%91%E6%B5%81%E6%95%99%E5%AD%B8-%E4%B8%8A/","title":"Stripe 串金流教學 (上)"},{"content":"在 MongoDB中，其 Isolation 與 SQL標準定義的 Isolation Level不同，畢竟NoSQL注重於海量讀寫 、集群式的應用場景，自然所面對的問題也就差異很多，但也因此在面臨 Concurrency 時的讀寫保證問題，以下是閱讀官方文件並整理的結果。\nRead Isolation, Consistency, and Recency - MongoDB Manual\nIsolation level 正如同 SQL 不同的 Isolation 試圖在每一層解決不同的問題 (dirty read/ unrepeatable read / phantom read) 的問題，同樣在 MongoDB中也有幾種Isolation level\nRead Uncommitted Client 可以讀取那些寫入但尚未持久化的資料\nClient 會讀取到尚未 durable 的資料，以及寫入MongoDB後在該寫入Client 收到成功訊息之前\n( A writes → MongoDB accept → B read A’s write → A accept success) 如果是多行寫入的操作(如 updateMany)，Client 可能會讀到更新未完全的文檔 (如果一次更新五個文檔，可能讀到三個更新後文檔，但每個文檔如果更新多個欄位，MongoDB保證不會讀取到部分更新欄位的文檔) Client 可能讀到之後被roll back 的數據 這是預設的 Isolation Level\n在 MongoDB 4.0之後才加入 多文檔的 transaction 保證，在沒有 transaction 保證下多文檔讀取會有幾個問題\nNon-point-in-time read\nA read d1 ~ d5 → B update d3 → A 可能會讀到B更新的 d3\n也就是在讀取發生的該時機點沒有產生 snapshot，可以理解成 unrepeatable read Non-serializable operations\nA read d1 → B write d2 → B write d1 → A read d2 對於 d1 來說，是 A 先讀接著換 B 寫 / 對於 d2 來說，是 B寫再換 A讀；\n這個情況導致兩者的讀寫相依順序剛好顛倒，所以沒辦法序列化讀寫順序 Miss match read\n**同樣是讀取中有更新發生，可能會導致更新的文檔與讀取的條件發生錯置，原本符合條件的可能更新後不符合種種 Cursor Snapshot 用 cursor 讀取在某些情況下可能會將同一個文檔返回多次，因為同期間文檔可能因為更新等因素而改變了位置\n但如果文檔中有 unique key，則同個文檔僅會返回一次\nReal Time Order 允許同一個文檔的多組 thread 讀寫宛如單一 thread 讀寫一般，亦即在同一個文檔中會以序列化方式執行讀寫請求\nCausal Consistency 因果一致性，例如刪除符合某特定條件的文檔，接著讀取同樣條件，後者狀況依賴於前者，於是前後兩者便有了因果關係；\nMongoDB 3.6+ 提供 writeConcern:majority / readConcern:majory 外加因果性的保證\nRead your writes\n寫入後緊接著讀取寫入的資料 Monotonic reads\n讀取資料的結果不可以是比上一次讀取結果更早的結果\n例如 w1 → w2 → r1 → r2，w2 相依於 w1，r1 這裏相依於 w2 ，那麼 r2 不可能讀取到 w1 的結果 Monotonic writes\n因果序先於其他寫入者必然先發生寫入\n也就是 w1 → w2，w1 / w2 之間可能還有其他的寫入操作，但是 w1 必然比 w2 早執行 Writes follow reads\n要在讀取之後寫入的寫入事件必然發生於讀取之後\n也就是 r1 → w1，則r1 必然發生於 w1 之前 Write Concern 與 Read Concern 截自 MongoDB 官方文件\nhttps://docs.mongodb.com/manual/core/causal-consistency-read-write-concerns/\nMongoDB 在讀寫有不同的 Isolation Level可以設定，也就是 Read Concern / Write Concern，不同的Level對應不同的一致性與可用性的考量，交錯後有多種組合狀況，Isolation level 可以定義在連線 session、Transaction、單次操作\nWrite Concern Write Concern 主要是指 當 Client 送出寫入請求後，Server會在什麼條件下回應Client 已經成功寫入了，總共有三個參數可以使用\nw: 寫入保證條件 j: Boolean，是否寫入 disk journal 後才返回 wtimeout: 多久後發生timeout避免鎖死，0 代表不需要 timeout 設計 其中 w 又有幾種選項\n\u0026lt;number\u0026gt;: 對應幾個 mongod server 收到寫入請求才會回應成功訊息\n0 代表不需要回應，但可能會回傳 network / socket 相關錯誤\n1 預設值，寫入請求被 standalone mongod 或是 primary 收到\n超過 1 僅適用於 replica set，也就是 primary + secondary 總共幾個收到才會回應成功 majority\n擁有投票權的節點*過半數都收到寫入請求，對應的 j 變數如果沒有宣告則看系統的預設值 [writeConcernMajorityJournalDefault](https://docs.mongodb.com/manual/reference/replica-configuration/#rsconf.writeConcernMajorityJournalDefault \u0026quot;writeConcernMajorityJournalDefault\u0026quot;) Arbiter 雖然有投票權，但是不算在其中，因為沒有儲存資料 \u0026lt;tag\u0026gt;\n在設定 replica set 可以指定 tag，此處的 tag 表示寫入請求被送往符合的 replica set Read Concern local 最低的讀取保證，讀到的資料不保證已經被 majority 寫入且可能資料會 rollback\navailable 基本上跟 local一樣，只有在 shard cluster 的時候 available 是最低延遲的讀取請求，並不會更新 sharding中的中繼資料(does not contact the shard’s primary nor the config servers for updated metadata.)\n因此可能會讀取到 orphaned document (某些資料已經搬移到其他chunk但是在原始位置因為錯誤或是意外shutdown導致沒有清除，需要額外清除 cleanupOrphaned\nmajority 保證讀取到的資料是被replica set 中的多數確認過，且不會 rollback，注意是 acknowledge 而不一定是 durable，要對應看writeConcern!\n如果是在多文檔transaction中，除非 writeConcern 是 majority，否則 同在 transaction 中的 readConcern 不能提供保證。\n但需要注意 majority 僅能確保讀取資料不會 rollback，但是不保證能讀到最新資料\n官方建議在 primary-secondary-arbiter PSA架構下關閉 majority，因為複製不會到 arbiter，所以要達到 majority條件就是 primary-secondary 兩台都成功寫入才算成功，會給系統帶來比較大的負擔，容錯性也變差*\nlinearizable 基於 majority 提供更強的保證，保證寫入後的讀取都不會讀到更舊的資料，除了選擇讀取的節點外，還會去跟多數節點確認，之後才返回值，所以效能需求又高出一截；\n為什麼還需要跟多數節點在確認呢？\n主要是怕 network partitioning，也就是因為網路延遲而導致原本的 replica set 被分割，例如原本 3台變成 1 + 2，而 2台一組的部分因為還符合多數可以產生新的 primary，此時如果有人去讀(majority) 1那一台就可能取得舊資料，因為 1徹底跟其他節點分離；\n加上 linearizable可以避免掉此問題，詳細可參考此問答 https://stackoverflow.com/questions/42615319/the-difference-between-majority-and-linearizable\nsnapshot 只在多文檔 transaction 中生效，文檔沒有過多敘述，但推敲應該也就是保證在同一個 transaction 中不會讀取到 transaction 之後的更動。\n讀取的實例說明 截自 MongoDB 官方文件\nreadConcern: majority t0 : primary 收到寫入請求，因為這時 primary 值還是 write_prev t1：secondary1 收到寫入同步請求，此時 primary / secondary 還是 write_prev t2：secondary2 收到寫入同步請求，此時值也都還是 write_prev t3：因為writeConcern是 majority，此時 primary 收到 secondary1的回覆，此時 primary才變成 write_new，並且回復 client 寫入成功，但是 secondary1 還是停留在 write_prev t4：primary 收到 secondary2 回覆 t5：secondary1收到 primary1，此時 snapshot才更新為 write_new t5：secondary2 動作同上 readConcern: local t0：primary 此時值是 write_new t1：secondary1 此時值是 write_new t2：secondary2 此時值是 write_new readConcern: majority 讀取時是透過 snapshot，根據此篇文章MongoDB readConcern 原理解析 指出\nMongoDB 会起一个单独的snapshot 线程，会周期性的对当前的数据集进行 snapshot，并记录 snapshot 时最新 oplog的时间戳，得到一个映射表\n只有确保 oplog 已经同步到大多数节点时，对应的 snapshot 才会标记为 commmited，用户读取时，从最新的 commited 状态的 snapshot 读取数据，就能保证读到的数据一定已经同步到的大多数节点。\n結語 這篇主要整理與理解一些官方文件的資料與設定，並沒有什麼太重大的結論，更進一步可以看一下，條列在 Causal Consistency 下的不同讀寫保證\nCausal Consistency and Read and Write Concerns - MongoDB Manual\n註記 擁有投票權 在MongoDB的 Replica set中，只有以下幾種狀態有投票權 (voting node)\nprimary secondary startup2：節點載入成員的設定檔正式成為 replica set的一員並開始初始化同步與索引建立 recovering：當節點尚未準備讀取請求時，等到 client 覺得ok便會轉為 secondary，這部分沒有明說 recovering 中做了什麼 arbiter rollback：當舊的 primary 因為某些因素被剔除，之後選出了新的 primary，舊的 primary 後來恢復同步發現有些寫入當初沒有同步，此時舊的 primary會選擇 rollback 這些沒有同步的資料 目前 MongoDB的 replica set最多只能有 50個成員，其中最多只能有 7 名成員有投票權；其他沒有投票權的成員可以當作備份或是讀取請求的節點。\nReplica Set 容錯性問題 replica set 規定是要在超過多數的形況下才能運作，所以最低必須要有三個可投票節點才能成立，試想以下狀況\n3台機器，可以容錯一台，需要保證 67%的機器運作正常 4台機器，也是僅可容錯一台，需要保證 75%的機器運作正常 這也是為什麼會推薦使用奇數台的機器；\n在 PSA下，Arbiter 不能同步資料，所以 write: majority 必須同步寫入 primary + secondary，所以系統容錯就變差，同時加重機器的負擔。\n","date":"2018-10-14T01:39:58.591Z","permalink":"https://yuanchieh.page/posts/2018/2018-10-14-mongodb-isolation-%E8%88%87-transaction/","title":"MongoDB Isolation 與 Transaction"},{"content":"Amazon S3 Performance Tips \u0026amp; Tricks + Seattle S3 Hiring Event | Amazon Web Services\nAWS S3 用來做靜態資源管理，可以用來儲存非常大量的資料且有很高保持性(duribility:99.9999999%)的保證；\n在這篇官方的部落格中記載著S3內部的一些儲存機制以及優化的小技巧。\n如果是每秒 API Request小於 50者，其實不太會有影響，S3 有自動的監控程序(Agent)，試著去平衡資源與增加系統的平均附載。\nS3在內部維護了一張 Map，Map對應的Key 是 Bucket 中物件的名稱，S3會用物件的初始名稱當作 Key；\n實際物件的儲存會被分散到不同的 parition中，而 S3為了提供依照按字典順序排列的API， 在做 partitioning 的依據同樣是透過 Bucket的物件初始名稱；\n所以如何取 Bucket的物件名稱就是影響性能的關鍵，在S3內部 Map的key\nbucketname/keyname ，S3會自動取前綴\n有些人會習慣用 user ID / game ID/ 日期 / 遞增數字等，這些 Key的共通特性是前綴類似，這當作S3 Key會導致幾個問題\n1. 傾向儲存在同一個 partition中\n2. 所有的 partition 會有漸冷的效果，例如用日期儲存，通常越近的日期資料越容易存取，也就是partition 的使用量不平均\n以下就是不好的示範\n1 2 3 4 5 6 7 8 9 2134857/gamedata/start.png 2134857/gamedata/resource.rsrc 2134857/gamedata/results.txt 2134858/gamedata/start.png 2134858/gamedata/resource.rsrc 2134858/gamedata/results.txt 2134859/gamedata/start.png 2134859/gamedata/resource.rsrc 2134859/gamedata/results.txt 有個非常簡單的方法可以做到，也就是在開頭加 hash，或是將日期字串反轉，總之要確保前兩到三位是不重複字串且寫入的物件數雷同\n1 2 3 4 5 6 7 8 9 7584312/gamedata/start.png 7584312/gamedata/resource.rsrc 7584312/gamedata/results.txt 8584312/gamedata/start.png 8584312/gamedata/resource.rsrc 8584312/gamedata/results.txt 9584312/gamedata/start.png 9584312/gamedata/resource.rsrc 9584312/gamedata/results.txt S3會自動識別 prefix並分散到不同的 partition\n/7\n/8\n/9\n至於 prefix 真正到底如何取值文章並沒有說明，僅提到 prefix 理論上只有兩到三位即可，根據另一篇 AWS官方文章 Request Rate and Performance Guidelines，應用程式存取S3的API限制為\n3,500 PUT/POST/DELETE and 5,500 GET requests per second per prefix in a bucket\n所以 hex hash 三位，最大讀取值可達 16 * 16 * 16 * 5500 GET API Call /per second，這理論上可以滿足絕大多數的產品需求了；\n甚至大多數的產品還不需要 prefix的命名優化。\n","date":"2018-09-25T21:34:31.149Z","permalink":"https://yuanchieh.page/posts/2018/2018-09-25-%E7%AD%86%E8%A8%98-aws-s3-%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87%E5%B0%8F%E6%92%87%E6%AD%A5-amazon-s3-performance-tips-tricks/","title":"[筆記] AWS s3 性能提升小撇步 — Amazon S3 Performance Tips \u0026 Tricks"},{"content":"在數位內容的時代，影音的閱讀比例也越來越高，相關的直播與影音串流服務越來越多，而其中最常使用的串流技術規格便是HLS；\n近日工作上開始使用不少HLS服務，決定自己翻Spec搞懂整個技術架構，並整理成筆記分享。\n目前預計會寫個兩篇：\n上篇(此篇)介紹 HLS Spec內容\n下篇介紹影音處理相關的規範與工具使用\n全文內容基於文件 https://www.rfc-editor.org/rfc/rfc8216.txt ，但內文為自己閱讀後並重組，如果有不清楚或是筆誤之處，再麻煩留言囉\n簡介與概論 RFC 8216 主要是介紹 HTTP Live Streaming (HLS)，主要定義資料的檔案格式與Server/Client應該有的相對應行為 ，目前的最新版本為 7。\nHLS 提供穩定、省花費的方式推送連續性或長時段的影片，有幾個特性\n接收方可以根據網路狀況調整不同解析度 支援加密 同個影音內容不同的播放條件，例如字幕切換 基於HTTP，可以有良好的Cache機制(省花費的原因) HLS 大致上是將影片切成小塊狀，並透過播放清單推送，Client 收到切成塊狀的影片片段後就可以直接播放，而不需等到整個影片載完才可以播；\n而每個片段都可以被Cache在CDN，方便推送到全世界。\n小塊狀的影片稱為 Media Segment，常用的格式為 ts / fmp4\n播放清單則是 Playlist，格式為 m3u8\nHLS 有兩種 Playlist 格式: Master Playlist 與 Media Playlist\nMaster Playlist 主要是為了提供多種解析度、翻譯字幕的播放清單 (Media Playlist)，當 Client (如播放器)依據網路條件與用戶選擇等，則選擇支援的播放清單下載影片檔\nMaster Playlist 主要針對相同內容提供不同的 Streaming，例如不同的解析度、不同的國家有不同的字幕等\n1 2 3 4 5 6 7 8 9 #EXTM3U #EXT-X-STREAM-INF:BANDWIDTH=1280000,AVERAGE-BANDWIDTH=1000000 http://example.com/low.m3u8 #EXT-X-STREAM-INF:BANDWIDTH=2560000,AVERAGE-BANDWIDTH=2000000 http://example.com/mid.m3u8 #EXT-X-STREAM-INF:BANDWIDTH=7680000,AVERAGE-BANDWIDTH=6000000 http://example.com/hi.m3u8 #EXT-X-STREAM-INF:BANDWIDTH=65000,CODECS=\u0026#34;mp4a.40.5\u0026#34; http://example.com/audio-only.m3u8 大意是在平均流量6000000時播放 hi.m3u8，在比較低點的1000000播放 low.m3u8，如果是針對音檔而已則播 audio-only.m3u8。\nMedia Playlist 就是 Master Playlist中 Streaming指定的串流播放清單，主要說明要播放的片段URI、播放時長、如何解碼影片或是解密內容等等\n1 2 3 4 5 6 7 8 9 #EXTM3U #EXT-X-TARGETDURATION:10 #EXTINF:9.009, http://media.example.com/first.ts #EXTINF:9.009, http://media.example.com/second.ts #EXTINF:3.003, http://media.example.com/third.ts 這個 Media Playlist 說明了大約最大個Media Segments播放時長為10秒；\n第一個片段 URI 是 http:…/first.ts 長度為 9秒；\n第一個片段播完換第二個片段，以此類推。\nMedia Segments Media Playlist 是由多個 Media Segments 而組成，每個Media Segments 都是必須註明檔案來源(URI)，且基本上需要是與前一個Media Segments 相連續的內容，也就是 1 播完換 2 ，2 播完換 3 的概念，除非有顯示宣告兩個Media Segments 是不連續的。\n任一個Media Segments 都必須帶有足夠的資訊讓播放器可以解碼，在不同格式下，有些格式的初始化片段(Media Initialization Segments) 是相同的，這時候可以宣告 EXT-X-MAP 指定，後續的Media Segments 就可以共用。\n以下常見的支援格式有\nMPEG-2 Transport Stream Fragmented MPEG-4 Packed Audio(音訊) WebVTT(字幕) 前兩者是不同的影音編碼格式，MPEG-2被廣泛應用於有線電視、DVD等，而MPEG-4 是基於 MPEG-2 改良，各自有不同的格式與 Media Initialization Segments 的格式。\nWebVTT通常需要額外的 EXT-TIMESTAMP-MAP 去與影音播放校時。\nPlaylist Playlist 的檔名必須是 .m3u8 / .m3u ，並且回傳的 HTTP Response Content-Type必須為 application/vnd.apple.mpegur 或是 audio/mpegurl ；\n編碼必須採用 utf-8 ，除了 CR / LF 外不能有其他 utf-8定義的控制碼。\n在Playlist中，每一行可以是 URI / 空白或是由 # 開頭的字串；\n特別注意 # 開頭的字串可以是標籤或是註解，標籤像是 #EXT-X 開頭，且區分大小寫(標籤全大寫)\nURI 部分可以是相對路徑，相對於 Playlist 的URI。\n接著介紹Playlist的組成標籤，這部分主要是節錄 Spec內容，因為是名詞定義有點枯燥，有興趣可以深入查詢，後續會有實際範例。\nBasic Tags Playlist 可分成 Media Playlist 與 Master Playlist，以下為兩者共用的標籤。\nEXTM3U\n指定檔案為 Extented M3U，必須是檔案的第一行(雷同於html的 DOCTYPE) EXT-X-VERSION\n指定版號，目前最新版為 7，整份 Playlist只應該出現一次。 Media Segments Tags Media Segments Tag 會應用於下一個 Media Segment 或是後續所有的Media Segment 上。\nEXTINF\n註明該 Media Segment的時間長度，這個標籤是必須的。\n格式為 #EXTIF:\u0026lt;duration\u0026gt;,[\u0026lt;title\u0026gt;] duration 單位為秒，建議用浮點數表示，如果版號小於三才使用整數 EXT-X-BYTERANGE\n註記該 Media Segment只截取某範圍而非整個完整檔案\n格式為 #EXT-X-BYTERANGE:\u0026lt;n\u0026gt;[@\u0026lt;o\u0026gt;] if( o ) {\no 代表 byte offset，n表示從o開始取幾個 byte\n} else {\no 取上一個 Media Segment的結尾\n} EXT-X-DISCONTINUITY\n先前說每個 Media Segment 都必須是連續的，但如果檔案格式、影音軌(Track)的數字/型別等、timestamp 有改變，就必須要顯示宣告 EXT-X-KEY\nMedia Segment 可以被加密，而#EXT-X-KEY則是宣告解密的方式 EXT-X-MAP\n定應 Media Initialization Segments 來源，效力應用於每個Media Segment 上；\n格式為 #EXT-X-MAP:\u0026lt;Attribute-List\u0026gt; EXT-X-PROGRAM-DATE-TIME\n宣告第一個Media Segment 的絕對時間，格式為 ISO-8601，須包含時區，例如 #EXT-X-PROGRAM-DATE-TIME:2010–02–19T14:54:23.031+08:00 EXT-X-DATERANGE\n指定該時間範圍內的屬性定義，這部分我看不懂在幹嘛OTZ Media Playlist Tags EXT-X-TARGETDURATION\n單個 Media Segments 的 EXTINT最大值，單位為秒，格式是整數，此標籤為必填。 EXT-X-MEDIA-SEQUENCE\n註記第一個Media Segment的序號，如果不存在則預設為0；\n此標籤需出現於所有Media Segments 之前 EXT-X-DISCONTINUITY-SEQUENCE\n用於多個串流同步的序號。 EXT-X-ENDLIST\n不會有更多的 Media Segment被加入 Playlist中，可以出現在清單的各處 EXT-X-PLATLIST-TYPE\n可以是 EVENT 或是 VOD\nEVENT 表示 Media Segment 會持續加到 Playlist之後\nVOD表示 Playlist 不會改變了 EXT-X-I-FRAMES-ONLY\n每個 Media Segment都是獨立的I-Frame，影片的編碼與前後沒有相依性，可用於快轉、快速倒帶、關鍵影格等場景＊ Master Playlist Tags EXT-X-MEDIA\n註明在不同的場景(例如純音檔/英文字幕/法文字幕等)下播放不同的 Media Playlist，也就是不同的Rendition，但這些Playlist都必須是相同的內容；\n其中有許多屬性可以設定\nTYPE:可以是AUDIO/VIDEO/SUBTITLE/CLOSE-CAPTIONS(隱藏字幕)\nURI:對應的Media Playlist 資源位置\nLANGUAGE\nASSOC-LANGUAGE：用於其他地方的語言指定，如語音 vs 文字\nDEFAULT：預設播放此Playlist\nGROUP-ID：自定義的名稱\n等等 EXT-X-STREAM-INF\n宣告 Variant Stream，屬性有\nBANDWITDH：加總Media Playlist的最大 Segment Bit rate\nAVERAGE-BANDWITDH：平均每秒幾個bits\nCODES：編解碼格式\nVIDEO：其值需對應#EXT-X-MEDIA 的GROUP-ID ，且該#EXT-X-MEDIA 的TYPE必須是 VIDEO\nAUDIO：狀況雷同\n#EXT-X-STREAM-INF與#EXT-X-MEDIA 相輔相成 EXT-X-I-FRAME-STREAM-INF EXT-X-SESSION-DATA\n任意的 Session值 EXT-X-SESSION-KEY\n預先加載在 Media Playlist宣告的 EXT-X-KEY的值 Media or Master Playlist Tags EXT-X-INDEPENDENT_SEGMENTS\n註記每個Media Segments 可以單獨decode而不需要其他 segment的資訊\n如果出現在Master Playlist對所有 Media Playlist都有效 EXT-X-START\n從何處開始播放清單\n需指名 TIME-OFFSET屬性，正值代表從影片開頭開始算，負值則從影片尾端開始算\nTIME-OFFSET值不可超過影片的長度，如果超過則正值從尾部算而負值從頭算(一個循環的概念) 以上是所有的標籤，也不是每個意思都很好理解，後續盡量以範例說明\nKey files EXT-X-KEY的URI屬性標記Key的檔案位置，用於解密Media Segments；\n目前的加密方式可支援 AES-128，如果採用此加密方式，則需要 IV值，用於增強AES-128加密的初始值。\nClient / Server Responsibilities 第六章主要定義Server / Client 遇到不同標籤的處理行為，算是上面的大統整，以及實做上的引導說明。\nServer 負責產生Playlist與提供Media Segments\n實際如何產生來源媒體檔不再 Spec範疇中，而Server必須負責把來源媒體檔分割成連續且時長小於總目標時長的Media Segments，需注意當Server分割檔案時需要切在 packet / key frame 上；\n接著針對每個Media Segment分配特定的URI，如果Server支援 Byte-Range GET則可以在Playlist中使用 EXT-X-BYTERANGE\n當Media Segment 被運用在 Playlist中，Server必須確保資源可以被取得且不會出錯，且下載必須是立即下載而非 Streaming\n如果TYPE是 VOD，Server 產生出 Playlist就不應該修改；\n如果TYPE 是 EVENT，已經產生的Playlist也不能修改，只能在Playlist之後繼續增加行數。\n如果Media Playlist沒有包含 EXT-X-ENDLIST，則Server 必須提供包含新的Media Segments 來更新Playlist，而提供的時間必須等 0.5 ~ 1.5 目標時間內，因為這樣才能最大化利用網路效能。\nClient 當Client 收到Master Playlist ，可以選擇一個 Variant Stream 播放，Client 必須檢查版號與標籤的語法，如果不合則停止播放，但為了更好的向前相容，遇到無法識別的標籤或屬性值就自動忽略。\n如果 Media Playlist TYPE 是 Event 且沒有 EXT-X-ENDLIST 則必須定期重載 Playlist 以便獲取最新的 Media Segments，但為了不帶給Server過多負擔，重新獲取的時間差必須在至少一倍的 target duration，如果第一次重載後沒有更新則至少等半個target duration 再重試。\n這部分內容很多，有興趣可以慢慢翻，主要就是講如何載入資源與遇到標籤要怎麼處理等等\nExamples Example Playlists for HTTP Live Streaming | Apple Developer Documentation\nSpec 中也有範例，但這邊我擷取Apple Developer 的範例\n安插廣告 1 2 3 4 5 6 7 8 9 #EXTM3U #EXT-X-TARGETDURATION:10 #EXT-X-VERSION:4 #EXT-X-MEDIA-SEQUENCE:0 #EXTINF:10.0,ad0.ts #EXTINF:8.0,ad1.ts #EXT-X-DISCONTINUITY #EXTINF:10.0,movieA.ts #EXTINF:10.0,movieB.ts 注意在 ad1.ts 與 movieA.ts 中有個 #EXT-X-DISCONTINUITY ，因為 Ad 跟 Movie 勢必是兩個不同的片段，所以必須加入才不會因為 timestamp 銜接不上而導致播放錯誤！\nMaster Playlist with Alternative Audio 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #EXTM3U #EXT-X-MEDIA:TYPE=AUDIO,GROUP-ID=\u0026#34;aac\u0026#34;,NAME=\u0026#34;English\u0026#34;, \\\\ DEFAULT=YES,AUTOSELECT=YES,LANGUAGE=\u0026#34;en\u0026#34;, \\\\ URI=\u0026#34;main/english-audio.m3u8\u0026#34; #EXT-X-MEDIA:TYPE=AUDIO,GROUP-ID=\u0026#34;aac\u0026#34;,NAME=\u0026#34;Deutsch\u0026#34;, \\\\ DEFAULT=NO,AUTOSELECT=YES,LANGUAGE=\u0026#34;de\u0026#34;, \\\\ URI=\u0026#34;main/german-audio.m3u8\u0026#34; #EXT-X-MEDIA:TYPE=AUDIO,GROUP-ID=\u0026#34;aac\u0026#34;,NAME=\u0026#34;Commentary\u0026#34;, \\\\ DEFAULT=NO,AUTOSELECT=NO,LANGUAGE=\u0026#34;en\u0026#34;, \\\\ URI=\u0026#34;commentary/audio-only.m3u8\u0026#34; #EXT-X-STREAM-INF:BANDWIDTH=1280000,CODECS=\u0026#34;...\u0026#34;,AUDIO=\u0026#34;aac\u0026#34; low/video-only.m3u8 #EXT-X-STREAM-INF:BANDWIDTH=2560000,CODECS=\u0026#34;...\u0026#34;,AUDIO=\u0026#34;aac\u0026#34; mid/video-only.m3u8 #EXT-X-STREAM-INF:BANDWIDTH=7680000,CODECS=\u0026#34;...\u0026#34;,AUDIO=\u0026#34;aac\u0026#34; hi/video-only.m3u8 #EXT-X-STREAM-INF:BANDWIDTH=65000,CODECS=\u0026#34;mp4a.40.5\u0026#34;,AUDIO=\u0026#34;aac\u0026#34; main/english-audio.m3u8 如果希望可以讓Video 依據不同的頻寬切換，而音檔根據不同的語言選擇切換，就可以參考這個範例\n#EXT-X-MEDIA 定義了Rendition，透過LANGUAGE指定語言，接著TYPE表明為AUDIO聲音檔，GROUP-ID自取為 aac；\n而#EXT-X-STREAM-INF 定義的是 Variant Stream，AUDIO指定為 aac，也就是對應 #EXT-X-MEDIA 的GROUP-ID，其URI則指定播放 video。\n可以看出常用的標籤就那幾個，有些標籤我來回看了幾次還是看不懂，像是 EXT-X-DATERANGE就很難懂，範例也都沒用上，如果有人知道實際上的應用與原理再麻煩留言了OTZ\n結語 整份Spec 死嗑完很多部份也還是懵懵懂懂，但至少有個全局的認知與介紹，下一章從影片處理到Playlist修改，來看實際應用的處理。\n註記 影片編碼 參考維基百科\n視訊壓縮圖像類型 - 维基百科，自由的百科全书\n影像是一幀一幀的圖片連續快速播放，但如果儲存時把每一幀都用圖片方式儲存資料量會非常大，但是影片往往一段時間的場景雷同，如果我可以先記第一張圖，其餘去計算第二張與第一張差多少的概念去儲存，就可以避免儲存過多相同冗余的數據\n但因為要避免掉了一點資料就全毀，保留一些彈性所以會在不同時刻安插取全圖的時機。\n影片壓縮透過建立 I-Frame / P-Frame / B-Frame方式\nI-Frame 關鍵影格，基本上就儲存並壓縮整張圖，作為其他Frame解碼的依據\nP-Frame則僅儲存與上一個 I Frame差異的像素\nB-Frame則為前後預測圖像，會參考前一個 I-Frame 與後一個 P-Frame\n所以大致會長成 IBB..PIBB..P，取 I-Frame / B-Frame 的頻率可以停整\n影音檔與 FFMpeg 教學：ffmpeg-libav-tutorial\n","date":"2018-09-24T09:54:09.34Z","permalink":"https://yuanchieh.page/posts/2018/2018-09-24-hls-%E6%95%99%E5%AD%B8-%E4%B8%8A-%E5%BE%9E%E9%96%B1%E8%AE%80spec-%E9%96%8B%E5%A7%8B/","title":"HLS 教學 (上) — 從閱讀Spec 開始"},{"content":"在大量資料需要儲存下，可以將 MongoDB做 Sharding 與 Replica Set 設置，增加DB的吞吐量與可用性。\n參考 MongoDB的官網，可以很簡單的就架起 Shard Cluster 架構，以下簡單記錄實作過程。\n目前使用 v3.4，不採用最新版 4.0 是因為公司環境用 3.4，但看了文件好像差不太多。\nWebinar: Everything You Need to Know about Sharding\n強烈建議看完這部一小時的影片，整個觀念非常清楚。\n資料庫設定必須從最一開始的\n「會有多少資料要儲存？ 結構大概是如何？ 需要保存多久或是有什麼業務需求？寫入會大概是怎樣情況？讀取又會是如何？會需要高吞吐量又或是低延遲嗎？」\n這些因子會根本性決定資料庫的設計，包含 是否要 sharding / database、collection、index 如何設計，甚至是主機的規格與地點選擇。\n架構圖 截圖自官網\n主要有三個部分：\nMongos：對外連接 DB Client 接收對DB的讀寫，接著再根據 Config Server 決定到哪個 Shard 讀寫(會cache住查詢結果) Config Server：紀錄 metadata與資料在哪個 shard 上，3.4之後必須是replica set 架構 Shard：分片儲存資料處，建議採用 replica set 架構 最基本要開 1台 Mongos / 3 台 Config Server / 2 組Shard Replica Set 共 11台，分享一下我在 AWS 上開 11台 t2.nano 配 EBS 一天大概兩美金。\n以下預設每台機器都安裝好 mongo@3.4\n實作 Config server 因為有些設定檔寫成 config file比較方便，以下是我的 mongo.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # mongod.conf # for documentation of all options, see: # [http://docs.mongodb.org/manual/reference/configuration-options/](http://docs.mongodb.org/manual/reference/configuration-options/) # where to write logging data. systemLog: destination: file logAppend: true path: /var/log/mongodb/mongod.log # Where and how to store data. storage: dbPath: /home/ec2-user/fs journal: enabled: true # how the process runs processManagement: fork: true # fork and run in background pidFilePath: /var/run/mongodb/mongod.pid # location of pidfile # network interfaces net: port: 27017 bindIp: 172.31.26.157,127.0.0.1 # Listen to local interface only, comment to listen on all interfaces. sharding: clusterRole: configsvr replication: replSetName: config_replica 三個參數要注意，其他都是預設值\nsharding：\n關於sharding的設定，記得config server 都必須設成clusterRole: configsvr replication：\n每個replica set 都要一組 name分辨 net：\n注意 bindIp 除了 127.0.0.1 外，還要加一個其他機器可以查找的ip，因為replica set 機器間必須要可以溝通，這邊我是用內部IP\n另外 127.0.0.1也要保留，有些權限操作必須用 localhost(如關閉Server) 啟動 server 指令是\nsudo mongod \u0026ndash;config mongo.conf \u0026ndash;fork \u0026ndash;syslog\n加 —-fork是為了跑在背景，三台 config server 都一樣，接著選一台當作 Primary，進入後做 replica set 設定\n這部分因為我看一開始的config 不能寫在一起，必須要 mongod啟動後再執行，我是另外寫script，接著直接喂進 mongo shell 執行\nreplica.js，_id對應是剛才的 replicaSetName，members就是三台 config server，注意要是可以連線的 ip。\n1 2 3 4 5 6 7 8 9 rs.initiate({ _id: \u0026#34;config_replica\u0026#34;, configsvr: true, members: [ {_id: 0, host: \u0026#34;172.31.26.157:27017\u0026#34;}, {_id: 1, host: \u0026#34;172.31.23.205:27017\u0026#34;}, {_id: 2, host: \u0026#34;172.31.16.98:27017\u0026#34;}, ] }) 指令是\nmongo \u0026lt; replica.js\nSharding Cluster 這部分也差不多，只差在 mongo.conf的sharding 參數\nsharding:\nclusterRole: shardsvr\n剩下的 replica set 設定都跟 config server 一樣\nMongos 接下來是Mongos，同樣用mongo.conf 啟動 mongos\n1 2 3 // mongos.conf sharding: configDB: config_replica/172.31.26.157:27017,172.31.23.205:27017,172.31.16.98:27017 執行指令\nsudo mongos \u0026ndash;config mongos.conf \u0026ndash;fork \u0026ndash;syslog\n接著就是設定 sharding，這邊一樣寫個 js script\nsh.addShard( \u0026ldquo;/,,\u0026rdquo;)\n\u0026hellip;. 有幾個加幾個\n這樣就完成了\n接著可以登入 mongo shell 檢查\n1 sh.status() 影片建議將 mongos 與 APP Server 放在一起\n設定 Sharding 架設好了之後，接著就是要設定 sharding 機制，目前可以針對某個DB的某個Collection 的 Key 做 hash shard / range shard，\nrange shard 是當 key 在某個範圍內就往哪邊塞；\nhash shard 則是會把 key 做 hash 比較容易分散。\n// 以下指令在 mongo shell\n// 首先啟動sharding\nsh.enableSharding(\u0026quot;\u0026quot;)\n// 針對某個 collection 設定\nsh.shardCollection(\u0026quot;.\u0026quot;, { : })\n1 2 3 // example sh.shardCollection(\u0026#34;test.test2\u0026#34;, {\u0026#34;name\u0026#34;: \u0026#34;hashed\u0026#34;}) sh.shardCollection(\u0026#34;test.test\u0026#34;, {\u0026#34;name\u0026#34;: 1}) 必須注意到如果在sharding 前collection裡面就有值，要先建立 index 才可以設定 shard。\n查看狀態可以用 sh.status()\n注意事項 有蠻多小細節與限制要注意\nChunk Size： Chunk 是分片儲存的一個單位，每當有寫入紀錄時會塞到Chunk中，如果超過Chunk Size ，Balancer 平衡器就會把Chunk 拆成兩半分散到不同的Shard去\n所以如果 Chunk size 太小會造成過度頻繁的拆分與搬遷，導致性能低落；\n如果Chunk size 過大則會造成資料分散不均勻\n1 2 3 // in Mongos server -\u0026gt; mongo shell use config db.settings.save( { _id:\u0026#34;chunksize\u0026#34;, value: \u0026lt;sizeInMB\u0026gt; } ) 特別注意，MongoDB初始化的 chunk size 為 1，也就是所有的寫入一開始都會往一個 chunk 塞，導致後續的再平衡很消耗性能，所以建議要調整 numInitialChunks 或是自行初始化 chunk size。\nhttps://docs.mongodb.com/manual/reference/method/sh.shardCollection/\nhttps://docs.mongodb.com/manual/tutorial/create-chunks-in-sharded-cluster/\nsharding key: 已知 chunk 超過size 會由 Balancer 負責平衡，如果可以在一開始選好一點的 sharding key 就能避免太多次的拆分與平衡，讓資料均勻的散佈。\n選擇 range shard，要避免選擇會遞增的Key，例如預設的_id ，預設_id 是會隨著 timestamp 而遞增，這會導致插入新資料都只會插到同一個 chunk 中，勢必要不斷的拆分！\n如果要用 _id，就很適合用 hash shard，因為透過 hash function 遞增的key也比較會被均勻分散；\n在配置 hash shard上，如果一開始 collection 是空的，MongoDB可以設定 numInitialChunks要初始化多少個 chunk；\n但要記得 key 不能是 float ，因為 hash function 會把 key 先轉乘 64 bit int，所以 2.1 / 2.2 / 2.9 都會被分散到同一個 chunk 上。\n參考影片的步驟，\n如果選則 data center id 不是好的id，因為可能造成資料不平均 如果選擇 timestamp 不是好 id ，因為是遞增，所以新增會插到同一個 chunk 如果選擇 hash(timestamp) 不是好 id，因為讀取時會需要到多個 shard 選擇 device_id 配合 hash shard 是個不錯選擇(假設 device_id 是固定的且每個 device 生成差不多的資料) 但如果讀取大多是要讀某個裝置近期某個時間的資料，那改用 (device_id, timestamp) 的組合 key 最理想。 1. Cardinality 高度異質性\n2. Write distributed 寫入分布均勻\n3. Query isolation 讀取可以集中於同一個 shard\n4. reliability 如果某個shard掛了，盡量不要影響到所有的搜尋\n5. Index locality 盡量讓讀取可以套用最多的索引\n注意：shard key 設定了就不能改變，如果要改就必須重新 sharding，務必謹慎。\n指令限制 根據文件 ，針對單一文檔操作如 updateOne / deleteOne必須附帶 shard key；\n$group 不能使用，必須改用 mapReduce / aggregate 方法。\n每個 shard key 都必須是 index或是 compound index的prefix，同時可以設定為 unique，但必須遵守\n1. 如果該文檔已經 sharded，不能把其他欄位設為 unique\n2. 如果其他欄位是 unique，則無法對此文檔 shard\n總之，要unique 只能是 shard key欄位。\nTag-Aware Shard 除了range shard / hash shard之外，還可以針對個別 shard 做標記，接著就可以透過條件設定讓某些資料固定儲存在該shard上，這樣最大的好處是可以做地理位置優化 ，例如美國用戶就可以讀寫在美國區的 shard上。\n1. 先增加Tag\nsh.addShardTag(, \u0026ldquo;EU\u0026rdquo;)\n2. 指定 tag shard 的條件，可以設置多個\n1 2 3 4 5 6 7 8 9 10 11 12 sh.addTagRange( \u0026#34;chat.messages\u0026#34;, { \u0026#34;country\u0026#34; : \u0026#34;US\u0026#34;, \u0026#34;userid\u0026#34; : MinKey }, { \u0026#34;country\u0026#34; : \u0026#34;US\u0026#34;, \u0026#34;userid\u0026#34; : MaxKey }, \u0026#34;NA\u0026#34; ) sh.addTagRange( \u0026#34;chat.messages\u0026#34;, { \u0026#34;country\u0026#34; : \u0026#34;DE\u0026#34;, \u0026#34;userid\u0026#34; : MinKey }, { \u0026#34;country\u0026#34; : \u0026#34;DE\u0026#34;, \u0026#34;userid\u0026#34; : MaxKey }, \u0026#34;EU\u0026#34; ) 接著 mongodb 就會自動平衡\n參考資料 非常詳細的簡中文章：http://www.cnblogs.com/zhoujinyi/p/4635444.html\n備註 根據官網，最好要有安全設定與角色配置，實作上必須要特別留意。\n","date":"2018-09-08T07:53:36.68Z","permalink":"https://yuanchieh.page/posts/2018/2018-09-08-mongodb-shard-cluster-%E6%9E%B6%E8%A8%AD/","title":"MongoDB Shard Cluster 架設"},{"content":"以前只注重把功能寫出來而已，慢慢地開始維護後發現一開始的系統規劃很重要，包含基本的 Loggin / Debugging / Error Handling，以及是否能將每個物件函式乾淨拆分[避免過多副作用無法編寫測試](https://medium.com\n此次主要研究 Express 與 Koa 框架下編寫如何做錯誤處理。\n原本的寫法 在最一開始使用Promise時，都習慣個別Promise.catch 處理錯誤；\n之後用上了 async / await ，也都習慣用 try{}catch(){} 個別處理；\n1 2 3 4 5 6 7 8 9 10 11 function handle(req, res) { Promise1().then(data =\u0026gt; ....).catch(err =\u0026gt; handleError(err)) } async function handle(ctx){ try{ await Promise1() }catch(err){ hanleError(err) } } 這樣的寫法缺點在於每個函式都必須重複一樣的事情(不斷 try catch)，此外回傳 http status code / error message 很多都會重複，也是此次想要改變的問題，希望改以統一拋出錯誤並註冊一個Middleware專門處理錯誤。\n改善寫法 Koa 先來看Koa如何實作，以下是一般使用方式\n1 2 3 4 5 6 const Koa = require(\u0026#39;koa\u0026#39;); const app = new Koa(); app.use(async (ctx, next){...}); app.listen(3000); 當我們創建一個 Koa 的 instance app後，接著就會用 app.use 註冊所有的 Middleware，最後就是 app.listen 啟動\n在 Koa 原始碼當中， new Koa()中重要的一段原始碼是\nreturn fnMiddleware(ctx).then(handleResponse).catch(onerror);\nKoa 處理的順序是 fnMiddleware將 app.use()註冊的全部Middleware轉成 Promise chain -\u0026gt; hanldeResponse(呼叫 res.end送出 http respond) / onerror (處理錯誤)\nPromise chain(自定義的) 是指當 Koa Middleware 呼叫 next() 會遞迴呼叫下一個 Middleware，有興趣可以看我另一篇文章 Koa2 源碼解析 — 簡潔美麗的框架\n這部分的錯誤處理又可拆成兩塊，一個是 app層級 一個是 ctx 層級；\n在原始碼中 /koa/application.js，有預設基本的 onerror的錯誤處理，基本上就是打印出來，這部分是透過 app本身繼承 Event Emitter屬性並註冊 app.on(“error”) 事件後處理\n另一方面當 Server 發生錯誤 Client 都會收到 Internal Server Error 回應，是在 ctx.onerror 處理，並 app.emit(“error”)，定義於 /koa/context.js 中；\n對應不同的Http Status Code 有不同的回應，這是基於statuses 模組定義的。\n這裡比較混亂的是 app.onerror 與 ctx.onerror 呼叫時間是交錯的\nfnMiddleware(ctx).then(handleResponse).catch(onerror); 的 onerror 是 (error) =\u0026gt; ctx.onerror(error) ctx.onerror 會 respond 預設的錯誤處理與 app.emit(“error”) app.emit(“error”) 是由 app.onerror 去接收，這部分可以自訂 app.on(“error”, ()=\u0026gt;{自行處理}) 官方文件有寫到錯誤處理，用戶可註冊 error 事件就會改由用戶自己處理\napp.on(\u0026ldquo;error\u0026rdquo;, (err, ctx) =\u0026gt; {\u0026hellip;.})\nBUT!! 這邊的 ctx 是拿來看 context 資訊，如果是希望客製化回傳的錯誤是沒辦法的喔！\n因為錯誤的狀態碼與內文 Reponse 是在 ctx.onerror 就處理掉。\n所以如果要自己處理整個錯誤，必須改用 Middleware ，記得第一個就註冊錯誤處理才可以捕獲所有的錯誤，如以下\nExpress Express 對比 Koa 是個比較全面的框架，內含了基本的 Middleware / Router，發送Response 方式也不像 Koa 是最後框架幫你發送；\n而是必須自己用 res.send() 之類的語法自行發送。\nRouting 機制 詳請請參考 express源码分析之Router\n在Express 實例化之後有一個 Router Instance，接著每個路由會對映一個 Route，一個路由中可能有多個Middleware 稱為 Layer，資料結構就是陣列儲存。\n當一個 Request 近來會流過依照 Route 中的註冊順序流過 Layer ，每個Layer 判斷是否 Match URL，如果 Match 則處理，發生錯誤則走錯誤處理\n特別注意 正常處理與錯誤處理路徑是分開\n1 2 app.use((err, req, res, next)=\u0026gt;{}) 對應是 Layer.handle_error app.use((req, res, next)=\u0026gt;{}) 對應是 Layer.handle_request 當發現有 Middleware 呼叫了 next(err)後，就開始走錯誤處理，也就是後面的 Layer 都是呼叫 Layer.handle_error !\n在錯誤處理Express 採用 next(err) 在Middleware 間傳遞錯誤，所以當 Middleware 或是 Routing 中有錯誤不能直接拋出或不處理，必須要用 try{..}catch(error){next(error)} 處理；\n但這樣就會很麻煩，因為都必須用 try catch 包起來，同理像是原生的 Nodejs module 如 fs 都沒有 Promise版，所以都要自己再 Promisify 後呼叫一樣的道理(麻煩)。\n(Using Async Await in Express with Node 9 有提及)\n後來看到一個蠻 Hacking 的方式，express-async-errors，他會複寫Express/Layer.handle 方法，把每個 Routing Function 的錯誤統一用 next(error) 傳遞\n他處理的方式也是很妙，簡化後可以這樣示意\n1 2 3 let pE = async () =\u0026gt; {throw new Error(\u0026#34;error\u0026#34;)} let fn = pE.call() if(fn \u0026amp;\u0026amp; fn.catch) fn.catch(err =\u0026gt; console.log(err)) 也就是如果發現註冊在Layer中的Function 是 Async Function，Async Function 執行後會回傳 Promise，接著就是用 fn \u0026amp;\u0026amp; fn.catch 判斷是否為 Promise，如果是則幫忙補上 .catch((err) =\u0026gt; next(err)) ，蠻聰明的作法。\n根據 Express Routing 機制，ErrorHandling 在 Express 必須宣告在最後面\n結語 就個人觀點，Koa 相比 Express 確實是個更進步的框架，最主要是在 Middleware 構建與執行上，Koa 是先轉成類似 Promise Chain，並預設有做 Error Handling；\n這比較符合現在以 Promise 為基礎構建的應用程式，也使得 Middleware 設計與錯誤處理直觀很多。\n而Express 必須很彆扭的使用 next(err)傳遞，對比就有點像 callback hell 的 error 放在 function 第一位的傳統寫法；\n另外我也是現在才知道 app.use((err,req,res,next)=\u0026gt;{…}) /app.use((req,res,next)=\u0026gt;{…}) 差一個參數在 Express 中呼叫時機完全不同，整個錯誤處理弄的有點不太直觀。\n看到Github Issue 討論有提到 Express@5 會加入更好的 async /await 支援，到時再來看看原始碼的更動。\n","date":"2018-08-27T07:20:36.575Z","permalink":"https://yuanchieh.page/posts/2018/2018-08-27-express-%E8%88%87-koa-%E5%A6%82%E4%BD%95%E8%99%95%E7%90%86%E9%8C%AF%E8%AA%A4/","title":"Express 與 Koa 如何處理錯誤"},{"content":"Postgresql 可以用欄位 jsonb 型別儲存 json 格式的資料，並提供不少內建的函式可以協助查詢，以下稍微整理一些常用的情景。\n示範資料庫 DB Fiddle - SQL Database Playground\n基本就是是 Orders / Products 兩張表，Orders 中資料用 data jsonb 格式存儲，Products 則是常規的欄位定義。\nOrders 資料大約長這樣\n1 2 3 4 5 6 { \u0026#34;id\u0026#34;: 1, \u0026#34;cost\u0026#34;: 200, \u0026#34;products\u0026#34;: [{\u0026#34;id\u0026#34;: 1, \u0026#34;nums\u0026#34;: 5}, {\u0026#34;id\u0026#34;: 2, \u0026#34;nums\u0026#34;: 3}], \u0026#34;details\u0026#34;: {\u0026#34;title\u0026#34;: \u0026#34;memo\u0026#34;} } 操作 JSON 如果是要做 json 的欄位索取，可以用 -\u0026gt; ，如果要回傳字串結果用 -\u0026gt;\u0026gt;\n例如說 data-\u0026gt;’details’-\u0026gt;\u0026gt;’title’ 就會回傳 “memo”\n陣列也可以指定哪個位置，例如 data-\u0026gt;’product’-\u0026gt;0-\u0026gt;’id’\n使用上如果是 key 用 ` 單引號括著，如果是陣列位置才不需要，不然會一直回傳 null\n另外要特別注意的是-\u0026gt;\u0026gt; 都是回傳字串，所以如果要跟其他類型做比對，需要自己做格式轉換，例如 Cast( data-\u0026gt;\u0026gt;”id” as int ) = id 拿 data.id 跟 int 型別的自增主鍵做比對\n陣列長度 如果是想要知道陣列的長度是多少，可以用 jsonb_array_elements\n例如 jsonb_array_length(data-\u0026gt;’products’) \u0026gt; 2 ，這會直接回傳 int\nKey 是否存在 有三種用法，?’比對key’ ?|[’比對key1’, ’比對key2’] ?\u0026amp;[’比對key1’, ’比對key2’] ，結果回傳 boolean\n例如找出 data 中包含`detail`欄位的資料\nselect * from Orders where data?’detail’;\n?| 則是後者陣列中只要命中一個就為 true，?\u0026amp; 則是要陣列中所有的元素都存在才是 true。\n攤平陣列 算是做第一正規化，我希望透過 Join Products 計算每筆訂單的總額，但因為 Orders 中的 data-\u0026gt;product 是陣列，所以需要 jsonb_array_elements ，jsonb_array_elements 會直接回傳 set\nselect jsonb_array_elements(data-\u0026gt;’products’) as product_item from Orders; 這樣就可以做到第一正規化\n接著蠻神奇的是可以直接將 Order 與 jsonb_array_elements 做 CROSS JOIN\n1 2 3 4 5 select Orders.id, Sum(Cast(product_item-\u0026gt;\u0026gt;\u0026#39;nums\u0026#39; as int) \\* Products.price) from Orders, jsonb_array_elements(data-\u0026gt;\u0026#39;products\u0026#39;) as product_item left join Products on Cast(product_item-\u0026gt;\u0026gt;\u0026#39;id\u0026#39; as int) = Products.id group by Orders.id; 但結果竟然不是預期的笛卡爾積 N*M，而是會自動幫你把屬於該訂單的 product 對應好，到現在還想不透怎麼會這樣。\njson vs jsonb Postgresql 支援兩種合法 json 可插入的格式 json jsonb ，但兩者有些微的差異\njson\n以純文字型態插入，包含空白、換行，同時保持原本的鍵值順序 jsonb\n以二進制方式儲存，會把重複的鍵值去除(後者覆蓋前者) jsonb 支援 index，在插入時需要較長的時間，但是查詢速度較快，是比較通用的選擇格式。\n但如果有需要保持原本鍵值順序、或是單純想保留原本的 json 格式才使用 json 格式\n參考資料\nhttps://my.oschina.net/swingcoder/blog/489769、https://stackoverflow.com/questions/22654170/explanation-of-jsonb-introduced-by-postgresql\n結語 有看到一些實驗是把 Postgresql 當作 NoSQL 使用，但個人覺得還是偏玩票性質，關聯式資料庫還是遵守正規化設計，僅把 json 當作額外的彈性就好\n","date":"2018-08-23T08:36:20.503Z","permalink":"https://yuanchieh.page/posts/2018/2018-08-23-postgresql-json-%E6%93%8D%E4%BD%9C/","title":"PostgreSQL json 操作"},{"content":"平常喜歡水上活動，但沒特別喜歡游泳，所以泳技、水性都很差，一開始先從捷式重新學起，兩個禮拜加強後學會換氣勉強可以遊完50公尺，接著就直接挑戰救生員訓練，真的是生不如死啊XD\n訓練菜單 訓練大概分成自救與救人；\n自救 一開始最基本的漂浮，又可分成水母漂、俯臥漂、仰漂、直立式漂浮；\n先練習建立自身浮力，以及換氣，在換氣過程中動作要輕，差不多嘴巴浮出水面可以換氣就好，不要太用力整個胸口浮出，避免反作用力過大反而身體會更沉\n接著是踩水 ，也就是立泳，讓身體保持在原地頭露出水面，有剪刀式踩水、蛙式踩水、攪蛋式踩水，這時候基本上是放鬆休息的時候，而且頭浮出水面才可以聽到教練的指令\n再來是借物建立浮力，如果是穿著衣服落水，可以運用衣物做緊急氣囊，建立自身浮力等待救援\n救人 當我們觀察到有溺者時，先假設沒有多餘的工具可以使用，只能依靠我們自身的能力去救援時，整個情境會是\n從岸邊或池邊入水，接著游泳過去接近溺者，觀察溺者並接近他，接著想辦法將他帶回岸上，如果遇到反抗或抓抱要想辦法掙脫，帶上岸後要做急救等後續處置。\n基本上整套SOP是\n入水法 -\u0026gt; 從遠處接近溺者 -\u0026gt; 接近法 -\u0026gt; 帶人法 -\u0026gt; (如果溺者掙扎)解脫法、防衛法 -\u0026gt; 起岸法 -\u0026gt; 救溺八大步驟 -\u0026gt; 後續急救處理\n也就是我們主要的訓練菜單\n入水法 又分成靜式入水，適用於水域不熟悉或是傷患有頸椎受傷之疑慮時採用，入水時不能激起水花；\n平跳式入水，有點像立定跳平飛出去，雙手交疊大臂貼緊耳朵壓頭，入水淺出水快，有個初速度可以快速接近溺者；\n跨步式入水，適用於水深處；海邊有浪時用跳躍式入水降低水阻；在救生艇上用背滾式入水。\n接近溺者 當從岸邊入水後，距離溺者有2、30公尺以上，此時會採取抬頭捷、抬頭蛙，基本上就是捷、蛙式的抬頭版，因為「所有動作眼睛都不能離開溺者」避免水流或是沈到水底找不到\n抬頭蛙基本上比較簡單，但是抬頭捷就比捷式累得多很多，因為頭抬起腳相對下沉，一開始划手抓水都會十分費勁\n接近法 當靠近溺者 2公尺左右要側身觀察，確定好自身安全、周圍環境與溺者狀態後才後續動作，避免危險發生或是被溺者抓抱。\n如果溺者無意識就直接抓手轉回仰面往回帶，相對於溺者的相對位置還有正面迂迴接近、背面接近、水中水底接近\n主要就是將溺者轉回仰面，用基本仰帶動。\n解脫法 溺者因為求生意志自然會四處抓抱物體，但救援路上最怕被抓抱，因為會消耗雙方力氣且非常危險，一不小心連自己的命都賠上去\n遇到抓抱，有分成被抓住手腕時的單手解脫，被溺者正面環抱的正面推肘解脫與背面環抱的背面推肘解脫；\n基本上都是先循機吸氣，接著撥水下沉，因為溺者是下意識想要建立浮力，所以你反而下沉他一怕就會鬆手了，如果不行則收下巴靠住手肘上方用力推即可掙脫\n防衛法 防衛法比較是碰到溺者撲上來但還沒有被緊抱住時使用，連續動作從單手阻擋-\u0026gt;雙手阻擋-\u0026gt;雙手下壓-\u0026gt;單腳蹬離；\n基本上就是對應溺者的自然反應而有的配套措施\n帶人法 接近法將溺者轉成仰面拖動後，為了更好控制溺者，通常會用抱胸式帶人，這大概是整個救溺環節最為累人的動作，救者採用側泳用腰將溺者頂著，接著用反剪夾水回到岸邊，考試夾完25公尺完全站不起來….\n起岸法 又分成深水淺水，溺者有無意識，主要有馬鞍式、背負式、拖拉式、深水起岸法等\n救溺八大步驟與急救 身為救生員都要通過16小時受訓的基礎急救課程，並學習救溺八大步驟；\n通常救生員第一線主要預防休克，給予保暖、止血包紮這些，為了避免二度與頸椎傷害，有學習操作長背板等使用\n如何身為一個良好的溺者 當然我們都不希望意外發生，但總是可能會有發生的風險，與其逃避不如花點心思準備，以備不時之需；\n以一個救生員受訓完，稍微用不同的角度來分享哪些溺者最容易被救上來，這些想法主要是結合自己受訓的心得與教練的指導，如果有遺漏或是錯誤再請指教\n在人多且有單位協勤的地方玩水\n在游泳池玩水相對蠻安全，如果要到野外最好是成群結伴，且到有救生單位協勤的地方，像是烏來的紅河谷就是新店同心救難隊夏日協勤地點；\n在這些地方玩水的好處除了有合格的救生員外，另一方面也相對會準備救生器材，如救生圈、拋繩袋、魚雷浮標、充氣式汽艇、救生板等\n如果是自己出去玩，泳技不太好或是水域不熟悉「務必攜帶浮標」，之前溪訓到碧潭，整個水溫分成三層，有時遇到地下水湧泉又會忽冷忽熱，結果我就游一游抽筋了，一方面有教練在旁邊頗安心，另一方面先抓到浮標建立浮力，就可以抽筋自解；\n所以學會建立浮力很重要，帶浮標靠外物可以很輕鬆的浮起來，或是學會基本的水母漂或抽筋自解也都很重要 呈上遇難時不要太緊張，不要抓抱救生員\n在最後考試時教練有模擬抓抱，原本抓抱前體力大概還有80%，抓抱一次解脫後直接剩20%，如果不是在考試途中我大概就直接落跑了OTZ 一方面是我自身體力泳技還不夠好，另一方面真的要帶人又要防衛溺者真的會很累，所以相對的如果溺者無意識了反而比較還好，有意識的話就配合，才可以大幅增加自己的生存機會；\n如果是在有協勤單位的地方戲水，都會被有妥當的救生用具，這些救生用具可以節省救生員很大部分的體力，相對也可以降低大家溺水的風險 總結：\n挑選安全的地方戲水 -\u0026gt; 不幸出事大聲求救 -\u0026gt; 建立自身浮力 -\u0026gt; 保持冷靜不要亂抓抱\n28天的課程，一到六早上五點到七點操練，練習動作與長泳鍛鍊體力，週日全天學習急救與到野外溪訓與海訓\n溪訓海訓 溪訓海訓真的很特別，之前我沒有太多到野外游泳的經驗，到了溪訓教練一下水長泳就是一個多小時…. 真的是超級累，而且要在踩不到底也看不到底的地方對鍊動作，心裡總是不踏實，最後溪訓就抽筋了\n海訓則是因為浮力大比較輕鬆點，雖然水喝到超痛苦，又苦又鹹喉嚨超痛，但整體輕鬆一些，也沒有抽筋，不過因為有浪所有同期學員有人就吐了\n除了折磨的長泳外，後續的課程就是操作救生用具，練習拋救生圈、學習收繩，還有充氣式皮艇之類的，非常好玩\n最後下午都會浮潛一個多小時結束整天的訓練\n結語 參加救生員訓練主要是希望給自己一個磨練的機會，在過程中學習到很多救生的知識，真的非常感謝教練們每天都陪我們早起，除了授課外還很仔細的糾正我們的姿勢，到野外受訓就有一群教練包圍著學員，一邊安全戒護一邊嚴厲的督促並指導我們\n原則上隊上每年七八月開兩期，所以有興趣也要等到明年囉XD 之後明年跟著隊上去協勤，希望也有為社會貢獻的機會。\n","date":"2018-08-21T12:28:19.157Z","permalink":"https://yuanchieh.page/posts/2018/2018-08-21-%E6%96%B0%E5%BA%97%E5%90%8C%E5%BF%83%E6%95%91%E9%9B%A3%E9%9A%8A-52%E6%9C%9F%E6%95%91%E7%94%9F%E5%93%A1%E8%A8%93%E7%B7%B4%E5%BF%83%E5%BE%97/","title":"新店同心救難隊 52期救生員訓練心得"},{"content":" 之前寫測試因為沒有注意細節，導致非常難編寫單元測試；\n改以 End-to-End測試，直接用docker 開DB輸入假資料，接著執行 Server App 對API一隻一隻測試。\n這樣的好處是測試方法與最終API調用的結果是一樣的，但缺點就是耗時較久，且邊寫測試的成本很高，要做到TDD之類的開發方式非常困難。\n另一方面之前用 Mocha，也算是 Nodejs 最大宗使用的測試框架，提供最基本的測試環境與語法定義，其餘的斷言、Mocking都要自己另外想辦法；\n這次改用jest，由FB開源可供前後端的測試框架，此次順便分享如何建構程式碼才方便寫測試的小小心得。\nJest 如何使用 Jest 使用上有點像 Mocha + Chai，除了測試環境外還以自定義的斷言，提供多種編寫語意化的測試情境。\n1 2 3 4 5 6 7 8 9 10 11 // ./sum.js function sum(a, b) { return a + b; } module.exports = sum; // ./sum.test.js const sum = require(‘./sum’); test(‘adds 1 + 2 to equal 3’, () =\u0026gt; { expect(sum(1, 2)).toBe(3); }); // 執行\n\u0026gt; jest\n另一點方便的是 Jest對於 Callback、Promise、Async/Await支援相當好，當初在Mocha中搞了一陣子….\n邊寫測試的小細節 使用Mock，透過預設情景與假資料，不用架設複雜的環境或是真的發出非同步請求，就可以達到測試的效果；\n在使用Mock之前，有一點要注意 盡量把API包成函式或整理成物件方法而非直接呼叫\n例如說在設計API: GET /user/:userId\n1 2 3 4 5 6 7 8 9 // 不要這樣直接呼叫 mongoose 方法 function getUserById(){ let user = await mongoose.findById(….) }// 拆開成獨立的函式，方便後續做 Mock function getUserById(){ let user = await getUserByIdInModel(id) }function getUserByIdInModel(){ return mongoose.findById(…) } 將非同步請求整理包成函式在調用有幾個好處\n1. 解耦，多一層抽象化\n試想今天是用MongoDB當作資料庫，但如果某天需要換成 PostgreSQL，把資料庫調用寫死在 API邏輯中會導致整份程式碼需要重寫；\n但如果拆開成獨立函式，就只要改函式調用的DB Client與對應原本的回傳資料格式即可(親身經驗\n2. 方便編寫測試\n非同步請求本身也可以獨立寫測試 (如果需要的話\n接著看 Jest如何生成假資料。\nMock 產生假資料可以分為 func\n假設我們原本的程式碼為\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // index.js const api = require(‘./apiWrapper’); const Obj = require(‘./objectWrapper’);exports.callAPI = async function(t){ let content = await api.readFile(t) return content }exports.callObject = async function(){ let obj = new Obj(“jay”) return obj.sayHi() } // ./util/apiWrapper.js const fs = require(‘mz/fs’);exports.readFile = async function(params){ return fs.readFile(“./test.txt”); } // ./model/objWrapper.js module.exports = class Test{ constructor(name){ this.name = name }sayHi(){ return \\`hi from ${this.name}\\` } } 基本上就是index.js對外輸出兩個函式，此兩個函式內有非同步請求與創建物件的方法，通常個人資料夾目錄是共用的非同步API Call會叫做 util，如果是資料庫相關的物件則是 model。\n接著看測試檔 index.test.js\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 const index = require(“./index”) jest.mock(“./util/apiWrapper”) jest.mock(“./model/objectWrapper”) test(“mock api test”, async ()=\u0026gt;{ let res = await index.callAPI(1); expect(res).toBe(“jest”); }) test(“mock api test2”, async ()=\u0026gt;{ let res = await index.callAPI(2); expect(res).toBe(“jest2”); }) test(“mock obj test”, async ()=\u0026gt;{ let res = await index.callObject(); expect(res).toBe(“hi from jest”); }) 這裡先定義了三個測試情境，並透過 jest.mock()顯式宣告 Mock檔案的位置；\n接著注意 mock預設有固定的資料路徑與命名規則。\n1 2 3 4 5 6 index.js index.test.js — util | — — apiWrapper.js | — — __mocks__ | — — apiWrapper.js 需注意mock要定義在檔案的同層 mocks 資料夾下，並且檔名需要一致且輸出同樣的函式名。\n如果是要 mock 如 ‘fs’等原生模組，直接定義在專案最上層的 __mocks__中即可。\n在 jest.fn()中，有需多 mock資料的方式 1. mockImplementation(fn)，可以取得函式調用的參數，並決定如何回傳\n1 2 3 4 5 6 7 // ./util/__mocks__/apiWrapper.js exports.readFile = jest.fn().mockImplementation(params =\u0026gt; { if(params == 1){ return “jest” } return “jest2” }) 2. mockReturnValueOnce 一次性的回傳值\n1 2 3 jest.fn() .mockReturnValueOnce(10) .mockReturnValueOnce(‘x’) 3. mockReturnValue 固定的回傳值\n有定義並宣告使用 mock，jest在執行原腳本時會改呼叫被 mock覆寫定義的函式 jest.fn()，達到產生假資料的效果，同時紀錄 fn()呼叫的次數與傳遞的參數；\n物件則同樣也是透過改寫物件的呼叫方法。\n其餘的在程式碼中，https://github.com/sj82516/very-simple-jest-example\n","date":"2018-08-06T10:42:40.623Z","permalink":"https://yuanchieh.page/posts/2018/2018-08-06-%E4%BD%BF%E7%94%A8-jest-%E5%81%9Aapi-%E5%96%AE%E5%85%83%E6%B8%AC%E8%A9%A6%E7%9A%84%E7%AF%84%E4%BE%8B%E8%88%87%E7%B4%B0%E7%AF%80/","title":"使用 Jest 做API 單元測試的範例與細節"},{"content":"資料庫日積月累資料量逐步攀升，MySQL在一般查詢是透過全表搜尋，所以大量的資料會導致查詢等方式越來越慢；\nMySQL提供索引建置，一般的索引透過 B+ Tree，在記憶體中快速查找資料所在位置，將搜尋從 O(n) 約*降至O(log n)，索引支援Where / Order by / Range中的條件判斷。\n以下產生User / Order兩張百萬筆資料的Table\n1. 並試著用Explain 分析SQL語法\n2. 透過索引設定比較前後的查詢速度優化\nTable 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 CREATE TABLE \u0026#39;users\u0026#39; ( \u0026#39;id\u0026#39; int(11) NOT NULL AUTO_INCREMENT, \u0026#39;uuid\u0026#39; char(36) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin DEFAULT NULL, \u0026#39;age\u0026#39; int(11) DEFAULT NULL, \u0026#39;firstName\u0026#39; varchar(255) DEFAULT NULL, \u0026#39;lastName\u0026#39; varchar(255) DEFAULT NULL, \u0026#39;createdAt\u0026#39; datetime NOT NULL, \u0026#39;updatedAt\u0026#39; datetime NOT NULL, PRIMARY KEY (\u0026#39;id\u0026#39;), ) ENGINE=InnoDB AUTO_INCREMENT=1000001 DEFAULT CHARSET=utf8mb4; CREATE TABLE \u0026#39;orders\u0026#39; ( \u0026#39;id\u0026#39; int(11) NOT NULL AUTO_INCREMENT, \u0026#39;uuid\u0026#39; char(36) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin DEFAULT NULL, \u0026#39;cost\u0026#39; int(11) DEFAULT NULL, \u0026#39;user_id\u0026#39; int(11) DEFAULT NULL, \u0026#39;user_uuid\u0026#39; varchar(255) DEFAULT NULL, \u0026#39;tradeNo\u0026#39; varchar(255) DEFAULT NULL, \u0026#39;createdAt\u0026#39; datetime NOT NULL, \u0026#39;updatedAt\u0026#39; datetime NOT NULL, PRIMARY KEY (\u0026#39;id\u0026#39;) ) ENGINE=InnoDB AUTO_INCREMENT=750001 DEFAULT CHARSET=utf8mb4; Explain Explain可以用於分析 SELECT / DELETE / UPDATE / INSERT / REPLACE 語句，條列出執行SQL語句時會使用到的 Table與欄位資訊，實際回傳的欄位有\nselect_type: SELECT查詢的狀態，常見有幾種型別 SIMPLE：簡單查詢 PRIMARY：主查詢，相對於子查詢Subquery UNION：在UNION語句中的非首個SELECT SUBQUERY：子查詢\n如果非SELECT則為其他動詞如DELETE table: 也就是使用的Table名稱 partitions:\n如果有使用 partition功能才會顯示 type: type 參數非常重要，這會決定此次SQL語句使用索引狀況，由優至劣順序介紹\nconst / system\n查詢用上primary / unique key，也就是條件剛好匹配一個行，因為只讀取一行所以速度最快；\nsystem是特殊類的const，用於查詢 system相關的表，如\nexplain select * from 'proxies_priv' where user=’root’ eq_ref :用於多表 join下，如果在條件判斷 = 上用了PRIMARY KEY 或UNIQUE NOT NULL也就是條件剛好匹配一個行\n此範例的 join select就是用上eq_ref因為 user.id是 primary key\nexplain select * from orders left join users on users.id = orders.user_id where orders.cost \u0026gt; 100;\nref\n用於多表 join下，如果是用leftmost prefix key或是非 [eq_ref](https://dev.mysql.com/doc/refman/8.0/en/explain-output.html#jointype_eq_ref) 條件中的key，也就是可能會匹配多行 index_merge：\n如果查詢用上多個key，例如\nexplain select id from users where id = 2 or id=100 or uuid=’21d9dadb-038f-427d-8ef1-c2b3aa0994e6';\n(id / uuid 都是 index) range 將key用於範圍查詢，如 = \u0026lt;\u0026gt; \u0026gt; \u0026gt;=`, \u0026lt; \u0026lt;=, IS NULL \u0026lt;=\u0026gt; BETWEEN, LIKE IN()` index：\n全表索引檢索，常用於索引可覆蓋查詢欄位，所以不需要到磁碟讀取資料\nALL：\n最差的查詢方式，全表搜尋 possible_keys：\n可能會用上的key key：\n實際用上的key key_len：\n實際key使用的比對長度 ref：\n用在與索引比對的常數或欄位，如\nexplain delete from users where id=1; ref值為 const，因為是1；\n如果比對的是欄位則會出現欄位名，如 index_search.orders.user_id rows：\nMySQL預計要讀取的行數 filtered：\nMySQL根據條件預計會篩選掉的比例，以百分比顯示，所以最大值為100，也就是每個 rows都用上 Extra：\n額外補充，有幾個值需要留意\n＊Using filesort：\nMySQL在排序上需要做額外的處理，會耗費大量的性能。\n* Using Where：\n有加入條件判斷，如果不是要刻意掃全表，理論上都會出現這個值；\n如果Extra沒有出現Using Where 且 type為 ALL/ index，小心就落入了全表掃描。\n* Using index：\n索引搜尋且覆蓋索引，就不用再額外讀取實際的row 資料。 實際使用 我透過 nodejs塞入百萬筆假資料，可以參考連結取得\nRIGHT JOIN 和 LEFT JOIN差異 users / orders Table目前只有 id是 primary key，比對以下兩個語句，兩者執行速度天差地遠\nexplain select * from users left join orders on orders.user_id = users.id;\nexplain select * from users right join orders on orders.user_id = users.id;\nMySQL內部會把RIGTH JOIN轉換成LEFT JOIN，所以其實就是比較先執行 users 還是 orders，在內部 JOIN是多層 for loop查找並比對 on 的條件判斷；\n在這個案例中， 後者的執行速度會遠快於前者因為後者先loop orders，接著拿 orders.user_id去 users中比對users.id，而 user.id是 primary key所以速度非常快；\n反之 orders.user_id沒有索引，只能全表掃描。\nexplain select * from users right join orders on orders.user_id = users.id;\nexplain select * from users right join orders on orders.user_id = users.id;\n子查詢 如果我想要條列所有訂單數超過兩筆的用戶，並同時顯示{用戶所有資料，訂單數}，可能有幾種做法\n從users , temp table取資料，temp table 是暫存 訂單數超過2的 table，兩者做 INNER JOIN select users.*, temp.order_count from (select user_id, count(distinct orders.id) as order_count from orders group by orders.user_id having order_count \u0026gt; 2) temp INNER JOIN users on users.id = temp.user_id; orders 先INNER JOIN users，接著才計算訂單數\nselect users.*, count(distinct orders.id) as order_count from orders INNER JOIN users on users.id = orders.user_id group by orders.user_id having order_count \u0026gt; 2; 第一點的問題是在子查詢 (select user_id, count(distinct orders.id) as order_count 不可避免的要跑一次全表搜尋，但是暫存成 temp Table做INNER JOIN 又會在跑一次，等同於全表搜尋 orders兩次\n1\n為了避免多一次無謂的全表搜尋，先JOIN在 GROUP BY 效率就好很多。\n2\n","date":"2018-07-30T10:14:40.633Z","permalink":"https://yuanchieh.page/posts/2018/2018-07-30-mysql-explain%E5%88%86%E6%9E%90%E8%88%87index%E8%A8%AD%E5%AE%9A%E6%9F%A5%E8%A9%A2%E5%84%AA%E5%8C%96/","title":"MySQL Explain分析與Index設定查詢優化"},{"content":"前陣子在看 Cloudflare一些相關設定，剛好看到 DNSSEC這個技術名詞，以下是認識後的筆記。\n參考資料：\nhttps://www.cloudflare.com/dns/dnssec/how-dnssec-works/ http://www.cc.ntu.edu.tw/chinese/epaper/0022/20120920_2206.html\nDNS與DNSSEC DNS，域名系統，用來將人類好記憶的文字域名轉換為 ip位置，就好像電腦世界的電話簿一般；\n但是DNS最大問題在於，取得紀錄後卻沒有驗證紀錄是否正確或是被竄改的方法，這也是 DNSSEC(Domain Name System Security Extensions) 的出現。\n數位簽章 DNSSEC是透過數位簽章的方式，確保DNS查詢的紀錄沒有被竄改，所以先來簡單介紹什麼是數位簽章。\n簽章，是為了確保消息不被竄改性與不可誣賴性 ，像是契約要一試兩份，雙方確認好內容，最終簽字畫押；\n假設某天甲方竄改了契約內容，乙方可以拿出當初保留的契約反駁；\n又或是乙方想要否認契約，但是甲方同樣可以拿出契約中的乙方簽章證明。\n甲 \u0026ndash;\u0026gt; 自己產生公鑰與私鑰，公鑰可以讓所有人取得，私鑰要保護好\n今天甲要給乙一份文件 Doc，為了比對用 hash產生雜湊值H\n接著甲將雜湊值H用私鑰加密成 H_encrypted，H_encrypted 也就是數位簽章\n甲將 {Doc,H_encrypted} 交由丙轉交給乙，此時丙偷改了 Doc\n乙收到後，將 Doc依照約定的hash產生出雜湊值 H\u0026rsquo;\n接著用甲的公鑰解開 H_encrypted =\u0026gt; H，發現H\u0026rsquo; != H，發現內容被竄改了！\n-\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;\n假設剛才 Doc沒有被竄改，乙方驗證並確認收到甲傳遞的訊息\n但是某日甲想要反悔說沒有簽署過該份合約\n此時乙一樣拿甲的公鑰解密數位簽章，接著比對 Doc透過hash的雜湊值，發現是一樣的，因為公鑰/私鑰是獨一無二(非常難碰撞)，只有公鑰可以解開私鑰加密的內容(反之亦然)，所以篤定 Doc是甲簽署的。\nDNSSEC 1.DNS紀錄有多種型別，假設是宣告AAAA型別，也就是將域名對應IPv6的紀錄，每一筆記錄縮寫為RR；\n而在簽署過程，會把同樣型別的紀錄整合為一組，也就是這邊看到的RRset。\n2. 接著將RRset產生數位簽章，也就是RRSIG，接著回覆DNS紀錄時，總共要給三樣東西\n{RRset / RRSIG / Public ZSK}\nPublic ZSK也就是DNS Server的公鑰。\n用戶就可以拿 Public ZSK解開 RRSIG，比對 RRset，就可知道該RRset是否真的來自DNS Server\n如何確保 Public ZSK是沒有被竄改 這又可以套用數位簽章了，子DNS Server會向父 DNS Server 提交他的 Public ZSK；\n父DNS Server就當作 {子DNS Server → Public ZSK} 當作是一筆 RRSet(又稱DS，Delegation Signer)\n接著一樣用自己的公鑰(Public KSK)簽署數位簽章\n但問題還是在，要如何在確保 Public KSK是正確的呢？\n這會無限遞迴數位簽章直到 DNS Root Server，這也是信任鍊(Chain of Trust)。\n有興趣看dns解析會走過些路徑可以用下列工具，像是 google.com，會先到 root dns server -\u0026gt; .com dns server -\u0026gt; google.com dns server -\u0026gt; return AA record\nTrace DNS Delegation\n那我們可以全盤相信 Root Server嗎? 這篇文章說明了整個Root Server在發布公鑰/私鑰的嚴謹性，頂級域名商(.com, .edu, .org …)他們的專業維護整個網路安全的可靠性與可信度。\nThe DNSSEC Root Signing Ceremony | Cloudflare\n","date":"2018-07-30T10:14:21.565Z","permalink":"https://yuanchieh.page/posts/2018/2018-07-30-dnssec-%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E4%BB%8B%E7%B4%B9/","title":"DNSSEC 基本原理介紹"},{"content":"Nodejs底層是事件驅動，透過 Event Loop處理非同步(non-blocking)操作，讓費時的I/O操作可以交由libuv去呼叫系統事件驅動的 system api或是用 multi thread方式處理，而Main thread則持續處理request或其他運算。\n1 // copy from nodejs 官網 ┌───────────────────────────┐┌─\u0026gt;│ timers ││ └─────────────┬─────────────┘│ ┌─────────────┴─────────────┐│ │ pending callbacks ││ └─────────────┬─────────────┘│ ┌─────────────┴─────────────┐│ │ idle, prepare │ (internal use)│ └─────────────┬─────────────┘ ┌───────────────┐│ ┌─────────────┴─────────────┐ │ incoming: ││ │ poll │\u0026lt;─────┤ connections, ││ └─────────────┬─────────────┘ │ data, etc. ││ ┌─────────────┴─────────────┐ └───────────────┘│ │ check ││ └─────────────┬─────────────┘│ ┌─────────────┴─────────────┐└──┤ close callbacks │ └───────────────────────────┘ 簡而言之，Event Loop有許多不同的階段(phase)，每個phase 是個陣列，當完成非同步操作後，會對應將 callback task push到 phase中；\nNodejs會不斷的輪詢 Event Loop並同步執行callback task，直到 Event Loop上都沒有 task且 Main thread也都執行完成，就會退出。\nNodejs一些非同步操作對應的 phase如下\n1.setTimeout 屬於 timers 2. pending callbacks主要處理系統錯誤 callback\n3. poll 階段則是向系統取得 IO事件\n4.setImmediate 屬於 check\n5.關閉事件如 socket.on(‘close’, …) 則屬於 close\nprocess.nextTick 是在每個phase結束前執行， Promise 屬於 microtask，同樣執行於每個phase結束之前，且在 process.nextTick之前；\n所以要小心，不能要遞迴呼叫process.nextTick，會導致整個Event Loop卡住，因為每個process.nextTick都會在phase結束前執行。\n以上大致介紹，實際運作複雜許多，附註參考資料：\n1. https://www.eebreakdown.com/2016/09/nodejs-eventemitter.html\n2. https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/\n3. https://jakearchibald.com/2015/tasks-microtasks-queues-and-schedules/\n4. 在瀏覽器中狀況跟Nodejs執行環境不太相同 https://github.com/kaola-fed/blog/issues/234#code-analysis-b-a\n但我看這個有什麼用? 只要我知道怎麼寫非同步程式碼，了解底層 Event Loop執行順序有什麼意義嗎?\n雖然說學習不該帶有太強烈的功利性，追根究底本身就是個樂趣，但這個問題著實困擾我頗久，在這兩天看到了 dataloader 驚為天人的 Library，以下正文開始。\ndataloader的用途 dataloader主要用於解決Graphql的N+1問題，稍微簡單介紹一下\nGraphql是 FB提出的技術，用來當作新一代的前後端API交互介面，主要是將搜尋的能力交還給前端控制，後端就被動配合；\n改善以往 RESTful API在 GET上麻煩的地方，以下為示範\n假設有 User / Post / Comment，\nUser 1 \u0026lt;-\u0026gt; m Post，User可以創建多個Post\nUser 1 \u0026lt;-\u0026gt; n Comment m \u0026lt;-\u0026gt; 1 Post，User可以在Post下發布 Comment\n假設今天我們要顯示某用戶的所有貼文，以REST來講可能是\n/user/:userId/post\n如果要某用戶下所有貼文帶評論\n/user/:userId/post/comment\n如果是某用戶某貼文的所有評論\n/user/:userId/post/:postId/comment\n\u0026hellip;\u0026hellip;\n所以在查詢上，前端多一個需求後端就要多一隻API，有點麻煩\n如果要省著用同一個API外加query來判斷，就變成資料層或邏輯層一樣要處理\n-\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\n而Graphql 則漂亮很多，前端定義好需要的資料格式，後端就會對照回傳(套件輔助)\n1 2 3 4 5 6 7 8 9 10 { User { name, id, Post { title, Comment { content, User ....... } 不論是要取得 User，User -\u0026gt; Post， User -\u0026gt; Post -\u0026gt; Comment無限遞迴，都是非常簡單的一件事\n後端則是對應好資料擷取，Graphql在後端會過濾把前端需要的欄位回傳，大幅降低網路 payload，非常的簡潔有力。\n如果是有大量查詢的應用程式，或是有跨裝置應用，可以考慮導入 Graphql，現在來說方案都已經非常成熟了。\nGraphql N+1問題 但是在讀取巢狀資料的過程中，Graphql會發生N+1問題，例如取得所有用戶下的所有文章{User{Post}} ，這時候因為架構設計的關係，Graphql會先讀取所有的User，接著再針對個別User去讀取Post；\n所以資料庫讀取會變成 1 次讀取全部 User + N次個別User讀取Post，例如\n1 2 3 4 5 6 7 8 9 10 11 SELECT \\* FROM User; --\u0026gt; 回傳了 [1,2,3,4] SELECT \\* FROM Post WHERE user_id = 1; SELECT \\* FROM Post WHERE user_id = 2; .... \\---\u0026gt; 較為理想情況 SELECT \\* FROM User; --\u0026gt; 回傳了 [1,2,3,4] SELECT \\* FROM Post WHERE user_id in (1,2,3,4); \\---\u0026gt; 最理想 SELECT \\* FROM User LEFT JOIN Post on Post.userId = User.id; 而dataloader提供的解法相當美妙，在應用層稍作改變，不影響原本graphql / 不干涉資料庫操作，將 N+1進化成第二優化的查詢條件。\ndataloader 介紹 dataloader作者\ndataloader 主要做兩件事 batch \u0026amp; cache ，因為在處理 http request上，為了上資料不過期，大多不會用上 cache，所以這裡僅介紹 batch。\ndataloader提供的解法是將異步操作合併，並在Event Loop的一個phase結束後用 process.nextTick 執行。\n先看dataloader說明範例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 var DataLoader = require(\u0026#39;dataloader\u0026#39;) var userLoader = new DataLoader(keys =\u0026gt; myBatchGetUsers(keys)); /* myBatchGetUsers是自訂的函式，接收被dataloader batch起來的keys，回傳等同keys長度的Promise */ userLoader.load(1) .then(user =\u0026gt; userLoader.load(user.invitedByID)) .then(invitedBy =\u0026gt; console.log(`User 1 was invited by ${invitedBy}`)); // Elsewhere in your application userLoader.load(2) .then(user =\u0026gt; userLoader.load(user.lastInvitedID)) .then(lastInvited =\u0026gt; console.log(` User 2 last invited $ { lastInvited } `)); /* userLoader.load(1) 代表要載入，這會被 dataloader batch起來，最後再keys =\u0026gt; myBatchGetUsers(keys)一併處理 */ // 接著來看程式碼，總共324行，大概有100行是註解… 非常精簡巧妙的設計 load(key: K): Promise \u0026lt; V \u0026gt; { var promise = new Promise((resolve, reject) =\u0026gt; { // Enqueue this Promise to be dispatched. this._queue.push({ key, resolve, reject }); // 在queue 重新填裝時觸發 if (this._queue.length === 1) { if (shouldBatch) { // If batching, schedule a task to dispatch the queue. enqueuePostPromiseJob(() =\u0026gt; dispatchQueue(this)); } else { // Otherwise dispatch the (queue of one) immediately. dispatchQueue(this); } } }); return promise; } load()執行後是回傳一個Promise，注意關鍵的一行 this._queue.push({...}) ，這個回傳的Promise resolve方法是被push到 _queue上，而_queue是 dataloader一開始創建時建立的空陣列。\nenqueuePostPromiseJob 1 2 3 4 5 6 7 8 9 10 11 12 var enqueuePostPromiseJob = typeof process === \u0026#39;object\u0026#39; \u0026amp;\u0026amp; typeof process.nextTick === \u0026#39;function\u0026#39; ? function(fn) { if (!resolvedPromise) { resolvedPromise = Promise.resolve(); } resolvedPromise.then(() =\u0026gt; process.nextTick(fn)); } : setImmediate || setTimeout; // Private: cached resolved Promise instance var resolvedPromise; resolvedPromise就是用來暫存的 Promise.resolve()，用來執行resolvedPromise.then(() =\u0026gt; process.nextTick(fn)) ，fn 是剛才的 () =\u0026gt; dispatchQueue(this) ；\n如果環境有 process.nextTick則用，不然用 setImmediate / setTimeout 也可以!\n這裡作者打上了25行的註解，大意是說：\nECMAScript 運用 Job / Job Queue描述當下執行結束後的工作順序安排；\n(對照Nodejs也就是Event Loop的實作)，Nodejs用 process.nextTick實作 Job的概念，當呼叫了 Promise.then 則會在 global Jobsqueue中加入 PromiseJobs這樣的一個 Job。\ndataloader會打包同一個執行的幀(frame) 中的操作，包含在處理PromiseJobs queues 之間的 load也會被一併打包。\n這也是為什麼要用resolvedPromise.then(() =\u0026gt; process.nextTick(fn)) ，確保在所有的 PromiseJobs之後執行 (*備註一)；\n這一段主要是因應cache處理，如果有開啟cache，執行過後一次dataloader會以 Promise.resolve儲存結果，也就是後續不管調用幾次都是立刻回覆結果；\n所以下列結果 1 / 5 /6 會被batch在同一個phase執行，而4會到下一個loop去\n1 2 3 4 5 6 7 8 9 10 testLoader.load(1).then(t =\u0026gt; console.log(\u0026#34;1 got \u0026#34;, t)) Promise.resolve().then(() =\u0026gt; { testLoader.load(5) }).then(() =\u0026gt; { testLoader.load(6) }) setTimeout(() =\u0026gt; { testLoader.load(4) }) 而瀏覽器沒有向Nodejs中提供 microtask(process.nextTick)，只能用 macrotask(setImmediate / setTimeout)取代，但會有性能上的影響。\ndispatchQueueBatch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 var keys = queue.map(({ key }) =\u0026gt; key); var batchPromise = batchLoadFn(keys); batchPromise.then(values =\u0026gt; { queue.forEach(({ resolve, reject }, index) =\u0026gt; { var value = values[index]; if (value instanceof Error) { reject(value); } else { resolve(value); } }); }) dispatchQueue 多做一些判斷，最後呼叫dispatchQueueBatch；\n在此處取出所有佔存在queue上的keys，並呼叫batchLoadFn，也就是在 new Dataloader((keys)=\u0026gt;{…}) 所定義的，執行後就對應queue上的 resolve，也就是把值回傳給 userLoader.load(k).then(value =\u0026gt; ….) 做後續的處理。\n總結一下，要使用時先建立 dataloader，並決定對應的 batch loader該如何處理，通常就是放資料庫 batch處理 new Dataloader((keys) =\u0026gt; customBatchLoader(keys)) ；\n接著定義操作， loader.load(key).then(value =\u0026gt; {}) ；\ndataloader內部透過 enqueuePostPromiseJob 機制，將一個執行幀內定義的操作都匯集起來，並在 Event Loop phase最後執行；\n最後內部呼叫 dispatchQueueBatch ，也就是實際調用 customBatchLoader 的地方，最後 resolve 當初宣告的 loader.load() 。\ndataloader簡單範例 打印結果，注意 setTimeout會在下一個loop執行，而Promise.resolve()則在同一個 phase被處理掉。\n1 2 3 4 5 6 batched by dataloader: [ 1, 2, 3, 5 ] content: [ 1, 2, 3, 5 ] 1 got 2 [2,3] got [ 3, 4 ] batched by dataloader: [ 4 ] content: [ 4 ] 對應我們要解決的N+1問題，因為Graphql怎麼處理底層呼叫算是個黑盒子，以下是大概示意\n1. 原本Graphql的 N + 1問題\n1 2 3 let userList = await db.getUsers() let post1 = await db.getPostByUserId(userList[0].id) let post2 = await db.getPostByUserId(userList[1].id) \u0026hellip;\u0026hellip;\n2. 如果用dataloader轉換\n1 2 3 4 5 6 7 8 9 10 11 let userList = await db.getUsers() // post請求集中成 let postLoader = new Dataloader((keys) =\u0026gt; { let res = await db.getPostListByUserId(keys) return Promise.all(res.map(r =\u0026gt; Promise.resolve(r))) }) // 照舊一個一個定義還是可以 let post1 = await postLoader.load(userList[0].id) let post2 = await postLoader.load(userList[1].id) 結語 原本以為 N+1問題要解決必須深入到改寫資料庫的SQL，例如轉成 SELECT IN，但沒想到有個如此簡潔又漂亮的解法，而且不限定用什麼資料庫，真的是太棒了。\n深入底層理解原理，抽象又費時，但我想這樣的投資是值得的，不禁再次感嘆這個Library的精妙，I dont know JS OTZ。\n備註 篇幅有點長，所以把一些細節放在這裡\n確保在所有的 PromiseJobs之後執行 這部分要深入了解 Nodejs 執行 microtask與macrotask的順序\nPromise.resolve ().then (() =\u0026gt; {\nconsole.log (2);\n}).then (() =\u0026gt; {\nconsole.log (9);\nprocess.nextTick (() =\u0026gt; {\nconsole.log (7);\n});\nPromise.resolve ().then (() =\u0026gt; {\nconsole.log (8);\n});\n});\nprocess.nextTick (() =\u0026gt; {\nconsole.log (1);\n});\nPromise.resolve ().then (() =\u0026gt; {\nconsole.log (3);\n}).then (() =\u0026gt; {\nconsole.log (6);\n});\nsetImmediate (() =\u0026gt; {\nconsole.log (5);\n});\nsetTimeout (() =\u0026gt; {\nconsole.log (4);\n});\n////// 打印結果\n1\n2\n3\n9\n6\n8\n7 \u0026ndash;\u0026gt; 等同於 dataloader 第一次 batch觸發的時機\n4\n5\n這部分蠻有趣的，一開始Event Loop會先從 nextTick queue開始，所以1會先打印；\n接著處理 Promise.resolve()的 promise queue，也就是2 3；\n接著 2 3分別又繼續往後resolve 9 6；\n8之所以在7之前是因為 Nodejs此時再處理 promise queue，所以會優先處理promise，處理完成後才會再處理 nexttick；\n等 microtask都處理完，才會進到其他timer phase與後續的phase處理。\n這也是為什麼enqueuePostPromiseJob 要用 promise.resolve(()=\u0026gt;process.nextTick(()=\u0026gt;dispatchQueue(this))) ，對應如果 Promise.resolve().then().then() 中的 Promise都是 resolve了就會自動在同一個 batch處理，對印也就是打印 7 的位置。\n如果開啟了 dataloader cache，dataloader 是直接儲存 resolved promise，性能會有顯著的提升。\n","date":"2018-07-16T12:35:50.102Z","permalink":"https://yuanchieh.page/posts/2018/2018-07-16-%E7%82%BA%E4%BB%80%E9%BA%BC%E8%A6%81%E7%90%86%E8%A7%A3-nodejs-event-loopdataloader-%E6%BA%90%E7%A2%BC%E8%A7%A3%E8%AE%80%E8%88%87%E5%88%86%E6%9E%90%E5%A6%82%E4%BD%95%E8%A7%A3%E6%B1%BA-graphql-n-1%E5%95%8F%E9%A1%8C/","title":"為什麼要理解 Nodejs Event Loop：Dataloader 源碼解讀與分析如何解決 Graphql N+1問題"},{"content":"網頁瀏覽時出現HTTPS的綠色鎖看了令人放心，這似乎代表著我們在網站瀏覽的資料有受到「完整的加密保護」，不用擔心資料被偷窺與被調包等等MitM中間人攻擊的風險，但事實當然沒有這麼簡單。\n在申請SSL憑證時是綁定域名申請，理論上 DNS解析域名後直接指向Server所在IP，也就是Client透過HTTPS在索取憑證、驗證憑證、與 Server共同生成對稱加密金鑰後，實際將資料加密傳輸到Server的整段網路過程是受到完整的保護。\n但是現在很多網站為了效能上與安全性上的考量，採用了向 Cloudflare這樣的 DNS代管 / CDN服務；\nClient ← → Cloudflare ← → Server\nCloudflare在全球各大洲部署多個資料中心，會自動將DNS / Cached資料散佈到多節點上，並提供多項優質服務，如\n從地理位置最近的節點回覆 Client所需資料，之前曾看過實測多家DNS/CDN服務商的資料，Cloudflare回應速度是最好的； 在Cloudflare後台觀察到網站有近八成的流量是命中Cache從 Cloudflare直接回覆 Client，降低Server 負擔與流量，再Server以流量計費的主機託管下節省很大的成本。 DDos防護與白/黑ip名單建立 但是採用 Cloudflare有些資安上的疑慮，也是此次筆記的內容\n主要紀錄\n1. 採用Cloudflare風險在哪\n2. 修正問題與實測\n採用Cloudflare風險在哪 Client ←→ Cloudflare 先前提到Cloudflare會擋在 Client與 Server之間，如果啟用CDN服務為了解析緩存資料，Cloudflare 會需要在他這層直接解密，所以如果是用免費版 Cloudflare會提供 Universal (shared)的憑證，這是簽發於 Cloudflare機構底下；\n如果要用自己域名簽發的憑證，必須透過Cloudflare購買或是上傳自己購買的憑證與私鑰 ，這兩項都必須採用付費方案；\n但這樣看來 Cloudflare 就是渾然天成的 MitM中的中間人，但是我們選擇相信他，有看到一些謠言是說 Cloudflare背後有美國的 NSA國家安全局把持，所以資料可能有洩漏的疑慮，畢竟 Cloudflare可以看到所有解密後的資料。\n看到一些討論是半信半疑，技術性上也不是所有的流量都會導向美國的服務器，但還是有必要留個心眼，畢竟先前也是有 FBI要求 Apple 解鎖Iphone的經歷，國家機器再處理這種法律、資安、個資等議題確實蠻掙扎的。\nTim Cook says Apple\u0026rsquo;s refusal to unlock iPhone for FBI is a \u0026lsquo;civil liberties\u0026rsquo; issue\n(雖然Apple 在中國也服軟了 :/)\n所以如果要有緩存機制且非常在意 Client → Server 必須全程走HTTPS且使用自家憑證不被任何人在中途解密資料，就適合自建Cache Server 不適合用Cloudflare。\nCloudflare ←→ Server 從Cloudflare到Server這段有不同層級的設定，可以於後台設定\nOff：\n全部走HTTP Flexible：\nClient → Cloudflare 走HTTPS，而Cloudflare → Server 走HTTP Full：\nCloudflare → Server 走HTTPS，但是 Cloudflare不會驗證憑證。\nServer需要開啟443 port 才能處理。 Full(strict)：\n呈上，但是Cloudflare會向CA驗證憑證的正確性，這部分需要搭配設定 Origin Certificates ，可以由 Cloudflare 產生或是自己產生後上傳。\nCloudflare本身也是合格的CA，所以往好處想是上傳憑證是可以被信任的，但同樣的雞蛋都在同一個籃子裡本身就是個風險，算是一體兩面。 修正問題與實測 所以除了Client要有HTTPS保護，在Server這段也建議要開啟 Full以上的防護措施，Server部分用 Let’s Encrypt 免費簽署，並同時開啟 80 / 443 port，來調整對應Cloudflare的SSL保護措施。\n1. 僅開啟 Cloudflare DNS： 這時候打 https://domain.com 會出現自己簽發的憑證，像我是用 Let’s Encrypt 簽署的憑證\n2. 開啟 Cloudflare CDN服務： 這時候同樣的 https://domain.com 會自動變成 Cloudflare Universal 憑證\n以上是 Client \u0026lt;\u0026ndash;\u0026gt; Cloudflare這段\n以下用 \u0026gt; sudo tcpdump -nn -i eth0 'tcp and (not port 22)' 查看封包，排除port 22 主要是避免 ssh 封包干擾觀察\n3. SSL 設為 Flexible 從 Cloudflare(用 whois 172.68.47.149查詢後確認) 到 Server是走 80 port\n4. SSL 設為 Full / Full (strict) 後台設定切換後等個三、五秒就立即改走 port 443\n5. 測試憑證檢查 Full (strict)差別在於會檢查憑證是否為公開的第三方CA頒布的合法憑證，那就來確認一下檢查的機制是否OK。\na. 在 Full 狀態下改用其他網域的SSL憑證 =\u0026gt; 可以!\nb. 在 Full (strict) 狀態下改用其他網域的SSL憑證 =\u0026gt; 不行，會檢查\n如果是自簽憑證，記得要上傳 CSR到Cloudflare，不然一樣會出錯。\n結論 僅使用 Cloudflare DNS服務，Client會看到自家提供的憑證 開啟 Cloudflare CDN服務，免費版轉為 Cloudflare Universal SSL憑證；\n如要替換為自己Domain簽署的憑證，需要付費方案 開啟 Cloudflare CDN，會讓 Cloudflare成為中間人，資料有被外露的疑慮 記得開啟 SSL 為 Full(strict)，保護 Cloudflare 到 Server這段 ","date":"2018-07-11T01:37:07.542Z","permalink":"https://yuanchieh.page/posts/2018/2018-07-11-https%E4%B8%8D%E4%BB%A3%E8%A1%A8%E5%AE%89%E5%85%A8cloudflare-ssl-%E7%A0%94%E7%A9%B6%E5%BE%9Eserver%E5%88%B0cloudflare/","title":"HTTPS不代表安全：Cloudflare SSL 研究從Server到Cloudflare"},{"content":"今天看到一部不錯的影片，主題是「利用Async Pattern 提升JS在多核心上的執行速度」，細拆個三個小節\n1. 用Async IIFE 解決任務間相依與並行的問題\n2. 用High Order Function概念設計 Semaphore，控制同時並行的任務數\n3. 透過Worker 讓任務跑在個別Thread上，增加多核心的效能利用。\nConcurrency vs Parallelism Concurrency 並行是指 多個Task執行時間有重疊，也就是說Task1執行中時Task2也開始執行，算是一個概念性質上的定義，即使只有單一核心也可以透過Time Sharing 達到 Concurrency。\nParallelism 並發，強調的是多個Task被分配到多個實體CPU上，同時且獨立執行。\nAsync IIFE Cross Stitching: Elegant Concurrency Patterns for JavaScript\n最基本使用 async/await 就是一行一行寫，如\nawait task1();\nawait task2();\nawait task3();…\n但如果不同 task之間可能有相互依賴，如 task2 並須等到 task1結束才能進行/ 但也有些 task互相獨立可以並行，如果單純一行一行 async/await 會浪費等待的時間。\n困難的點在於任務複雜，如何漂亮地表示任務間的先後順序相依，並讓並行最大化\n所以作者提到可以用 async iife ，也就是 async function 立即執行函式，async function 不論何時都會回傳 Promise，即使失敗也是回傳 Promise.reject；\n以下是我參考作者範例程式碼改寫的，主要比對原先的做法與async iife的差別，挑戰任務間相依的狀態如下\n原本就有的寫法是描述Task橫向的並行，所以我們會用 Promise.all([])將 TaskB/TaskC並行處理，但是這種寫法可讀性差，而且架構的方向也是不好的，因為 Task相依應該是垂直 ，像是 TaskD明明就只要等 TaskC完成就可以開始執行，但因為寫法所以要等到 TaskB也執行完成後，TaskD與TaskE才並行處理。\n所以使用 async iife 最大好處即是每個Task的相依性都被封裝的很直觀，主要是利用 Promise一宣告就執行的特性，像是\n1 let taskA = (async ()=\u0026gt; { await timePromise(1000); console.log(“TaskA done”)})() 此時taskA已經開始執行了，但是後續的code因為 await taskA還是要等taskA結束才能繼續，所以一樣可以做到流程的控制；\n這也是為什麼 taskB/taskC這樣寫可以並行處理，接著taskD因為只相依taskC所以在範例中會比taskB更早結束，非常漂亮的寫法。\n另外分享一個當初疑惑的點：taskC在 taskD / taskE被呼叫兩次，會不會就執行兩次?! 答案是不會，因為taskC本身是Promise，執行完一次後就會轉成 Promise.fufilled(or rejected) ，如果後續要用 await taskC取值就會立即回傳結果。\nHigh Order Function 高階函式，定義是可接受函式當作參數的函式本身，如過往在數學上學到的 f(g(x))，在JS中結合closure 就可以有非常多的應用。\nSemaphore 信號機，用來控制有限的資源量，作者利用信號機，設計限制最大並行數的機制\nnybblr/semaphorejs\n程式碼只有40行左右但蠻精妙的，使用上 ：\n1 2 3 4 s = Semaphore(3) s(async _ =\u0026gt; {console.log(“start”); await timePromise(3000); console.log(“done”)}) s(async _ =\u0026gt; {console.log(“start”); await timePromise(3000); console.log(“done”)}) ..... 對應程式碼，Semaphore(3) 將 task上限設為 3，接著回傳一個高階函式。\n仔細看這個高階函式的第一行await acquire();\n1 2 3 4 5 6 7 8 9 10 11 12 let dispatch = () =\u0026gt; { if (counter \u0026gt; 0 \u0026amp;\u0026amp; tasks.length \u0026gt; 0) { counter--; tasks.shift()(); } }; let acquire = () =\u0026gt; new Promise(resolve =\u0026gt; { tasks.push(resolve); setImmediate(dispatch); }); 最有趣的地方莫過於作者用 acquire / dispatch 達到數量上的控制，acquire 將 Promise.resolve() 推上了 task陣列，接著觸發 dispatch，而dispatch只有在未達上限前可以執行 task.shift()的 function，這裡也就是Promise.resolve()，透過這個機制就可以去卡 await acquire() 。\nWeb Worker and Multi Thread JS本身執行於 Main Thread上，其餘非同步API是由底層瀏覽器的Web API或 Nodejs Libuv等支援，最後透過 event loop 將 callback 返回 Main Thread 執行；\n但如果需要用JS執行費時的操作而沒有底層函式的支援(如自己編寫 Addon)，現在可以透過 Web Worker分開 Thread，避免阻擋 Main Thread。\n程式碼請參考：http://jsfiddle.net/sj82516/uqcFM/350/\n","date":"2018-06-27T04:03:50.027Z","permalink":"https://yuanchieh.page/posts/2018/2018-06-27-jonathan-martin-async-patterns-to-scale-your-multicore-javascript-elegantly-%E7%B8%BD%E7%B5%90%E8%88%87%E8%A9%A6%E9%A9%97/","title":"Jonathan Martin: Async patterns to scale your multicore JavaScript elegantly 總結與試驗"},{"content":"2018/03/13，參加了新店同心救難隊的龍舟訓練團，每天清晨 05:30 ~ 06:30 在碧潭操練，假日不停一路練到 2018/06/15 總計99天的訓練，個人出席了 90天。\n接著是 2018/06/16~2018/06/18為期三天的比賽，最終拿下了 台北市公開小型男子組第六名的成績。\n事隔一週，稍微調整一下作息與思緒，記錄一些關於划龍舟訓練到比賽的一些心路歷程。\nphoto credit : 隊上的明德教練與南廷大哥，感謝他們不時幫大家留下美好的回憶\n為期約三個月的訓練，大致可以拆成三個階段：\n基礎體能操練 / 下船練習 / 賽前準備期\n基礎體能操練 首先要聲明一件很多人都會問的事：「划龍舟手臂會不會變很壯？」\n隨手畫畫，不要太在意(PS. 槳面會與船緣垂直，此為示意)\n但其實划龍舟主要是全身性的運動，一般龍舟坐定位後前方有塊腳踏板可以撐著(上圖)\n接著發力的過程是身體盡量往前趴，下手(靠船的外側)打直，接著大腿發力=\u0026gt;轉腰並往後拉/上手下壓，接著水就會被後推，根據反作用力船就會前進。\n划龍舟靠得是全身力量去帶動，重點是槳入水後划距要拉長，也就是身體要趴得夠下去 =\u0026gt; 身體反彈加速度要起來，這樣划起來才夠力。\n所以划龍舟比較吃大腿**、**背部肌群、核心肌群、肩膀、柔軟度! ，另外就是心肺耐力這幾個部分，反而二頭肌/胸肌就不太會用上。\n第一個月訓練主要是基礎體能訓練，暖身完跑個3k，接著就是教練帶核心訓練與深蹲，接著就是在岸上做基本的動作練習。\n有空檔會到船上練習，不得不說下船後感覺完全不同，下船第一件事教練要我們感受「船的律動」與「身體的律動」\n不拿槳，單純的把身體往前趴然後出力蹬 -\u0026gt; 把身體拉直，也就是拆解動作只做下半部，教練在示範的時候，一蹬整艘船就往前滑動了一段。\n教練表示先學會基本功很重要，因為划船一開始錯誤姿勢都是靠手臂小肌肉群拉，忘了身體的節奏與律動，即使是健身壯漢也不可能單用手臂肌肉划完全程，重點是用大腿/背/腰這些大肌肉群帶動，才可以划得長遠。\n這大概是第一階段訓練，主要是幫大家暖身調整身體狀況，我個人平常有運動健身的習慣，是沒有特別健壯但體力還算行，可是之前運動完都沒有收操的習慣，所以身體的柔軟度很差，在船上訓練過程就會一直被教練唸「那個誰怎麼不趴下去一點」 內心只能OS我的大腿內側都拉到快撕裂了；\n同行的女生反而表現都很好，覺得自己表現有點掉漆\n不過每日練習完，教練都會帶收操，乖乖跟著練習持續了兩個月終於 趴 下 去 了。\n下船練習 這個階段每日就直接下船練習了，先稍微介紹一些龍舟的基本知識。\n龍舟分大龍/小龍，大龍是 20人座，小龍則是 10人座，另外多一個舵手，比賽時小龍龍頭會多一位鼓手，而大龍則是再多一位奪標手。\n個人部分，船分左右槳，我個人是被教練安排於左槳，也就是面對龍頭方向的左側，有些划了數年以上的老手可以左右槳都划。\n比賽距離我們是划 500公尺，整趟下來約需要180槳左右，時間大概在2~4分鐘，時間範圍之所以拉這麼大是因為划船會受到水流與風向影響，順流跟逆流時間可以差到兩倍之多。\n剛剛說比賽完整是需要划 180槳，但我們在一開始練習划個30槳就手痠腰痠，一開始最遠也都划100公尺而已，心想 My God 500公尺也太遙遠了吧。\n當時教練就是很有耐心的指導，不斷的修正姿勢，船上資深的大哥大姐也都很熱情的給予指點，不得不說划船的姿勢拆解步驟就頂多五六項，但是要真的熟練需要花非常多的時間與精力，尤其是龍舟是團體運動，需要大家的動作/節奏一致船才能順順的前行，速度才會快。\n這時候慢慢的有開始感受到船與身體的律動，稍微分享個教練常用的詞「撐船」也就是槳往下壓、身體後拉這個發力的動作，教練表示這感覺很像是撐竿跳，撐到一個點身體就立刻做出反應。\n後來會感覺到撐的力量在於身體斜側趴到極致，大腿與胸腔幾乎靠在一塊，那時候因為已經沒有更多伸展的空間所以有個張力支撐，感覺真的是把身體撐起來。\n賽前準備 在五月就開始要備賽了，練習開始按照比賽分組練習，我就被分配到男子組一同練習，每日訓練的菜單加重很多\n1. 800 公尺暖身\n2. 1200 公尺負重訓練\n3. 500 公尺模擬比賽\n4. 5 30 槳法練習數組\n分享一下我們的槳法\n起步槳：\n因為船一開始是靜止的，所以首要之重就是把船速拉起來，此時的起步槳必須用全身的力量帶，用全力把腰轉側。 加力槳：\n主要也是用全身力量帶，包含手臂也會加力，此時槳速會拉快到約1.4秒一槳，主要就是用於衝刺，同嘗試起步槳之後與最後終點前衝刺。 長槳\n扣除起步與加力，剩餘就是長槳，主要靠身體帶動此時手臂會放鬆，儲備接下來衝刺的力量，大約是7分力左右，因為比賽500公尺不可能全程衝刺，所以長槳槳速會放到約 2秒一槳，但此時就要靠身體完全的伸展讓槳距變長，維持船速。 我們划是5個起步槳 =\u0026gt; 30個加力槳 =\u0026gt; 長槳 100槳 =\u0026gt; 50個加力槳。\n比賽場地我們報了兩組，分別是台北市大佳河濱公園與新北市微風運河，台北與新北的船是不一樣的，前者是木船很沉，後者是塑膠製很輕，也是平常練習的船。\n我們的主要目標擺在台北市，所以平常都會負重練習，划的阻力大概是2~3倍。\nphoto credit：左邊台北市，右邊新北市\n但一開始練習超崩潰，因為強度拉升的很多，從四月一開始只能3/50槳划，逐步爬升到暖身就要滑800公尺約300槳，強度攀升的很可怕，有時候練習教練都會說跟不上可以收槳休息，內心開始掙扎，但最後還是咬著牙努力緊跟，隨著肌肉跟著適應，慢慢的也開始跟上大夥的步調。\n堅持，是我唯一能做到的事 XD\n但只能說勉強跟上節奏，但是整體划船的力量還是輸前輩很多，從後側觀察划船推水的力量，前輩們都是 轟～轟～轟非常大力，我則是 呼～呼～呼而已，姿勢也還有細節需要微調，但也來不及了\n分享一些趣事，這段期間可以明顯感受肌肉在長大，所以食量也變成兩倍，早餐吃兩份、下午開始吃點心，伙食費瘋狂的攀升；\n另外因為划船大腿蹬屁股會稍微挪移，很多人都會準備軟墊避免磨破屁，一開始不知道就有磨破，結果就請假休養，之後偶然發現四角褲四分五裂，看來是因為太激烈所以破得亂七八糟XD 荷包表示心疼。\n比賽!! 經過三個月的備戰，終於要在端午連假三天進行比賽，根據教練玩笑話說「隊上是靠老人與女人吃飯」因為長青組跟女子組往年比賽表現亮眼，男子組就稍微差強人意，今年決定崛起努力奮戰。\n不得不說第一次上場比賽真的超緊張，但好在平常訓練夠，一鳴槍就照著練習的步調一槳一槳來，但因為緊張讓心跳加速更快呼吸也比較短促，划到後來有點喘不過氣，但還好都有撐過終點才斷氣OTZ\n比賽的話一開始是每組取前二晉級，男子組順利進到前八強，但是在搶前四時輸了0.5秒無緣前四，最終搶5~8拿下第六名的成績，有一個小獎盃與獎狀一紙，但是沒有獎金；\n而女子組在新北市拿下第二名佳績! 有大大的獎盃與12萬獎金啊！\n今年延續了隊上優良傳統XD\n分享一個小故事，因為另一位資深的教練肩膀有點受傷，所以我跟他是輪流上場比，在搶前四那場是由我上場比，理論上只要進了前四都會有獎金，但是最終以 0.5秒小輸對手就覺得挺自責，不禁在想如果是教練上場是不是我們就挺進決賽了?\n後來我也主動跟教練詢問這件事，但教練安慰說總是要訓練新人上場，不然練了三個月都沒有上場也不行等等，很感謝教練的信任並願意給我機會上場\n心得 我過往沒有參加過體育團隊練習，很感謝新店同心救難隊的教練與隊員們都很熱情，雖然年齡層尺度很大(19~6X)，但是大家因為對龍舟有興趣一同集結練習，為了爭取榮耀一起努力，最終也取得不錯的成績。\n整個訓練過程有汗水也有歡笑，也感謝教練信任我也願意派我上場，心中留下的一點遺憾就看明年有沒有機會來努力。\n整趟三個月的里程就告一段落，結束每日4:30前起床的日子，還在努力調整作息中XD\nPS. 以上言論僅代表個人體悟，另外龍舟划法也是我總結教練的訓練，其中可能有些細節漏講如有錯誤歡迎告知。\n最後工商一下隊上其餘活動，龍舟就要明年三月見囉～\n主要有游泳班跟救生員班可以參加\n新店同心救難隊\n","date":"2018-06-23T10:11:14.727Z","permalink":"https://yuanchieh.page/posts/2018/2018-06-23-%E9%BE%8D%E8%88%9F%E6%AF%94%E8%B3%BD%E5%88%9D%E9%AB%94%E9%A9%97-%E4%B8%80%E4%BA%9B%E9%97%9C%E6%96%BC%E8%A8%93%E7%B7%B4%E6%AF%94%E8%B3%BD%E7%9A%84%E5%BF%83%E8%B7%AF%E6%AD%B7%E7%A8%8B/","title":"龍舟比賽初體驗 — 一些關於訓練、比賽的心路歷程"},{"content":"最近突然好奇如何做LBS服務，最基本的應用場景就是找出某經緯度位置內方圓距離多少內的所有資料，所以就來研究一下MySQL如何處理地理位置。\nSpatial Reference Systems(SRSs) 空間參考系統 Spatial Reference Systems in MySQL 8.0\n所有的幾何物件如點/線/面，都必須定義在同一個座標系統，例如說在XY軸平面上有一點(1,2)，但是座標系統可能是一個足球場/或是一台筆電螢幕，足球場上的座標(1,2)跟螢幕上的座標(1,2)是截然不同且無從比較起的。\n所以在定義幾何物件時，必須要同時宣告該物件所屬的座標系統，也就是 SRID(spatial reference system identifier)，MySQL和其他的DBMS都必須在相同的SRID下才能夠進行幾何運算。\n在MySQL 8.0中支援超過5000種SRSs，常見的有兩種：\nSRID 3857：\n將地球表面投影至平面上，屬於笛卡爾座標系( Cartesian 2D CS)，也就是XY軸相互垂直且單位長度一致。\n常用於網頁上的地圖系統，如Google Map / Open Street Map等 SRID 4326：\n屬於真實空間座標系統，將地球視為橢圓體，也就是轉換成經緯度，座標軸彼此不是垂直的，經度最後都在南北極彙整。\n常用於GPS 語法使用 DB Fiddle - SQL Database Playground\n創建欄位上，可以定義 GEOMETRY型別，欄位可以指定 SRID，需要索引可加上 SPATIAL INDEX(g)，InnoDB/MyISAM都支援。\n在插入欄位有兩種做法，一種是直接使用幾何形別，一種是透過 ST_GeomFromText 從字串轉換，目前只有看到後者可以指定 SRID\nInsert into geom VALUES(Point(1,1), “word”); Insert into geom2 VALUES(ST_GeomFromText(‘POINT(1 1)’, 4326), “hello”); Insert into geom2 VALUES(ST_GeomFromText(‘POLYGON((0 0, 0 2, 2 2, 2 0, 0 0))’, 4326), “zip”); MySQL支援多種幾何形別，例如Point / Line / Polygon等。\n運算上MySQL 8.0支援多種函式，函式皆已 ST_開頭，例如\n算兩點距離\nSELECT ST_Distance((SELECT g from geom2 where name=’hello’), ST_GeomFromText(‘POINT(0 0)’, 4326)) AS distance;\n判斷是否在某範圍內 (hello是 point 型別，zip 是 polygon型別，判斷)\nSELECT ST_Within((SELECT g from geom2 where name=’hello’), (SELECT g from geom2 where name=’zip’))\n補：經緯度距離換算 在SRID 4326中，地球是偏橢圓狀，人們為了方便透過經緯度用網格做劃分，分辨地理位置的判別，所以要將經緯度換算回實際的距離需要有公式的轉換。\nLearn How Far It Is From One Latitude Line to the Next\n經度換算 經度差會隨著往南北極而遞減至零，在赤道大約是 111.321km，而南北緯 40度則為 85 km。\n緯度換算 緯度間基本上是平行，所以每單位緯度差之間都是相近的，只是因為地球偏橢圓所以高緯度距離越長。\n像是在赤道每一度緯度差實際距離是 110.567 km，在回歸線附近則是 110.948km，如果是在南北極則是 111.169 km，平均大約可以取 111 km。\n所以如果要用經緯度差換算實際距離，可以用 haversine 公式，詳細內容可以參考 Wiki\n目前也存在多種換算公式，不同的運算複雜度帶來不同的運算準確度，可以再多加研究。\nHaversine formula - Wikipedia\n在MySQL 5.7中支援度上沒有這麼好，所以球型距離轉換公式需要自行換算，詳細可以參考此篇 https://mysqlserverteam.com/mysql-5-7-and-gis-an-example/\n","date":"2018-06-21T22:15:22.655Z","permalink":"https://yuanchieh.page/posts/2018/2018-06-21-mysql-%E9%97%9C%E6%96%BC%E5%9C%B0%E7%90%86%E4%BD%8D%E7%BD%AE%E7%9A%84%E5%84%B2%E5%AD%98%E8%88%87%E9%81%8B%E7%AE%97/","title":"MySQL 關於地理位置的儲存與運算"},{"content":"最近有個需求是希望可以將各網站的退款紀錄同步到Google Sheet上，方便PM與財務部門追蹤，相比於自建網站，Google Sheet 有方便的同步編輯 / 匯出匯入 / 登入權限管理等，就不必自己重造輪子，而且也不見得造得出來(汗\n整篇分成兩部分，第一是研究Google OAuth2 API授權部分，第二是Google Sheet API。\nGoogle OAuth2 API授權 Google API授權有兩種方式\nOAuth 2.0：\n如果是涉及用戶隱私資料，就必須走OAuth 2.0授權方式取得用戶授權；\n例如說 Drive / Sheet / Doc / Google+ … 等等。 API keys：\n如果是單純使用Google提供的對外服務，例如 Map / URL Shortener / Geocoding 等。\nAPI Keys使用蠻簡單的，也不是這次使用重點，就不另外贅述。 這次要使用 Sheet API 就需走 OAuth 2.0授權，Google 支援多種OAuth 2.0 授權方式：\nServer-Side Web App APP主動觸發，讓用戶登入Google帳號並授權，接著透過 redirect 或其他方式取得code，最後用code去換token。\n這也是對終端用戶最為常見的授權方式。\nFrom Google\nApplications on limited-input devices 適用於在沒有螢幕或是輸入的裝置，例如列印機、TV等嵌入式裝置\nFrom Google\n這裡同樣是 App主動觸發，但變成回傳一個URL連結，用戶必須另外找支援瀏覽器裝置輸入URL並授權；\n這裡因為 App(ex. TV)跟用戶授權的裝置(ex. 手機)不同，兩者辨識用戶方式透過 device code(“code” \u0026amp; URL -\u0026gt; User)，App透過polling不斷間隔請求Google才能知道該 device code 的用戶是否已經授權，後續流程就相同。\n但須注意此流程授權的權限不多，需要先評估，因為我專案需要 Sheet API授權，這個授權流程就不支援，所以無法採用。\nServer-to-Server From Google\n在Google APIs Console中的憑證欄位，除了API金鑰 / OAuth 2.0 用戶端 ID ，第三個能夠創建的就是 服務帳戶金鑰，服務帳號像是創建一個新的用戶，只是此用戶是被用於 Server 端授權，並透過 IAM 管理權限。\n透過服務帳號最大好處是應用程式是授權於服務帳號，而非個體用戶，像是遇到人員流動就不需要手動在管理授權；\n而且也不需要在 Client又要跳出用戶授權頁面，非常適合用於對內的專案開發(此流程又稱為 two-legged OAuth)。\n也是我們這次會嘗試的流程之一。\nLong Live Token 授權後Google會回傳token，但token都是短期的只有一小時的壽命，Google並不像FB，不會核發長期無限期的Token ，反之Google是不斷的透過 refresh token機制，不斷的核發短期Token，而refresh次數是沒有限制，也不需要用戶再次授權，所以可以長期儲存起來使用。\n但必須注意，如果發生以下幾點 refresh token機制會失效\n1. 用戶取消對應用程式的授權\n2. refresh token超過6個月沒有使用\n3. 用戶更改帳號密碼且refresh token含Gmail的權限\n4. 用戶擁有太多refresh token，目前是每個應用程式每用戶上限50個refresh token。\n5. 用戶本身也存在著refresh token總數上限，但文件只寫到正常狀況不會發生，沒有明講多少。\n實作 這一步主要是先取得 access token與 refresh token。\nNode.js Quickstart | Sheets API | Google Developers\nGoogle Sheet API有個 Node.js 快速上手說明，並用 google-api-nodejs-client，運行後會在終端機出現授權URL，點擊後就可以取得token。\n但我個人不愛google-api-nodejs-client 的寫法，因為是採用層層 Callback的方式，還不如自己接REST API來得清爽。\nscope部分需要取得 [https://www.googleapis.com/auth/drive](https://www.googleapis.com/auth/drive) [https://www.googleapis.com/auth/spreadsheets](https://www.googleapis.com/auth/spreadsheets)\nOAuth 2.0 for Web Server Applications 取得授權方式 這個做法會透過瀏覽器取得用戶授權，並儲存 refresh_token，方便後續Server呼叫API使用。\n需注意Google OAuth2登入文件並沒有明講如何取得refresh_token，所以查了一下作法\nNot receiving Google OAuth refresh token\n文章中的解答說要 approval_prompt=force\u0026amp;access_type=offline 我嘗試是不行的，後來驗證結果是必須在 redirect URL中同時加入prompt=consent\u0026amp;access_type=offline\n這部分比較簡單常見，就不贅述。\nServer-to-Server 取得授權 這部分Google文件強烈建議使用sdk而非 REST API，主要是因為Server-to-Server是透過加密JSON Web Tokens (JWTs)實作，如果出錯容易有資安風險，但範例還是採用接 REST API\n首先先到 Google APIs Console 加入憑證，這次要加入的是服務帳號金鑰，記得要下載 json檔的金鑰檔案 2. 接著到 IAM管理權限，找到剛才的服務帳號 Email，類似於 ….@….iam.gserviceaccount.com，並加入「Service Management 管理員」權限，才能操作API。\n3. 記得比剛才的服務帳號加入表單的共同編輯者。\n左圖為IAM管理 / 右圖為表單共用設定\n需注意 Server-to-Server不會有 refresh_token，token過期就必須重新產生JWT並取得新的token。\n文件中有提到，部分API可以用JWT而不用access token就可以呼叫，但同樣只有部分支援，所以避免踩坑還是統一用 access token。\nSheet API 使用 Introduction to the Google Sheets API | Sheets API | Google Developers\nGoogle Sheet API有兩個部分：\n1. 單純讀跟寫是在 spreadsheets.values底下\n2. 其餘表單的屬性(合併欄位、更改欄位顏色等)等在 spreadsheets底下\n這次只有單純的讀寫，所以就只研究上者。\n操作表單API，首先需要先創建一個 Google表單，連結大概會是\nhttps://docs.google.com/spreadsheets/d/1dMJ…../edit#gid=0\n1dMJ…. 代表是表單的ID，而 gid={id}則是內頁的ID，在API用內頁名稱直接描述如 sheet1(也支援中文如 表單一)；\n整體上API操作可以 讀/寫/寫入文件最後(Append)/清除，每筆API都可以指定範圍如 A1:A5 ，就跟一般表單操作類似。\n程式碼請參考\nsj82516/google-sheet-api-test\n在插入欄位的部分(API如 append / update)，有個參數可以指定Google Sheet如何處理輸入的欄位(valueInputOption)，可以指定為：\n1. RAW，不處理完全依照輸入\n2. USER_ENTERED，按照一般 Google Sheet輸入處理，如 =MAX(A1:A2) 就會自動轉成公式，字串如果符合格式會被轉製成數字等。\n3. INPUT_VALUE_OPTION_UNSPECIFIED，有趣的是這個預設值不能用，API會報錯。\n另一個參數是處理 Concurrency 問題，InsertDataOption可以指定當遇到插入新欄時遇上手動更新時，選擇覆蓋 OVERWRITE 或是插入在新欄位 INSERT_ROWS。\n這次只有簡單用到 read / append 這兩個API，read可以指定範圍，append蠻有趣的如果有手動插入新欄，或是手動Delete整個表單，下次 append會自動正確的位置開始，這點倒不需要擔心。\n其餘的寫入/更新/刪除都沒用上，不過在文件中的參考網頁有提供測試很方便。\n補更：插入新行於表單最上層 透過append是將新行插在表單最下層，但很多時候我們希望後面的資料出現在最上層，查了一下SO，有個解法是先創建新行再將值更新進去\nPush Notification 試想如果表單有新增資料，可以自動通知Server就能處理一些更進階的需求；Google Drive API 提供 Push Notification，而表單身為 Drive下的其中之一檔案型態，也有支援此功能，那就來研究一下吧。\nPush Notifications | Drive REST API | Google Developers\n參考 Files: watch 文件說明，以下為操作步驟\n定義好callback url並通過網域驗證：\n因為是webhook關係，所以必須定義好 Google回呼的 callback url，這部分需要再 Google API Console \u0026gt; 憑證 \u0026gt; 網域驗證申請，網域驗證有多種方式，最簡單是Google會提供一份html並指定要放在網域的對應路徑下，用於驗證網域是屬於本人的。 呼叫API 1 2 3 4 5 axios.post(\\`https://www.googleapis.com/drive/v2/files/${spreadsheet_id}/watch\\`, { type: \u0026#39;web_hook\u0026#39;, id: \u0026#39;channelIdAndShouldBeUnique\u0026#39;, address: \\`${domain}/drive/webhook\\` }); address 放 callback url，id則是自行定義channel，到時候回呼用於驗證，type固定為 web_hook。\nGoogle回呼是用POST，但是內容是放在header，以 x-goog- 開頭的標頭，反而body中沒有資料。\n但不得不說Google的Push Notification有點弱，因為他只會回傳哪個檔案改了、channel ID等，但不會有實質的改變內容，例如表單的哪一行修改、哪一行新增等等；\n不過也是，因為這是廣泛的Drive檔案異動通知，所以不會有太細緻的內容呈現。\n總結 以上是Google OAuth2 授權模式研究，以及簡單的Google Sheet API研究，評估後符合專案的需求\n1. Server主動append遞增資料\n2. 保留Google表單原始功能\n3. 取得Google表單異動觸發\n因為是對內專案，所以是採用 Server-to-Server授權方式。\n可惜不能得知更進一步的異動，只能自己重新抓資料，在Server端重新比對。\n另外Google Sheet還可以透過 App Script插入資料，客製化前端，之後有空再來研究。\n","date":"2018-06-01T13:03:55.867Z","permalink":"https://yuanchieh.page/posts/2018/2018-06-01-google-sheet-api-%E8%88%87google-oauth2-api%E6%8E%88%E6%AC%8A%E7%A0%94%E7%A9%B6/","title":"Google Sheet API 與Google OAuth2 API授權研究"},{"content":"Transaction 交易機制，可以讓單一或多筆操作聚合為單一的原子性操作，一次性成功寫入或失敗回滾，避免資料庫出現資料不一致的狀況。\nIsolation，關聯式資料庫的基本要素之一，描述當同時有多筆請求要讀寫同一筆資料時的處理狀況。\n以下參考 《Designing Data-Intensive Application》第七章與\n透過 Payment Service 與 DB Isolation Level 成為莫逆之交\n越高層級的Isolation提升資料的一致性，但也帶來效能上的損耗，以下整理實際應用邏輯與對應適合的 Isolation設計。\nDirty Write 複寫其他尚未commit 的 transaction 更新。\nDirty Read 讀取到其他Transaction 更新但尚未 commit 的值。\nRead Skew (Non Repeatable Read) 在同一個 transaction中，讀取的值會受到其他commit的 transaction 更新影響。\nPhantom Read 當其他transaction 插入或刪除資料，會影響同一 transaction中先後的讀取。\nLost Update 兩個Transaction都是走 先讀取某值 -\u0026gt; 基於某值運算 -\u0026gt; 更新原資料 ，例如說購票流程，必須先檢查票種的剩餘票券，如果有剩就建立訂單並扣除名額。\n這狀況如同 票券剛好剩一張，T1 / T2 同時讀取發現票券剛好為一，兩者都以為彼此可以買票，接著 T1 / T2 先後將票券數量改為零，雖然最後票券為零但 T1/T2卻分別消耗了兩次。\n解法 將語法改成 compare-and-set，將原先讀取到的值帶回更新時的判斷式中合併成單一SQL，如 Update ticket where ticket.num = 1 ，這樣較晚更新的 transaction就會更新不到失敗 rollback。 在MySQL中使用 select … for update ，這是 exclusive row lock，當取得鎖後其他 transaction 如果要 select lock in share mode / select ... for update / update / delete都不能取得鎖，直到 commit；\n從而避免並行讀取的錯誤。\n(對比可以並行讀取的是 select … for share ，但同樣會排斥寫鎖) 使用 select for update 就可以避免 t2 在不知道 t1更新的情況下更新，因為 t2 的 select for update 必須等到 t1更新後才能讀取，解決 Lost update 問題\nWrite Skew And Phantom Write Skew是 Lost Update更廣泛地集合，同樣是讀取後修改，但Write Skew讀取與修改的對象可以是不同的(若相同則為 Lost Update)。\n例如說：\n有張 會議室的表，上面記錄哪些會議室被佔用的時間紀錄如 (id1, from 12:00 to 13:00)，當 T1 / T2 同時想要預約同一間會議室同一個時段，在掃過兩者都以為沒有被佔用就插入新紀錄，結果就發生兩筆預約衝突。\n又或是 遊戲暱稱不可重複，T1/T2掃過全部用戶名稱沒有發生重複，插入後才發現 T1/T2的值是重複的。\n上面兩個範例是無法透過 Lost Update提供的解法，因為沒有一個特定的 row 可以去 lock。\n整體上發生Write Skew的條件是\n1. Select 某些資料\n2. 依據上一步的資料做判斷\n3. Update / Delete 一些資料\nWrite Skew 就是發生於 2,3 步之間在重複第一動會發現Select結果是不一樣的，也就是phantom read 的出現。\n解法： 使用 serializable isolation。\n在MySQL Inno DB serializable中 ，select是用表級共享鎖，也就是 select 後其他 select還是可以用，但是全表不能插入更新等 Materializing conflicts 具象化衝突\nWrite Skew是Lost Update的超集合，換言之如果可以將 Write Skew 問題降維成 Lost Update，就可以從 range lock 降為 row lock提升性能。 會產生 Write Skew 在於插入/刪除時沒有特定的鎖可以預先鎖住該行，換言之如果可以預先鎖到就可以避免問題。\n例如說會議室資料，假設我們先將所有的會議室與時間表全部插入資料庫中，這樣當有 transaction 要預約會議室，就可以用 select for update 某行資料，就不會有其他人ˋ並行預約同一間同一個時段的會議室。\n但這不是萬用解，有些狀況不能預先窮舉所有可能。\n這點在文章中，有個範例有用上Serializable， 退費使用ChargeRefund 一張表紀錄，而Order在另一張表，每次要產生退費時就必須先 讀取訂單相關的所有Refund計算額度，如果訂單還有足夠餘額才會創建新的 ChargeRefund 這時候符合Write Skew發生要素，所以他們是用 Serializable 去解；\n我們網站是會將目前訂單退費餘額紀錄在Order上，所以只要透過select for udpate 去鎖該筆訂單就好，不需要用到 Serializable。\n3. 加入判別條件\n利用 Consistence 特性 同樣在會議室問題，如果創建個 unique cluster index (room_id, startAt, endAt) 或許就可以用關連式資料庫本身特性去解決，但同樣不是萬用解\n有趣的是，Serializable 在 MySQL中不是真正的序列化執行 Transaction，他還是可以並行處理的，文件中有提到 「This level is like REPEATABLE READ, but InnoDB implicitly converts all plain SELECT statements to SELECT … FOR SHARE if autocommit is disabled.」\n所以要避免 Write Skew，還是要去思考要鎖多少區間，或是鎖整張表，單單改成 Serializable是沒辦法解決問題的\n在 MySQL InnoDB中，select for update / select lock in share mode 在 Repeatable Read下，會加上 Next Key Lock，Next Key Lock等同於 row lock 加上 gap lock，gap則是根據 index 拆分區段，進而可以分區鎖定，阻擋幻讀的發生； 而select for update 在沒有 where 條件下基本上就等同於table lock。\n在研究過程中，有文章提到 SQL Standard 對於 isolation level定義模糊，各家資料庫的實作也略有差異，所以相對於採用預設的 isolation level，實際了解 locking 機制比較實在。\n總結： 在書中有提到 SQL標準對於 isolation level的定義是不太明確的，導致各個資料庫都會定義各自的isolation level，而且都會有些微的差異；\n所以與其盲目的去使用常見的四種 isolation level，更重要的事針對業務邏輯去分析到底 concurrency 發生的狀況以及如何防止 race condition。\n","date":"2018-05-28T09:29:27.191Z","permalink":"https://yuanchieh.page/posts/2018/2018-05-28-%E8%B3%87%E6%96%99%E5%BA%AB-isolation-level-%E8%88%87%E5%AF%A6%E9%9A%9B%E6%87%89%E7%94%A8%E6%83%85%E5%A2%83%E8%99%95%E7%90%86/","title":"資料庫 Isolation level 與實際應用情境處理"},{"content":"The Twelve-Factor App 是由一群有豐富經驗的工程師，整理開發一個Web應用程式(或是所謂的SAAS software-as-a-service)開發方針，基於以下幾個方向\n設定明確，降低新人加入專案的上手成本 最大化可移植性 適合部署到雲端平台 降低開發與部署的差異性，增加持續部署的敏捷 容易擴展(scale up) 基於上述幾個要點，整理成12項要素，而符合這 12要素的應用程式則稱為twelve-factor app (後文會採用此名詞)，瀏覽過後蠻有趣的，稍微摘要並提出自己在實踐上的想法(主要是Nodejs，作者看來比較愛用Python當範例)。\n為避免歧義或英文水準不佳，有些關鍵字會中英夾雜，或是直接顯示英文單字。\n1. Codebase 用版本控制工具管理一份Codebase，但可以有多份部署 (One codebase tracked in revision control, many deploys) 原始碼必須使用版本控制工具，如 Git / Subversion / Mercurial，每份APP只能有一個Code Repo，但可以有一份Code Repo可以執行多份且版本不一樣的部署 Deploy；\n2. Dependencies 命確定義與隔離相依(Explicitly declare and isolate dependencies) 許多語言程式都有對應的函式庫(library)管理工具，例如Ruby有Rubygems、Perl有CPAN、NodeJS有NPM(或Yarn)，在安裝上有系統層級或是被侷限於專案目錄下(又稱 bundling / Vendoring)\n一個正確的應用程式「不應該依賴系統層級的函式庫」，應該要在宣告文件明確的宣告所有依賴的函式庫；\n並確保函式庫不會影響全域環境，並維持隔離。例如NPM在專案目錄下的 package.json ，預設安裝也是在該專案目錄下的 node_modules 中。\n明確定義的好處是對新人友善，只要瀏覽宣告文件就可以得知所有相依的函式庫，而且通常函式庫管理工具都有自動安裝的指令，如 npm install\n另外專案也不隱式依賴系統工具，例如 curl 、ImageMagick 等，因爲即便這些工具在大多系統都存在，但不能保證應用程式運行的環境有安裝；\n如果需要，則考慮將系統工具也打包進應用程式中。\n3. Config 將設定儲存於環境中(Store config in the environment) config是那些會依據部署環境而有所不同的資料，例如資料庫連線資料/ AmazonS3等外部服務的機敏資料 / hostname等，所以也不會放入版本控制工具追蹤。\n必須注意，程式碼與設定檔「**必須嚴格分離」，**程式碼是所有部署都同一份，並且不應該包含任何機敏資料。\n許多程式語言本身有偏好的設定文件格式，如 xml / yml / json 等，為了方便最好是使用儲存於環境變數中。\n此外，環境變數應該是每個環境獨立於彼此，所以不需要像 Rails用 “production” / “test”/ “development” 在細拆定義環境變數。\n\u0026gt;\u0026gt; 但這裏比較tricky的是公司專案 stage / prod都是跑在同一台機器上，沒辦法用環境變數，都是採用 config.js 當作設定文件\n4. Backing services 將支援服務當作附加的資源(Treat backing services as attached resources) 這裡的Backing services泛指應用程式相依的外部服務，例如資料庫、寄信服務商、快取資料庫等\ntwelve-factor app 應該是可以在抽換外部支援服務而不需要更動程式碼，只需要改變設定檔並重啟而已；\n例如說將資料庫從本機端的MySQL替換成雲端 Amazon RDS。\n5. Build, release, run 嚴格區分建制與執行步驟(Strictly separate build and run stages) 當原始碼要轉換成部署的程序時，會經過三個步驟\n1. 建置 build：\n將程式碼轉換成可執行的包裹(Bundle)，這步驟會抓取外部相依的函式庫並編譯二進制檔案等。\n2. 發布 release：\n將build好的可執行包裹，並結合該部署環境的設定檔\n3. 執行 run：\n執行release階段打包好的程式。\n每份release都必須有特定的編號，不論是日期格式 2011–04–06–20:32:17 抑或是遞增版好 v100 ，方便後續有問題可以回滾。\n此外當發布release後不能有任何更動，只能透過發布新的release。\n建置階段通常是當工程師覺得有新的程式碼更動需要部署，則主動觸發的，所以在建置階段可以執行相對複雜的指令與操作，當發生錯誤時還可以有工程師手動修復；\n但相對在執行階段，可能是自動化執行如應用崩潰後自動重啟，所以在執行階段應該要越簡潔越好。\n6. Processes 執行無狀態的單執行緒或多執行緒(Execute the app as one or more stateless processes) 一個應用程式可能有單個或多個執行緒，twelve-factor app 應該是stateless且 share-nothing ，需要長期保存的資料可以放在外部服務如資料庫或CDN中\n程序的記憶體或是檔案系統只能當作暫時的操作空間，例如下載檔案、操作、接著將結果儲存於資料庫中，應用程式應該確保被暫存在快取與檔案系統的資料在未來不會被用上；\n因為在多執行緒下，未知的請求可能被不同的執行緒所執行，即使是單執行緒程式當系統崩潰時快取跟檔案系統的資料都可能消失。\n有些應用程式會採用 sticky-session 確保同一個用戶被同一個執行緒所服務，這是違反規範的。\n有時效性的暫存狀態資料(如session)可以放在像Redis / Memcached這類的資料庫中。\n7. Port binding 透過綁定埠口對外開放服務(Export services via port binding) 應用程式有些時候被執行在其他容器中，例如 PHP服務可能執行於 Apache HTTPD / Java服務 執行於 Tomcat下，但是 twelve-factor app 應該是服務直接綁定埠口並處理請求。\n這通常都有函示庫可以支援，例如 python的 Tornado / Ruby 的 Thin 等，這使得整份應用程式都是在 user-space執行，且全部包含在原始碼中。\nHTTP只是其中一個可以綁定埠口的協定，其他像是 XMPP / Redis Protocol 等服務都需參照此規範。\n8. Concurrency 透過執行緒模型擴展(Scale out via the process model) 應用程式在執行緒管理上有多種方式，例如 Apache 會在收到請求時開一隻新的子執行緒(Process)跑 php應用，而Java則是透過JVM先保留一大塊CPU/Memory資源接著透過內部分配線程(Thread)達到並行效果。\n在 twelve-factor app 中，執行緒是第一公民，開發者可以決定由哪一個執行緒跑任務，例如 HTTP請求執行在 Web執行緒上 / 背景服務則跑在 Worker執行緒上\n看說明作者舉 Foreman 當做例子(下圖輔助說明)，這樣的好處是重要的任務可以分配較多的資源\n但這並不排除執行緒內部多工處理、VM內部處理線程分配、又或是像 非同步/事件觸發的NodeJS，但是單一VM能夠容量有限，應用程式須可拓展多個執行緒到多台實體主機上。\n這樣的設計在需要水平擴展時會大放異彩，share-nothing / 可水平拆分的執行緒在增加更多並行(Concurrency)時十分地簡單。\n在 pm2 同樣可以做到同樣的事情。\n最後一段提到，twelve-factor app should never daemonize or write PID files. Instead, rely on the operating system’s process manager (such as systemd, a distributed process manager on a cloud platform, or a tool like Foreman in development)\n技術名詞太多直接貼原文，追蹤完相關內文連結大致上想要說明「應用程式應該管好開發應用程式就好，其餘的執行交給工具即可」\n也就是 KISS(Keep it simple, stupid) 的設計理念。\ndaemon是處於背景、不予用戶互動的執行程式，其父進程為 init (在 linux 中為系統啟動後第一個進程，PID編號為1)，init會一直存在到電腦關機為止；\n當有任何父進程先行關閉而子進程仍然執行時，init會轉為這些子進程的父進程，所以常見啟動daemon作法也都是在父進程 fork出子進程後退出，此時子進程就是daemon process了。\n參考 Nodejs Daemon 函式庫就是此作法，用 child_process.spawn 關閉 stdin 並重導 stdout / stderr 與其他設定，另開新的進程。\nindexzero/daemon.node\n9. Disposability 快速啟動+優雅關閉=\u0026gt;增強程式可靠性 (Maximize robustness with fast startup and graceful shutdown) 應用程式應該盡可能降低啟動時間，這樣才可以在快速擴展 / 修改設定時快速啟用服務；\n當應用程式收到SIGTERM訊號時，應當優雅的關閉，也就是停止處理新的請求並將現有的請求處理完成才退出。\n應用程式需要處理非預警的關閉，例如硬體壞掉等，這時候最好有可靠的queueing backend 例如Beanstalkd，當應用程式終止或超時可以自動將任務重新返回 queue中。\n10. Dev/prod parity 盡可能保持環境一致 (Keep development, staging, and production as similar as possible) 因為一些歷史緣由，開發環境與正式環境可能有幾種差異存在\n1. 時間：開發環境可能持續開發了數天到數個月才同步到正式環境\n2. 人：開發工程師負責開發，而維運工程師部署到正式環境\n3. 工具：本地端可能用SQLite 而 正式環境則用 MySQL\ntwelve-factor app 則是要縮短上面的差距\n1. 縮短時間：開發到部署在數小時甚至數分鐘之間\n2. 人：開發者應該緊密的參與正式環境部署\n3. 工具：盡可能用同樣的工具鍊\n環境宣告工具如 Chef / Puppet 結合輕量的虛擬化環境工具如 Docker / Vagrant 可以大幅同步開發與正式環境。\n11. Logs 用串流處理Log( Treat logs as event streams) twelve-factor app 本身不應該去管理或煩惱 Log應該要儲存在哪裡，執行中的程式應當直接輸出到 stdout ，在本地開發上工程師可以直接在終端機介面看到所有的打印訊息；\n而在正式環境上，Log應該被執行環境處理，彙整所有的Log到不同的地方儲存或是到Hadoop/Hive 做更進一步的資料處理，有開源工具可以使用如 Logplex / Fluentd，值得注意是應用程式本身不知道Log其他設定。\n在Nodejs開發中，pm2 有提供基本的Log，但如果要更進階的處理就還是要用 bunyan / winston Log 函式庫，但這就會違反原則，因為變成是應用程式要自己處理Log的部分。\n除非自己另外開發一套直接接 stdout / stderr 的串流工具(好像可以試試看?!)。\n12. Admin processes 執行一次性管理任務 (Run admin/management tasks as one-off processes) 一次性的管理任務如 資料庫Schema更動 / 特定任務腳本 / 使用REPL執行任意的程式碼等，這些一次性的管理任務應當一同放入版本控制，並與應用程式使用相同的工具鍊與設定檔。\n==========\n以上只是個人的濃縮總結，有興趣可以看原資料網站。\n","date":"2018-05-19T07:02:08.523Z","permalink":"https://yuanchieh.page/posts/2018/2018-05-19-the-twelve-factor-app-%E9%96%B1%E8%AE%80%E7%AD%86%E8%A8%98/","title":"The Twelve-Factor App 閱讀筆記"},{"content":"昨天在 Android 手機上用 Youtube 分享影片到 LINE 聊天室給朋友，突然靈機一動覺得 APP 有這麼方便的分享機制，那我是不是可以在 Chrome 用 Extension 方式整合社群分享，按一個按鈕就可以把當前頁面的連結或文字快速分享；\n正好之前也沒有開發過 Chrome extension，正好拿來練練技術，但果不其然在 Chrome extension shop 有看到很大量的社交分享工具了 OTZ 但還是不死心想要自己動手玩玩看\n參考資料：\nChrome team 本身的官方教學還不賴\nGetting Started Tutorial — Google Chrome\n強者 羅拉拉在 2016 年 IThome 鐵人賽的系列文，前半段把官方教學翻譯，後續加上作者個人開發分享，非常棒的教學。\nChrome Extension 開發與實作 系列文章列表 — iT 邦幫忙::一起幫忙解決難題，拯救 IT 人的一天 以下為個人開發簡略與簡化過後的筆記，所以要看詳細版還是看上附兩個文件\n基本介紹 基本上開發 Chrome extension 跟寫網頁很類似，只是改了檔案結構、發布機制、API 調用多增加了 chrome.* 函式\n在 extension 開發中，目錄下必須先宣告 manifest.json，裡頭會註明 extension 的名稱/icon/所需權限等資料，還有不同時機對應執行的不同腳本\n基本上開發會有幾個頁面可以選擇\nbackground page 背景頁面：\n基本上是常駐，從安裝插件之後就一直執行到插件被移除(不完全正確的描述，但基本上可以這樣理解)\n所以會在 background 指定的 script 中執行套件安裝或更新，又或是初始化的一些程序 popup page 彈出頁面：\n也就是右上角的小圖標，點擊後產生的頁面，又細分成 browser-action / page-action；\nbrowser-action 代表 Extesion 是普遍性在每個網站都可以用，而 page-action 則代表只針對某些特定網站使用，平常會自動呈現灰暗的 icon 直到顯示宣告 pageAction.show()，像是 vue-devtool 只有在網站使用 vue.js 才能使用。 option page 選項頁面：\n應用程式如果太過於複雜，希望有個選項頁可以供客戶客製化行為，可以採用 content script：\n在 Extension 中，只要有索取權限就可以對每個頁面注入 content script，進行 DOM 操作等一般 js script 會執行的動作，可以看成合法的 XSS 注入。 實戰分享 sj82516/quick-share\n定義與載入其他 script 可以參考 manifest.json，這裡我用了\nbackground：event.js，主要註冊與定義右鍵按下出現的選單行為\npopup page：分成 popup.html / popup.js，主要是定義瀏覽器右上角的工具列跳出視窗行為，並定義為 browser-action\n因為右鍵行為跟 popup 行為類似，所以我將行為收整成 util/handleShare.js，需注意在 background 中要使用其他 script 必須宣告在 manifest.json 中；\n1 2 3 4 \u0026#34;background\u0026#34;: { \u0026#34;scripts\u0026#34;: [\u0026#34;util/handleShare.js\u0026#34;, \u0026#34;event.js\u0026#34;], \u0026#34;persistent\u0026#34;: true } 而在 popup page 則以 \u0026lt;script src=\u0026quot;util/handleShare\u0026quot;\u0026gt;加入 html 中即可。\n另開新頁、Popup script 與 Content script 溝通 社群媒體分享必須另開該網站的新頁面，同時為了可以自動把反白選取的文字自動插入，在開啟新頁，首先要設法取得新頁面的 Tab，之後要執行 content script\n1 2 3 4 5 6 7 8 9 10 11 12 13 // 先取得所有的分頁資料，可以透過 tab 的 url / title 找到指定頁面 chrome.tabs.query({ active: true }, function(tabs){}) // 必須確保分頁已經加載完成，這樣 content script 才有用 chrome.tabs.onUpdated.addListener(function (tabId, info) { if (tabId == factoryTab.id \u0026amp;\u0026amp; info.status == \u0026#39;complete\u0026#39;) { // 在該分頁注入 content script chrome.tabs.executeScript(factoryTab.id, { file: \u0026#34;content/share.js\u0026#34; }) .... 上面有個小 tricky 是必須要等到新分頁加載完成才可以執行 chrome.tabs.executeScript ，否則會出現 tab is closed 之類的錯誤訊息!\n在跨 script 傳輸資料一次性可以使用sendMessage(tabId, message)/ chrome.runtime.onMessage.addListener((message, sender, sendResponse)=\u0026gt;{}) 其餘方式可以在查詢文件。\n結語 開發一個簡單的 Chrome Extension 蠻有趣的，但也意外發現 Extension 的威力極為強大，尤其是可以注入 content script 這塊，所以搜尋一下 chrome store 可以找到例如密碼管理工具等應用，突然覺得一陣害怕因為整個 DOM 都被摸光了，甚至 cookie 再不小心授權出去後也都可以被讀取，在安裝插件上務必小心啊！ 尤其是沒有開源、非官方的插件更需要小心。\n不過我上傳的插件被 Google 認定為 Spam 不給上架……\n","date":"2018-05-15T09:56:14.414Z","permalink":"https://yuanchieh.page/posts/2018/2018-05-15-chrome-extension-%E9%96%8B%E7%99%BC%E5%88%86%E4%BA%AB/","title":"Chrome extension 開發分享"},{"content":"至從看了《娛樂至死》這本書後，開始思考「外在環境與內在基因如何影響人類思維」這類型的議題\n《娛樂至死》大致上是在描述人類思考方式受到不同媒介，印刷式紙本與電視的影響，而有不同的轉變；\n電視時代充斥著大量的訊息，迫使人們思考開始變短促，在網路、社群時代這個影響越發劇烈，所以才有了金魚腦一詞\n順著這個議題，近三週看了所謂 反烏托邦系列三部曲\n《我們》《1984》《美麗新世界》，所謂反烏托邦即是對於傳統的美好與幸福提出強烈的質疑，並以諷刺的文字來探討人們追求的完美生活。\n《我們》 這本以第一人稱視角，用短篇日記的方式推動劇情，大體上描述一個大一統帝國，人們只追求理性、直線、公式化的生活，每個公民都是為了大一統帝國而存在，沒有太多個人的思維；\n整個帝國是以科技為導向，並用一個玻璃罩限制帝國居民的移動，玻璃罩之外保留野生的叢林以及過著原始生活的野人。\n主角是帝國中的首席飛船設計師，飛船設計目的是為了將這種美好理性的社會理念散佈到全宇宙，主角一開始以此為榮，深信人活著就是為了大一統的帝國而存在，不該有其他個人的想法；\n但後來遇上了一位女角，因為戀愛的衝動，開始有了自己的思想，而思想是一種罪。\n整本書說實在我看起來也略顯冗長與沈悶，因為其實故事線不複雜、要闡述的道理也蠻直觀，作者花蠻多章節反覆的描述類似的事件，像是愛情、個人叛逆思想的萌芽、社會外在環境大一統規範間的拉扯。\n《1984》 描繪高壓統治的軍權帝國，老大哥用各種方式洗腦、改寫歷史，書中幾句話深刻切中要害\n「誰控制過去就控制未來，誰控制現在就控制過去。」\n主角的工作便是配合黨的政策竄改所有文書紀錄，今年生產部只生產了兩千頓糧食沒有達到政治口號的增量50%，沒關係改去年紀錄就好；\n當所有文書、報紙都被竄改，唯一的真相只留在作者腦中，但這真相就真的是「真相」嗎?!\n主角後來愛上了一位叛逆的青春期少女(至少角色描繪起來很像)，到最後因為愛情而付出代價，主角一開始以為不論肉體如何折磨，只要夠堅持就可以保護心中的一方淨土；\n但是監控單位開始用各種酷刑折磨主角的思想，最後狠狠地把愛情也連根拔起，作者描述的十分寫實，非常的黑暗….\n《美麗新世界》 跟前兩本書描述方式恰好相反，但更令人毛骨悚然；\n書中描述人類由基因培養所批次製造，在出生時基因就已經決定好你的所有外觀、思想包含社經地位，出生後有所謂的「制約」，透過睡眠學習法不斷洗腦，例如說 討厭紅色，在5–7歲每天晚上播放5000次，自然而然長大後已經無法方便為什麼會討厭紅色，只會覺得「很自然就該討厭紅色」\n整個世界被管治者所規範，整體社會定期提供「索麻」，一種迷幻類似於毒品，四處皆有音樂、不用煩惱糧食，沒有傷痛沒有老化，這似乎就是「美麗的新世界」\n後來故事推演到野人保留區，雙方的思想開始衝突，到底要痛苦的苦難還是美好的和諧? 野人不斷用莎士比亞劇本的台詞來表達個人思想對於真善美、苦難的堅持等傳統美德，來諷刺與對抗新世界中的放蕩、及時行樂、被規範好受到制約的社會。\n這幾本書建議在風光明媚的戶外看，因為很黑暗XD\n這幾本書描繪的社會都很不同，但非常驚人的一致性就是\n1. 視個人思想為犯罪\n2. 剝奪愛情，但有趣的就是主角都是透過愛情找到自我的思想\n3. 去除家庭，完全去除親子間的羈絆\n4. 拒絕個人獨處，強調社會的和諧與一致\n但我喜歡這種諷刺的筆法，因為他可以最大程度的激發我想捍衛思想、自由、愛情等，書中社會所排斥或禁止的不正好是身而為人最重要的因子嗎?!\n只是真的很黑暗，尤其是《1984》最後一部分…. 小心服用\n三本書看來，我其實最喜歡《美麗新世界》，前陣子有遇到佈道者在傳教，他問我「如果可以到天堂無憂無慮，沒有痛苦，你應該也會想去吧?」\n那時候我愣住了快30秒，他只好訕訕地說「對嘛，沒有人喜歡在凡間痛苦的生活…. 」\n當時我猶豫的點就是 什麼是永生? 什麼是天堂? 無憂無慮沒有苦痛聽起來很好，但這樣還存活著是要幹嘛?! 又或是在這種狀態下怎麼知道自己還活著?! 會不會就變成是一個美麗新世界?!\n這或許就是我現在沒有宗教信仰的原因，又或是現在的自己沒有太多苦痛的經歷，所以不會也不想去思考這類的問題，只知道現在每天活著可以工作、可以運動、可以閱讀、可以思考就蠻充實的，我也不是抖M也不想一直處於苦痛的狀態XD\n但就是不會去想過著無憂無慮的生活，就覺得很彆扭吧\n我還是搞不懂信仰宗教，又或是各方教義中的「天堂」「西方極樂世界」到底是怎樣的地方? 靈魂處於那種狀態下又可以幹嘛呢? 永生到底是怎樣的狀態?\n有點不解但也沒有很在意\n上天給了一副牌，坐上台桌就盡興的賭到最後吧\n* 之後想看《人類大歷史》《人類大命運》《槍砲鋼鐵與細菌》，用不同的角度來思考這個議題\n","date":"2018-05-06T02:11:33.686Z","permalink":"https://yuanchieh.page/posts/2018/2018-05-06-%E9%96%B1%E8%AE%80%E5%BF%83%E5%BE%97%E6%88%91%E5%80%911984%E7%BE%8E%E9%BA%97%E6%96%B0%E4%B8%96%E7%95%8C%E5%8F%8D%E7%83%8F%E6%89%98%E9%82%A6%E4%B8%89%E9%83%A8%E6%9B%B2/","title":"[閱讀心得]《我們》、《1984》、《美麗新世界》反烏托邦三部曲"},{"content":"MySQL :: MySQL 5.7 Reference Manual :: 13.1.18.6 Using FOREIGN KEY Constraints\n整理閱讀MySQL v5.7的FOREIGN KEY Constraints文件，並針對細節在DB Fiddle加上範例，後續FOREIGN KEY縮寫FK。\nFK限制可保證資料關聯間的完整性，關係是建立child Table指定要與 parent Table的某欄位建立關聯，FOREIGN KEY 必須在 child Table中指定，child / parent需使用相同的DB引擎且不可為 TEMPORARY Table。\n在宣告語法上，可以透過 [CONSTRAINT symbol ] 指定這筆關聯限制的名稱，但須注意名稱不可重複否則會出錯；\n如果擔心就不要宣告 CONSTRAINT讓 DB Engine自動產生\nDB Fiddle - SQL Database Playground\nFOREIGN KEY 資料型別 有趣的是，MySQL文件只說parent中的資料欄位與child FOREIGN KEY欄位的資料型別必須類似而非一致，又可細分成三類\ninteger：\n_Size \u0026amp; Sign必須相同，例如說 TINYINT / INT 就是不一樣的 string：\n長度不一定要相同，因為不論是parent長或是child長，FOREIGN KEY在插入時必須parent存在該筆資料；\n所以在插入不可超過欄位長度與資料必須存在的兩個條件下，字串長度的限制就不是這麼必要 對於非位元型別的字串，character set 和 collation必須相同 FOREIGN KEY 限制 除了上述的資料型別限制外，MySQL為了搜尋上避免 full table scan\nparent 的欄位必須是「某Index中的第一欄位」，例如 index(id) / index(id, uid, ….)，因為MySQL的Index順序是由左至右，所以如果是多欄位Index只要該欄位是在第一順位也可以當作建立 FK的欄位。\n假設不符合條件，會直接出錯無法建立child Table。 child 的FK欄位必須是「某Index中的第一欄位」，如果沒有會自動創建。 FK可以是多欄位，但同樣要符合「某Index中的前面順位」 又因為FK必須建立Index，所以像 TEXT / BLOB 就不可能是 FK選項。 關聯操作 在定義FK時可以指定關聯操作，也就是如果 parent的對應FK欄位發生改變(UPDATE / DELETE)時 child FK欄位該怎麼應對，總共有幾種定義：\nCASCADE:\n如果是 parent紀錄被刪除了，那對應FK的chlid紀錄也一併刪除；\n如果是 parent FK更新了新值，則child 對應的FK會更新。 SET NULL:\n刪除更新 parent都會導致 child的 FK紀錄設為 Null，如果設定為 SET NULL 記得 「child FK 欄位不要加 NOT NULL 會衝突」。 RESTRICT:\n只要child中有包含該 FK值，完全不能刪除該筆parent或更新 parent的FK欄位。 NO ACTION: 在MySQL等同於 RESTRICT。 SET DEFAULT:\n在MySQL InnoDB中不支援。 DB Fiddle - SQL Database Playground\n增加或刪除FK 可以透過 ALTER TABLE 來增加FK，透過 DROP TABLE 指定刪除FK，但如果當初沒有使用 CONSTRAINT 宣告FK的名稱，可以用 SHOW CREATE TABLE 取得Table資料，即可得知系統自動創建的FK名稱。\n暫時關閉 FK CONSTRAITS 當使用 mysqldump 時，為了方便重建資料MySQL會自動先關閉FK CONSTRAITS的限制\nmysql\u0026gt; SET foreign_key_checks = 0;\nmysql\u0026gt; SOURCE dump_file_name;\nmysql\u0026gt; SET foreign_key_checks = 1;\n但必須注意，即使關閉了在創建 FK時仍然不可以違背 FK資料型別的驗證，不然一樣會噴錯誤!\n關閉限制後刪除/更新都不會檢查 parent是否存在對應資料，即使重新開啟限制檢查也不會回溯，所以務必小心。\n","date":"2018-04-24T10:16:08.524Z","permalink":"https://yuanchieh.page/posts/2018/2018-04-24-mysql-foreign-key-constraints-%E6%95%B4%E7%90%86/","title":"MySQL FOREIGN KEY Constraints 整理"},{"content":"上集:Designing Data-Intensive Applications 上\n隨著軟體應用程式的發展，應用的侷限(bottleneck)從CPU移轉至資料的處理，資料的巨量、複雜性與改變的速度變成棘手的問題，也就是作者所定義的「Data-Intensive」資料密集的應用程式。\n第二部分主要探討分散式儲存資料會遇到的問題與解法，分散式系統有幾大優點：\nScalability：\n大幅增加系統的乘載能力 Fault Tolerance / High Availability(HA)：\n當單一節點失效時，不影響系統的運作 Latency：\n如果用戶散佈全球，可以透過多節點部署使用戶地理位置最近的資料中心，降低網路延遲 Ch5. Replication Replication 也就是將相同的資料透過網路複製到多台機器上，有幾個好處\n1. 讓用戶可以取得地理位置最近的資料\n2. 即使部分機器失效，系統也可以繼續正常運作\n3. 增加機器 讀 的負載能力\n在架構上有三種 Single-Master / Multi-Master / Leaderless，Replication帶來許多好處，但是在系統實作面會有取多的衡量，例如說複本是要同步還是非同步、如何處理錯誤的複本節點、最終一致性的曲解、read-your-writes、monotonic-reads保證等問題。\nLeaders and Followers 每個擁有資料庫備份資料的節點稱為 replica，為了讓每筆寫入都可以被同步到replica上，常用的架構為 leader-base，也就是Leader 負責資料的寫入，並將資料同步到 Follower中保持同步。\n這種replication架構被廣泛使用，包含Oracle / PostgreSQL / MySQL / Kafka等。\nSync and Async 當Leader接收到寫入，這時候要決定說是否等到同步完全部的 follower才回傳成功，如果是全部 follower說成功才成功便是 Sync，若非則 Async，也可以設定超過某數量 follower 變成 Semi-Async；\nSync好處是確保follower都擁有最新的資料，避免用戶在follower讀取到過期資料，但缺點就是如果一個 follower卡住了就會阻擋整個系統運作，系統容錯能力就大幅下降；\nAsync好壞處則剛好相法，寫入回應速度快，系統容錯好但用戶可能會讀取到過期的資料。\n通常來說Leader-base系統都是以Async為主，因為讀取過期資料這點後續有其他方式可以彌補。\nHandle Node Outages 當節點出錯要重新恢復時，會有不同的狀況，又可拆成 Leader / Follower來處理\nFollower出錯要恢復或是加入新節點比較容易，只要資料記錄有追上Leader最新資料即可\n但是Leader出錯就非常麻煩，必須要先 判定Leader死亡(通常透過Timeout) -\u0026gt; 選出新的Leader -\u0026gt; 整個系統承認新的Leader，並將寫入請求轉導到新的Leader上，這裡會有幾個容易出錯的地方\n例如說 舊的Leader如果是採用Async，有可能會有部分寫入尚未同步到備份中所以會遺失，在Github採用MySQL案例中就發生過掉資料結果 Primary Key重複的現象；\n又或是程式沒設計好，意外選出多個Leader，產生人格分裂(Split brain)；\nTimeout設定也很重要，太短會導致網路波動就造成不必要的Leader切換，太長則遺失的資料可能會很多，這部分很麻煩；\n基於這些理由，作者提到現在有蠻多公司都採用人工處理的方式。\nImplementation of Replication Logs Statement-based replication\n最簡單的方式是直接將每筆寫入都pass到 replica上，但這會有幾個壞處，如 NOW() / RAND()等函式會因為執行的時候值而有所不同、Auto-Increment也會有所差異，此外還必須保證複製的過程寫入資料順序必須相同。 Write-ahead log (WAL) shipping\n將WAL的資料直接複製到replica就不會有上述的問題，但缺點是log相當底層，也就是會跟系統的儲存引擎嵌套很深，跨系統不易。 Logical (row-based) log replication Problems with Replication Lag Reading Your Own Writes\n當用戶更新資料到Leader上，如果該用戶馬上需要讀取該筆資料，可能Follower還沒有收到複製回傳了過期的資料；\n解決方法有 透過應用程式追蹤時間，當在更新後的一定時間內從Leader讀取資料、\n用戶紀錄更新資料的時間，由資料庫確保從replica取出正確的資料 Monotonic Reads\n當用戶從多個replica讀取資料，可能某些replica同步比較慢導致用戶讀取多次資料卻不同；\n可以綁定用戶在同一個replica讀取資料，避免不一致的資料狀況出現。 Consistent Prefix Reads\n同樣是因為replica同步有時間差，用戶可能會讀到順序錯亂的資料，例如問題勢必在答案之前出現(有問題才能有回答) Multi-Leader Replication 多Leader的架構系統容錯性更好、寫入的性能也可以提昇，但缺點是資料衝突的情況會很多，當發生衝突時很多時候必須由應用程式來解決。\nLeaderless Replication 寫入和讀取都一次使用多個節點，透過量來保證資料的正確性與系統容錯性，例如總共有5個node(n)，只要我們可以寫入3個以上節點(w)與讀取時讀到3個節點以上(r)，在符合 w + r \u0026gt; n的情況下，就可以保證會讀取到最新的資料，此時系統可以容忍 n-w 個節點失效。\nhttps://tech.liuchao.me/2017/12/ddia-5/\nCh6. Partition Partition分片主要是提升系統的Scalability，當大量的資料可以被平均分散到節點上，寫入和讀取「理論上」就可以隨著節點數成線性成長；\n在實作上，Partition常與Replication做搭配，將資料分片並複製到其他節點上增加容錯空間\nhttps://tech.liuchao.me/2017/12/ddia-6/\n有個良好的資料分片機制很重要，如果分片的方式不夠號，如遇上 hot-key產生歪斜skewed，導致資料不平均分散到節點上，會導致系統的性能無法得到顯著的提升，以下有兩種常見的分片機制\nPartitioning by Key Range 一種常見的做法是將 Key排序並將某連續段的Key分片，這種做法好處是簡單實作、如果要範圍搜尋資料很方便；\n缺點是容易會有 hot-key的問題。\n所以在套用上需要特別注意應用程式本身的資料Key特性，例如說感測器收集系統，採用 timestamp 當作key可以使後續日期範圍查詢非常容易，但缺點是當天的寫入都會集中在同一個partition上；\n可以考慮改成用 感測器編號，這樣寫入就可以平均分散，但相對就變成範圍讀取比較麻煩。\nPartitioning by Hash of Key 可以透過 hash function將key轉成可平均分散的雜湊值，好處是寫入可以更平均分散，但缺點就是失去了範圍搜尋；\nCassandra嘗試透過組合Key來融合上面兩種方式，Key的前半段用Hash決定partition位置，partition中則依照Key的後半段排序增強範圍搜尋；\n例如說(user_id, update_timestamp)，透過user_id分散用戶的資料，但如果有需要取得某用戶的範圍資料更新就可以用update_timestamp。\nPartitioning and Secondary Indexes 先前提的是透過 Primary Key，但如果要查詢 Secondary Key就會遇到不同的問題，例如說 汽車資料是 {carId , name , color}等，主鍵是carId並依此來分片，但如果用戶要查詢 color = ‘red’的車子就需要另外處理\nPartitioning Secondary Indexes by Document 在個別partition中自行維護各自資料的 Secondary index table，所以用戶的讀取請求需要送到每個partition中並整合，也就是 scatter /gather。\n缺點就是讀取效率很差，因為可能某些partition回復比較慢整個請求就會被卡住。\nhttps://tech.liuchao.me/2017/12/ddia-6/\nPartitioning Secondary Indexes by Term 與其個別partition分別維護Secondary index(local)，可以將相同的Secondary index紀錄統一儲存在某特定partition中，透過讀取該紀錄，再去所有紀錄所在的partition讀取資料即可。\n至於該筆 Secondary index要記錄在哪可以透過前述的兩種方式分散到不同node上。\nhttps://tech.liuchao.me/2017/12/ddia-6/\n這種方式提升讀取的效能，但缺點是寫入變得十分複雜，修改單一筆資料需要同步更新不同partition中的 Secondary Index，這在分散式中會有很多隱藏的問題。\nRebalancing Partitions 如果系統附載增加，可能會需要增加 partition的機器或是移除出錯的機器等，這時候會需要重新調整partition，將資料重新平衡到所有機器上。\n平衡的方式有\nhash mod N：\n直接把 hash key mod {所有機器數}即可，但缺點是增加一台機器就會有大量資料需要搬移，造成不必要的負擔 Fixed number of partitions：\n將所有資料切成很多份，並平均分散到機器上，例如說 10台機器可以將資料分成 1000份，平均每台負擔 100份資料；\n此時如果有新機器加入，這10台機器每台抽9~10份搬移即可，如果要移除舊機器反之；\n這種方式有效降低不必要的資料搬移。 Dynamic partitioning 使用 Key-range partition不太方便，如果一開始設定錯誤，會導致資料太過集中於部分節點，如果要重新設定又會很麻煩；\n所以某些資料庫(HBase、RethinkDB)內部提供動態分割的方法，當發現 partition資料超過某個size，會自動分割並把資料分成兩個partition；\n如果資料大量刪除，也會自動合併成單一個partition；\n每個partition分散到一個節點上，而一個節點可能儲存多個partition。\nDynamic partition可支援 key-range partition和 hash-key partition。\nRequest Routing 當用戶要發送請求，如何得知該往哪個節點發送呢? 這個問題通稱為_service discovery_ ，任何透過網路的軟體都會遇上此問題，尤其是分散式系統。\n主要解法有三種方式，\nclient隨機發送請求到某節點，每個節點都有分片依據的紀錄；\n統一一個負責routing的節點；\nclient端自行判斷紀錄分片的依據。\nCh8. The Trouble with Distributed Systems Faults and Partial Failures 分散式系統傳遞資訊都是靠網路，但網路的本身是不可靠的，所以分散式系統最大的疑慮便是會有部分錯誤的產生，在某些時刻我們無法得知操作是否完全成功，所以在設計上必須考量這一點，設計出可容忍錯誤(Fault-Tolerance)的系統。\nUnreliable Networks 談論一些網路的不可靠性，對比Telephone circuit 用硬體與固定頻寬的連線方式傳輸資料，TCP為了增進總體使用量會有flow control導致傳輸速度無法估測。\nUnreliable Clocks 時間在電腦系統扮演重要角色，大致有兩個時間屬性 區間(Duration，如計算request時間)、確切時間(points in time，錯誤的時間log)，再分散式系統中時間很tricky，因為溝通不是即時的，透過網路傳遞訊息會產生不可預測的延遲；\n再者每台電腦都有各自的石英震盪器計算時間，因為各種物理性質上的差異導致無法完全精準同步每台電腦的時間。\nMonotonic Versus Time-of-Day Clocks Time-of-day clocks(又稱 wall time)：\n返回日曆時間，通常是透過NTP同步時間，但如果電腦時間太快或太慢，同步之後可能會產生時間跳躍，產生一些後遺症(如資料庫寫紀錄的跳針導致資料不一致) Monotonic clocks：\n適合用來測量時間區間，「實際上他指的是系統啟動後流逝的時間，這是由變數jiffies來紀錄的。系統每次啟動時jiffies初始化為0，每來一個timer interrupt，jiffies加1，也就是說他代表系統啟動之後流逝tick數**」\n**(出處：[Timer学习]wall time和monotonic time)但也因為如此，所以跨系統間比較Monotonic clocks是沒有意義的。 Timestamps for ordering events 假想系統有2個Node，當兩個客戶A、B同時對同一筆資料做改寫，但是A的Request先到Node 1而 B先到Node 2，再處理併發時分散式系統通常採用LWW(Last-Write-Win)，所以更新後Node 1與Node 2資料就會不同步\nProcess Pauses 試想一個危險的情況，資料庫採用Single-Leader架構，Leader需要固定一段時間通知所有Follower Leader還活著，但如果Leader的執行卡住了過一段時間沒有通知，其餘Follower選出新的Leader後同時舊Leader復活，產生了雙Leader的分歧局面\n在以下情況會產生程式執行被無預期卡住，如垃圾回收(GC)，這會導致程式運作直接卡死直到垃圾回收結束；\n在虛擬機中VM也有可能被暫時終止執行、又或是在CPU切換任務時系統負載過多而延遲等等因素\nResponse time guarantees 上述問題產生於 應用程式不知道執行時會遇到怎樣的突發終止狀況，所以如果可以保證突發終止的時間是可預測的，就可以透過設計避免問題，但問題就是如何保證回應時間?\nRTOS(real-time Operating System)保證CPU的時間切割是固定的，所以可以得知最糟狀況應用程式會被終止多久；\n但RTOS開發很貴，通常用於高度安全的系統設計如飛行系統，而且RTOS會限制應用程式可用的Library、寫法等，不適用於一般資料庫系統。\nThe Truth Is Defined by the Majority 分散式系統中，每個節點只能透過網路收發資料來確認彼此的狀態，但有時候節點運作正常但在與其他節點的網路溝通上出了問題，或是遇上GC整個節點卡住等，就有一樣會被其他節點誤認為「死亡」\n正因為分散式系統解決了單一節點失效的問題，失效的決定則依賴多數的節點的決定。\nByzantine Faults 先前描述都是以節點失效為主，但如果節點會「說謊」呢? Byzantine Generals Problem 拜占庭將軍問題即是描述多節點在不知道彼此的情況下如何確保叛徒不會影響正確性\n拜占庭将军问题 - 维基百科，自由的百科全书\n結論大致是只要出錯的結點不超過 1/3即可保證整體的正確性。\n","date":"2018-04-19T04:08:48.494Z","permalink":"https://yuanchieh.page/posts/2018/2018-04-19-%E6%8A%80%E8%A1%93%E7%AD%86%E8%A8%98-designing-data-intensive-applications-%E4%B8%8B/","title":"[技術筆記] Designing Data-Intensive Applications 下"},{"content":"隨著軟體應用程式的發展，應用的侷限(bottleneck)從CPU移轉至資料的處理，資料的巨量、複雜性與改變的速度變成棘手的問題，也就是作者所定義的「Data-Intensive」資料密集的應用程式；\n作者前言提及 在現今Buzzword滿天飛 Big Data / NoSQL / Web Scale blablabla，身為技術人員應該更透徹的了解資料儲存的基本觀念，這些才是永恆不變的基石；\n有了清楚的概念，才能在不同的應用場景套用最適合的解法，老話一句\n沒有銀彈\n此書切成三大部分：資料系統的基本組成(Foundation of Data Systems)、分散式資料儲存(Distributed Data)、取得資料(Derived Data)；\n作者章節設計循序漸進，先定義基本用語與資料系統的設計理念與實作方式，接著加深內容且不斷的引用前面所述說的觀念；\n以下是我簡單筆記本書2/3的內容，Derived Data目前有點看不懂所以就先暫時跳過 OTZ\nCh 1. Reliable / Scalable / Maintainable 再討論系統設計時，就必須先反問「想要設計出符合怎樣需求的系統」\n作者提出三大軟體設計的核心\nReliable 運作需與使用者預期的相同 可以容忍用戶以非預期方式操作系統 效能需好到可以應付用戶需求 防止非授權登入與濫用 基本上就是要能夠在一定的錯誤容忍下運作正常，常見的錯誤有\n硬體錯誤：\n作者提及像硬碟的 MTTF(平均時間出錯)是10~50年，如果你的資料中心有10,000顆硬碟基本上預期是每天都會壞一顆! =\u0026gt; 常見解法是：增加硬體冗余、使用RAID、額外供電設備、異地備份等 軟體錯誤 人為錯誤 Scalable 隨著用戶增加，系統需要負擔的資料也成幾何增長，所以需要系統的擴充性去回答「我們如何增加運算資源來應付增長的流量?」\n再回答此問題之前，必須先知道如何描述「此刻系統的負載量」，根據不同的系統設計有不同的衡量方式，例如常見的 request per seconds / 資料庫讀寫比 / 最大同時上線人數等等，也就是要找出不同的「關鍵附載係數(Key Load Parameters)」；\n衡量性能部分可以透過 percentile追蹤，分析回應速度的百分比圖，Amazon有做一個有趣的分析指出 最慢的Request意外發現是最有價值的客戶，因為往往他們的Request夾帶最多的資訊，所以也導致速度最慢；\n作者舉 Twitter在2012年做用戶發送訊息為例：\nTwitter 有兩個主要操作\n1. 發 tweet：\n用戶發送新訊息(4.6K req/sec，高峰超過 12K req / sec)\n2. Home timeline：\n用戶查看他們追蹤的對象消息列 (300k req/sec)\n在設計上有兩種方式：\n1. 發tweet時就是單純插入新的一筆紀錄，如果有用戶要讀home timeline就做 DB Join取出所有追蹤對象的訊息\n2. 針對每位用戶都Cache home timeline，如果有用戶新增tweet就加到對應追蹤用戶的cache中\nTwitter後來從方法一轉至方法二，原因是因為方法二的讀取timeline速度快了兩個級別，所以傾向於 花多點時間在寫入以節省龐大的讀取時間。\n對於Twitter來說，用戶的追蹤數就是關鍵附載係數 **，**但每位用戶的追蹤數與被追蹤數差異十分的大，尤其是名人，所以Twitter後來採用兩種方法混用，一般用戶採用方法二，而名人則另外處理。\nMaintainable 軟體最大的花費不在於建置，而在於維護，此處又拆成三個子項目\nOperability\n透過一些方式可以讓軟體更好維運，例如 監控系統健康、錯誤追蹤系統、定期更新系統、隔離不同系統等 Simpilicity\n隨著系統營運不可避免功能一直加，系統也相對跟著複雜，這裡的簡易性不是說要去削減功能，而是**避免不必要的複雜性，**這裏複雜性的定義是「企圖加入實作層面的解法而跟解決問題無關」，要避免複雜性就必須透過 **抽象化(Abstraction)，**例如說高階語言理當不需要理會CPU的暫存器等操作，因為語言本身已經透過抽象化隱藏了不必要的實作複雜性。 Evolvability\n需求會加、功能會改，所以系統必須保留彈性面對改變。 Ch 2. Data Models and Query Languages 這一章探討資料儲存的形式，也就是 Relation / Document / Graph Data Model，資料儲存的形式很重要，這會決定性影響了應用程式的編寫以及底層儲存方式的不同；\nRelation Data Model最為常見，將資料的關聯以tuple的集合儲存(也就是SQL的row)，但是現今的程式語言都是物件導向，所以要在應用程式中操作資料就必須透過ORM將資料轉換為物件形式；\n這也是為什麼會有 Document Data Model，因為資料(在沒有複雜關聯下)的本身就是個文本物件，這裡的Document偏向於描述可以用 JSON / XML 結構化語言描述的資料格式，例如 履歷 {name: “hello”, age: 25, ….}。\n但現實上資料本身有不同的關聯性，有One to Many 也有 Many to Many，基本上採用何種Data Model可以從資料集多對多的關聯性複雜度來決定；\n如果資料僅有少部分多對多關聯，可以選擇採用Document Data Model，因為相對應用程式的代碼比較好寫，不需多一層ORM轉換、少有的多對多可以用應用程式代碼自己實踐Join功能；\n如果注重物件的多對多關聯性則可用 Relation Data Model；\n如果物件的關聯性相當複雜可以考慮 Graph Data Model，這也是我第一次接觸到此觀念，資料模型改以圖形化方式呈現，資料可分成vertice與edge，vertice比較像是儲存單體的資料，而edge則儲存vertice間的關係，透過圖形化的特性可以表達相當複雜、遞迴的關係，這部分可以參考我之前嘗試 neo4j的筆記。\n當然上述只是單純以資料關聯與對應的模型來衡量，現實的技術採納需要有更全面通盤的考量。\n這一章比較是在講古，把過去曾出現過的資料模型與對應的Query語言都稍微帶到。\n最後筆記一點蠻有趣的觀點，作者提到 Document Database都會被誤稱為 schemaless，但其實更準確說法是 schema-on-read，雖然DB沒有顯性的資料欄位，但是在應用程式讀取時一樣會有資料的架構，對應的是Relation Database的schema-on-write；\nschema-on-read有點像是動態語言，資料的檢查在應用程式本身；而schema-on-write像靜態語言的型別檢查，在寫入時就先檢查了。\nCh 3. Storage and Retrieval 這章節算是我覺得最有趣的一章，主要談到資料庫底層如何將資料存入硬碟中，以及如何快速的透過索引 Index 從硬碟取出資料；\n首先作者用shell script編寫最簡單的資料庫\ndb_set () { echo “$1, $2” \u0026raquo; database}\ndb_get () { grep “^$1,” database | sed -e “s/^$1,//” | tail -n 1}\n$ db_set 123 ‘{name: “123”}’\n$ db_get 123\n採用類似 CSV格式的 鍵-值儲存，將新的資料不斷append到文件上，最後搜尋時從文件最末的資料開始找起；\n這樣寫的效能很好，因為單純append資料速度非常快，但是讀取則需要O(n)的時間；\n為了增加讀取的效能，我們可以建立索引，在記憶體中用Hash Table資料結構，儲存鍵對應欄位在硬碟的儲存位置，加速讀取的效能。\n雖然這看起來很簡單，但實際上卻也有資料庫是採用此設計方式，如 Bitcask。\n先前提到這種做法是不斷將新資料append到硬碟文件上，即使同樣的鍵也是，但是這樣文件總不能無止盡的增加下去，所以會有所謂的 compact process 壓實程序，也就是重整舊的文件，將冗余重複的鍵去除只保留最新的紀錄，減少不必要的過時資料儲存空間。\n但上述方式有兩個缺點 索引必須小於記憶體總量否則性能會很差 不支援範圍讀取 Sorted String Table 圖片來源：https://tech.liuchao.me/2017/11/ddia-3/\n所以後來有提出新的作法 Sorted String Table (簡稱 SSTable)，差別在於索引改透過 依照字串順序排序後的資料結構儲存，這有點像是字典索引，當我們要查一個單字 hello 我們可以透過前後的字去找到相近的位置；\n所以如果在記憶體中所以只有 he / hola，那我們至少知道 hello 勢必在此兩個位置中間，透過 順序讀 sequential read 可以非常快的在硬碟中找到資料。\n作者強調 順序讀寫對於硬碟的效能有很大的幫助，即使是SSD\nB Tree 但是SSTable並不是最常用的資料系統儲存的方式，而是B Tree，B Tree是個平衡二叉樹，母節點紀錄多個區間值與對應的子節點，每個子節點則紀錄一段連續值。\n圖片連結：https://tech.liuchao.me/2017/11/ddia-3/\n在儲存方式上不是採用append only，而是將儲存空間切割固定大小的 Page，並把節點的資料放入，如果有新的資料產生會覆寫舊的Page，這點與SSTable大大不同。\n在實作上，B Tree因為會先將Page資料暫存在記憶體中，為了避免資料遺失會在資料寫入時先將資料紀錄到append-only 的WAL(Write-Ahead Log)，所以一筆資料寫入其實會有兩次寫入硬碟動作。\nB Tree 對比 SSTable 好處在於\n資料冗餘少 找尋不存在的鍵值比較快(SSTable必須查完所有舊資料才可以確定) 不需要 compact process去重整資料，作者提到雖然 compact process通常在背景執行，但你永遠無法預期何時系統會爆量，也就是說好死不死大量讀寫時卡到系統在 compact process就欲哭無淚，相對來說 B Tree的系統附載比較可預期；\nSSTable好處在於 B Tree切割Page容易有空間的破碎化與浪費，且部分資料更新也必須更動對應的Page； Column-Base 一般資料庫在OLTP中都是以row-based為導向的；\n但是在OLAP中，我們常會將多個DB資料彙整到統一的資料倉儲中，所以資料量非常龐大，且每筆request也都要運算非常大量的資料，這時候有個新的做法以column-base儲存資料，這樣最大好處在於同一個column通常資料重複性高，例如顏色就那幾種、產品ID可能也會重複，這種特性可以使 資料壓縮 得到非常好的效果；\n但缺點就是寫入很麻煩。\n在底層紀錄上，B Tree無法套用在壓縮欄位上，因為索引是紀錄row-based的鍵，所以多個column欄位更動會導致過多的Page都要一并刷新；\n所以會採用 SSTable當作寫入硬碟的資料儲存方式，每次寫入都產生新檔案。\nCh 4. Encoding and Evolution 在系統的演進上，需要注意相容性問題，可分成\n・向後相容：新的code可能讀取到舊資料\n・向前相容：舊的code可能讀取到新資料\n程式在處理資料上會有兩種形式\n一種是 in memory，像是 object / array / int等，主要是方便CPU做運算；\n另一種在需要寫入檔案或是透過網路傳輸時，需要重整成字串的形式，也就是encode\n常見的 encode形式有非常多種，包含JSON / XML /CSV，為了讓資料可以用最小容量傳遞，作者介紹了數種基於JSON的encode / decode技術，在encode的過程中要注意資料格式與型別需要可以decode。\n原本JSON原始資料 66 bytes，透過不同的binary encode技術最終可以壓縮到 34 bytes!\n下集 技術筆記 Designing Data-Intensive Applications 下，第二部分主要探討分散式儲存資料會遇到的問題與解法，分散式系統有幾大優點\n","date":"2018-03-28T10:23:52.066Z","permalink":"https://yuanchieh.page/posts/2018/2018-03-28-%E6%8A%80%E8%A1%93%E7%AD%86%E8%A8%98-designing-data-intensive-applications-%E4%B8%8A/","title":"[技術筆記] Designing Data-Intensive Applications 上"},{"content":"一開始會想讀這本書是因為看到學長的心得文，簡略提到印刷時代的思想比腳深層完整，而電視(網路)時代則是片斷瑣碎；\n當初看完心得不太認同，因為我會直覺地認為說「媒體只是載具，重點是人的思想」，也就是膚淺的人不論在哪個時代都是膚淺的；\n後來與學長留言討論一下，發現這本書的論述遠比我想像的深遠，今日拜讀後深感體悟自己的膚淺，所以特此紀錄讀後感。\n自我背景並非傳播媒體相關，過去也鮮少接觸此類型的書，所以看起來蠻硬的，卻又很過癮，正因為少接觸所以在接收一些新觀點時會十分的震撼。\n書籍背景 這次我買的是20週年再版的版本，此書出版於1985年代，處於電視蓬勃發展的世代，作者 波茲曼是著名的媒體文化理論家，談論到媒體受到載具科技的不同，導致人類文化上與思想上的惡行轉變，雖然這本書是論述電視時代，但是其觀點在網路時代更值得去省思。\n這本書我自己拆成三個部分\n媒體傳播方式對於思想的影響：\n前兩個章節作者舉了一些案例說明，順便重重的打臉我剛才的假設 印刷式思想 電視式思想 媒體傳播方式對於思想的影響 不同的媒體偏好特定的內容，從而得以支配文化，作者舉例到 原始的煙霧訊息技術是沒辦法傳遞哲理，因為陣陣的煙霧不夠複雜，所以「形式會排斥內容」\n同樣的在電視螢光幕上，如果總統候選人是個135公斤的超重體重，理論上個人思想內容不該用其外表來衡量，但是透過視覺影像傳達就不免會抹殺其言論的精妙，同理「形式會排斥內容」\n此外，「真理概念的表達和表達形式唇齒相依」，例如說在做研究報告時書面文獻的可信度遠比口語文獻來得高，因為書面文字比較容易驗證、駁斥且客觀；\n所以事實陳述形式的重要程度都視傳媒的影響力量而定。\n印刷式思想 作者定義了一個「闡釋時代」，闡釋是種思維模式，一種學習方式與表達途徑，包含著優異的概念、推理和條理思考能力、崇尚理性和秩序、客觀思維等等；\n作者描述林肯和道格拉斯的七場著名辯論，整個雄辯的過程達七小時之久!討論內容涵蓋了歷史事件、政治議題、廢奴政綱等，講詞內容處處反應印刷式思維，包含論證與反證、主張與對立主張、基於文本的批評和檢驗對手的措辭，聽眾不單需要有豐富的背景知識，還需要有對於公眾事務參與的熱情與理性的批判思考，而這群聽眾在當時不是少數的貴族菁英，而是下至販夫走卒都有這樣的思維能力。\n正因為使用純文字表達，重點著重在文字乘載的意念，所以內容往往更加嚴肅與豐富，不論是作者或閱眾都習慣理性思考去理解冷靜的抽象內容，並用嚴苛的態度去檢視內容前後的一致性與合理性。\n電視式思想 十九世紀中期，電報改變傳遞資訊的方式，以極快的速度跨越浩瀚的距離，這改變了以往印刷時代追求理性的訴求，轉變成追求時效性、誇飾性的新聞；\n作者提出所謂的行動資訊比，意即收到的資訊會有多大的比例導致行動的改變，在印刷時代這兩者的比較雷同；\n但是在電報時代之後，因為資訊大量產生與破碎化，人們面對資訊過剩，導致面臨社會與政治行為能力也逐步衰弱，例如說 要怎麼保護環境、如何降低核戰風險、通貨膨脹率、失業率與犯罪率等問題，雖然現今新聞號稱國際化，我們常常在電視都可以看到各國的新聞報導，但擁有這些破碎化的資訊頂多是茶餘飯後的話題，但不能提昇我們的思維進而去用行動解決問題。\n**「給我娛樂，其餘免談」\n**這是書本的其中一個章節名，闡述在電視論述中娛樂是唯一的表現意識形態，以辯論節目為例，辯論的過程在於思考並反駁對方論述，但是這樣「思考」的過程在電視節目上是不討好的，所以很難在電視辯論節目上看到「讓我想想」、「你說的…是什麼意思」、「你的資料是引述哪些來源」等論述，因為這會拖慢節目速度，也容易讓觀眾感到無聊，但偏偏這些卻是批判思考重要的過程；\n取而代之的是表演藝術且娛樂感十足的演說，就好比 1984年的電視總統辯論會，每個候選人可發言五分鐘並質問對手一分鐘，試問這麼短的時間如何闡述「你對中美洲的外交政策為何」這樣的問題呢?\n所以候選人注重的不再是如何表述自己的理念，更重要的是如何在短短的數分鐘內建立「印象」\n最後作者提到 教育就是娛樂，這是個反諷的標題，在反思聲名遠播的電視教育節目 芝麻街，作者提出電視教育為了迎合電視節目的特性有幾大特色：\n隨時可以加入、不得令人心生疑惑、遠遠避開闡釋，所以我們只能在芝麻街中學到破碎化的知識，可能幾個單字、幾句片語，但我們永遠學不會真正的學習方法，因為學習方法最基本的便是長期思索某個難題，但電視節目本身就是為了娛樂，強加教育目的於上頭根本達不到效果，只能自欺欺人。\n當然有一點很重要，作者並不排斥娛樂，但是當所有形式都以娛樂呈現就非常可怕\n結語 作者在書中反覆提到兩本書 《1984》、《美麗新世界》，兩者都是在談論民眾會有墮落的趨勢，前者理念是高壓政府使民眾屈服；後者提及我們無需外力終將毀於自身所愛，在這電視時代，作者認同赫胥黎 《美麗新世界》的觀點，現在我們不用擔心真相會被隱藏，真正該擔心的是真相被繁瑣的小事給淹沒。\n","date":"2018-03-27T12:10:32.066Z","permalink":"https://yuanchieh.page/posts/2018/2018-03-27-%E9%96%B1%E8%AE%80%E5%BF%83%E5%BE%97-%E5%A8%9B%E6%A8%82%E8%87%B3%E6%AD%BB/","title":"[閱讀心得] 娛樂至死"}]